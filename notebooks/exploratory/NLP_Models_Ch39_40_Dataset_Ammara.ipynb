{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "virgin-anchor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.6.1)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (0.16)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.1-cp38-cp38-manylinux2010_x86_64.whl (3.2 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.56.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (20.9)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.43-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: tokenizers, sacremoses, filelock, transformers\n",
      "Successfully installed filelock-3.0.12 sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n",
      "Collecting bert\n",
      "  Using cached bert-2.2.0-py3-none-any.whl\n",
      "Collecting erlastic\n",
      "  Using cached erlastic-2.0.0-py3-none-any.whl\n",
      "Installing collected packages: erlastic, bert\n",
      "Successfully installed bert-2.2.0 erlastic-2.0.0\n",
      "Collecting bert-tensorflow\n",
      "  Using cached bert_tensorflow-1.0.4-py2.py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from bert-tensorflow) (1.15.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.4\n",
      "Collecting keras\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting h5py\n",
      "  Using cached h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.8/dist-packages (from keras) (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras) (1.20.1)\n",
      "Installing collected packages: h5py, keras\n",
      "Successfully installed h5py-3.1.0 keras-2.4.3\n",
      "Collecting dask_ml\n",
      "  Using cached dask_ml-1.8.0-py3-none-any.whl (141 kB)\n",
      "Collecting dask[array,dataframe]>=2.4.0\n",
      "  Downloading dask-2021.2.0-py3-none-any.whl (900 kB)\n",
      "\u001b[K     |████████████████████████████████| 900 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.52.0)\n",
      "Collecting distributed>=2.4.0\n",
      "  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n",
      "\u001b[K     |████████████████████████████████| 675 kB 26.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dask-glm>=0.2.0\n",
      "  Using cached dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.24.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.2.2)\n",
      "Collecting multipledispatch>=0.4.9\n",
      "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from dask_ml) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.20.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.6.1)\n",
      "Collecting cloudpickle>=0.2.2\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (5.4.1)\n",
      "Collecting fsspec>=0.6.0\n",
      "  Downloading fsspec-0.8.7-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 53.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting partd>=0.3.10\n",
      "  Downloading partd-1.1.0-py3-none-any.whl (19 kB)\n",
      "Collecting toolz>=0.8.2\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 609 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting psutil>=5.0\n",
      "  Downloading psutil-5.8.0-cp38-cp38-manylinux2010_x86_64.whl (296 kB)\n",
      "\u001b[K     |████████████████████████████████| 296 kB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zict>=0.1.3\n",
      "  Downloading zict-2.0.0-py3-none-any.whl (10 kB)\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from distributed>=2.4.0->dask_ml) (45.2.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (6.1)\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.2-cp38-cp38-manylinux1_x86_64.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 51.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from multipledispatch>=0.4.9->dask_ml) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dask_ml) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dask_ml) (2.8.1)\n",
      "Collecting locket\n",
      "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->dask_ml) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->dask_ml) (2.1.0)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: llvmlite<0.36,>=0.35.0 in /usr/local/lib/python3.8/dist-packages (from numba->dask_ml) (0.35.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->dask_ml) (2.4.7)\n",
      "Installing collected packages: toolz, locket, heapdict, dask, zict, tblib, sortedcontainers, psutil, partd, multipledispatch, msgpack, fsspec, cloudpickle, distributed, dask-glm, dask-ml\n",
      "Successfully installed cloudpickle-1.6.0 dask-2021.2.0 dask-glm-0.2.0 dask-ml-1.8.0 distributed-2021.2.0 fsspec-0.8.7 heapdict-1.0.1 locket-0.2.1 msgpack-1.0.2 multipledispatch-0.6.0 partd-1.1.0 psutil-5.8.0 sortedcontainers-2.3.0 tblib-1.7.0 toolz-0.11.1 zict-2.0.0\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.20.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn\n",
    "!pip install graphviz\n",
    "!pip install transformers\n",
    "!pip install bert\n",
    "!pip install bert-tensorflow\n",
    "!pip install keras\n",
    "!pip install dask_ml\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amateur-hearing",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eba28711c936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mddf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpanel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import dask.dataframe as ddf\n",
    "from math import nan\n",
    "import panel as pn\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import seaborn as sns\n",
    "import plotly as pty\n",
    "import plotly.express as px\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, plot_confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "printable-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-engineering",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "celtic-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(dataframe_column, arguments):\n",
    "    \"\"\"\n",
    "    Takes a pandas series df['columnname'] and applies various text preprocessing methods\n",
    "    passed to function as list.\n",
    "    Returns a pandas data series as new column\n",
    "    \"\"\"\n",
    "#     print(type(dataframe_column))\n",
    "    new_series = dataframe_column\n",
    "    if 'lower' in arguments:\n",
    "        new_series = new_series.str.lower()\n",
    "    # Remove stopwords and convert back to string\n",
    "    if 'remove_stopwords' in arguments:\n",
    "        new_series = new_series.apply(lambda x: [item for item in x.split() if item not in stop]).str.join(\" \")\n",
    "    # Remove HS Codes from Product Description as it is supposed to be predicted\n",
    "    if 'remove_HS_Codes' in arguments:\n",
    "        new_series = [re.sub('\\d{4,}', '', x) for x in new_series]\n",
    "    return new_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "altered-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, X, y, X_test, y_test, labels=None, classifier_name='unnamed'):\n",
    "    ### provide classifier, train and test set\n",
    "    ### get train/val split\n",
    "    ### fit on val\n",
    "    ### test on test\n",
    "    ### return accuracy score for test\n",
    "    \n",
    "#     print(\"validation results:\")\n",
    "    classifier.fit(X, y)\n",
    "#     print(classification_report(y_val, classifier.predict(X_val)))\n",
    "    \n",
    "    print(\"Dev set results:\")\n",
    "    X_test_preds = classifier.predict(X_test)\n",
    "    # save the classifier\n",
    "    with open(classifier_name + '.pkl', 'wb') as fid:\n",
    "        pickle.dump(classifier, fid)    \n",
    "\n",
    "#     print(classification_report(y_test, X_test_preds) )\n",
    "    # Plot non-normalized confusion matrix\n",
    "    cm = confusion_matrix(y_test, X_test_preds)\n",
    "#     print(cm)\n",
    "    return [accuracy_score(y_test, X_test_preds), precision_score(y_test, X_test_preds, average=\"weighted\"), recall_score(y_test, X_test_preds, average=\"weighted\")]\n",
    "# f1_score(y_dev, predicted, average=\"weighted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-proportion",
   "metadata": {},
   "source": [
    "## Load the PARQ data files and save to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "compatible-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_parq = dd.read_parquet('/data/common/trade_data/2019_updated/data_samples/sample_chap39_40.parq', engine='fastparquet', chunksize=\"100MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bulgarian-light",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.9s\n",
      "[########################################] | 100% Completed |  1.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134890"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_df_parq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "informal-invitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n"
     ]
    }
   ],
   "source": [
    "sampled_df = sample_df_parq.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "funky-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find length of Product Description free text\n",
    "sampled_df['Product_Desc_Length'] = sampled_df['Product Desc'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "injured-blocking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Desc</th>\n",
       "      <th>Product_Desc_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;</td>\n",
       "      <td>2870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003&lt;br/&gt;POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - HDPE WHI0650&lt;br/&gt;POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003&lt;br/&gt;POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003&lt;br/&gt;POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003&lt;br/&gt;POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003 ==================== 8,910 (25KG) BAGS LOADED INTO 9 X 40 CONTAINERS LD PE ZLF003 (198.000 MT / 7,920 BAGS) - HS CODE390110 HDPE WHI0650 (24.750 MT / 990 BAGS) - HS CODE 390120 21 DAYS FREE TIME AT DESTINAT ION FREIGHT PREPAID ========================= = ========================== ======================= MEDITERRANEAN SHIPPING COMPANY (SHANGHAI) LIMITED - NINGBO BRANCH NO.88, CHANGSHA BAY, BAIZHONG ROAD BAIFENG BEILUN 315813 NINGBO, CHINA PHONE+86 574 8672 5042 FAX+86 574 2769 8762&lt;br/&gt;POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003&lt;br/&gt;POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003&lt;br/&gt;POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003&lt;br/&gt;</td>\n",
       "      <td>1091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>74.250 MT LLDPE 118NJ 148 DE LIVERY NO:800833132 SALES OR DER NO:4503354988 CARGO NET WEIGHT: 74.250 MT CARGO GROS S WEIGHT: 75.870 MT CONTAINE R TARE WEIGHT: 12.000 MT TOT AL GROSS WEIGHT:87.870 MT H. S.CODE 3901.10 TOTAL PALLETS 54 AGENT AT DESTINATION: MAERSK PERU S.A. - ALCONSA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CA LLAO 100 CALLAO PERU PHONE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;74.250 MT LLDPE 118NJ 148 DE LIVERY NO:800833132 SALES OR DER NO:4503354988 CARGO NET WEIGHT: 74.250 MT CARGO GROS S WEIGHT: 75.870 MT CONTAINE R TARE WEIGHT: 12.000 MT TOT AL GROSS WEIGHT:87.870 MT H. S.CODE 3901.10 TOTAL PALLETS 54 AGENT AT DESTINATION: MAERSK PERU S.A. - ALCONSA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CA LLAO 100 CALLAO PERU PHONE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;74.250 MT LLDPE 118NJ 148 DE LIVERY NO:800833132 SALES OR DER NO:4503354988 CARGO NET WEIGHT: 74.250 MT CARGO GROS S WEIGHT: 75.870 MT CONTAINE R TARE WEIGHT: 12.000 MT TOT AL GROSS WEIGHT:87.870 MT H. S.CODE 3901.10 TOTAL PALLETS 54 AGENT AT DESTINATION: MAERSK PERU S.A. - ALCONSA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CA LLAO 100 CALLAO PERU PHONE: 51 1 6140050 FAX: 51 1 5776153&lt;br/&gt;</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Product Desc  \\\n",
       "8   173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153<br/>173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153<br/>173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153<br/>173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153<br/>173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153<br/>173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153<br/>173.250 LLDPE 218WJ 148 H.S. CODE: 3901.10 DELIVERY NO:80 1742665 SALES ORDER NO: 4503 669596 CARGO NET WEIGHT: 173 .250 MT CARGO GROSS WEIGHT: 177.065 MT CONTR TARE WEIGHT : 28.000 MT TOTAL GROSS WEIG HT: 205.065 MT TOTAL PALLETS : 126.00 AGENT AT DESTINATI ON: MAERSK PERU S.A. - ALCON SA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CALLAO 100 CALLAO PERU PHO NE: 51 1 6140050 FAX: 51 1 5776153<br/>   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003<br/>POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - HDPE WHI0650<br/>POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003<br/>POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003<br/>POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003<br/>POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003 ==================== 8,910 (25KG) BAGS LOADED INTO 9 X 40 CONTAINERS LD PE ZLF003 (198.000 MT / 7,920 BAGS) - HS CODE390110 HDPE WHI0650 (24.750 MT / 990 BAGS) - HS CODE 390120 21 DAYS FREE TIME AT DESTINAT ION FREIGHT PREPAID ========================= = ========================== ======================= MEDITERRANEAN SHIPPING COMPANY (SHANGHAI) LIMITED - NINGBO BRANCH NO.88, CHANGSHA BAY, BAIZHONG ROAD BAIFENG BEILUN 315813 NINGBO, CHINA PHONE+86 574 8672 5042 FAX+86 574 2769 8762<br/>POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003<br/>POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003<br/>POLYMERS OF ETHYLENE, IN PRIMARY FORMSI. PRIMARY - LDPE ZLF003<br/>   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  74.250 MT LLDPE 118NJ 148 DE LIVERY NO:800833132 SALES OR DER NO:4503354988 CARGO NET WEIGHT: 74.250 MT CARGO GROS S WEIGHT: 75.870 MT CONTAINE R TARE WEIGHT: 12.000 MT TOT AL GROSS WEIGHT:87.870 MT H. S.CODE 3901.10 TOTAL PALLETS 54 AGENT AT DESTINATION: MAERSK PERU S.A. - ALCONSA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CA LLAO 100 CALLAO PERU PHONE: 51 1 6140050 FAX: 51 1 5776153<br/>74.250 MT LLDPE 118NJ 148 DE LIVERY NO:800833132 SALES OR DER NO:4503354988 CARGO NET WEIGHT: 74.250 MT CARGO GROS S WEIGHT: 75.870 MT CONTAINE R TARE WEIGHT: 12.000 MT TOT AL GROSS WEIGHT:87.870 MT H. S.CODE 3901.10 TOTAL PALLETS 54 AGENT AT DESTINATION: MAERSK PERU S.A. - ALCONSA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CA LLAO 100 CALLAO PERU PHONE: 51 1 6140050 FAX: 51 1 5776153<br/>74.250 MT LLDPE 118NJ 148 DE LIVERY NO:800833132 SALES OR DER NO:4503354988 CARGO NET WEIGHT: 74.250 MT CARGO GROS S WEIGHT: 75.870 MT CONTAINE R TARE WEIGHT: 12.000 MT TOT AL GROSS WEIGHT:87.870 MT H. S.CODE 3901.10 TOTAL PALLETS 54 AGENT AT DESTINATION: MAERSK PERU S.A. - ALCONSA AV. NESTOR GAMBETTA S/N KM 14.5 CARRETERA VENTANILLA CA LLAO 100 CALLAO PERU PHONE: 51 1 6140050 FAX: 51 1 5776153<br/>   \n",
       "\n",
       "    Product_Desc_Length  \n",
       "8                  2870  \n",
       "10                 1091  \n",
       "13                 1206  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df[sampled_df['Product_Desc_Length']>1000][['Product Desc','Product_Desc_Length']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ancient-leave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    134890.000000\n",
       "mean        353.686589\n",
       "std        2060.392594\n",
       "min           8.000000\n",
       "25%          68.000000\n",
       "50%         151.000000\n",
       "75%         338.000000\n",
       "max      429170.000000\n",
       "Name: Product_Desc_Length, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_df['Product_Desc_Length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "frequent-durham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG [390110 390120 390130 390140 390190 390210 390220 390230 390290 390311\n",
      " 390319 390320 390330 390390 390410 390421 390422 390430 390440 390450\n",
      " 390461 390469 390490 390512 390519 390521 390529 390530 390591 390599\n",
      " 390770 390791 390799 390610 390690 390710 390720 390730 390740 390750\n",
      " 390761 390769 390810 390890 390910 390920 390931 390939 390940 390950\n",
      " 391000 391110 391190 391211 391212 391220 391231 391239 391290 391310\n",
      " 391390 391400 391510 391520 391530 391590 391610 391620 391690 391710\n",
      " 391721 391722 391723 391729 391731 391732 391733 391739 391740 391810\n",
      " 391890 391910 391990 392010 392020 392030 392043 392049 392051 392059\n",
      " 392061 392062 392063 392069 392071 392073 392079 392091 392092 392093\n",
      " 392094 392099 392111 392112 392113 392114 392119 392190 392210 392220\n",
      " 392290 392310 392321 392329 392330 392340 392350 392390 392410 392490\n",
      " 392510 392520 392530 392590 392610 392620 392630 392640 392690 400110\n",
      " 400121 400122 400129 400130 400211 400219 400220 400231 400239 400241\n",
      " 400249 400251 400259 400260 400270 400280 400291 400299 400300 400400\n",
      " 400510 400520 400591 400599 400610 400690 400700 400811 400819 400821\n",
      " 400829 400911 400912 400921 400922 400931 400932 400941 400942 401011\n",
      " 401012 401019 401031 401032 401033 401034 401035 401036 401039 401110\n",
      " 401120 401130 401140 401150 401170 401180 401190 401211 401212 401213\n",
      " 401219 401220 401290 401310 401320 401390 401410 401490 401511 401519\n",
      " 401590 401610 401691 401692 401693 401694 401695 401699 401700]\n"
     ]
    }
   ],
   "source": [
    "random_seed = 1\n",
    "all_classes = sampled_df.HS_Code.unique()\n",
    "print('DEBUG', all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "varying-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = sampled_df[['HS_Code', 'Product Desc', 'Product_Desc_Length', 'Merged_Description']].rename({'HS_Code' : 'label', 'Product Desc' : 'text'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "romantic-bloom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>Product_Desc_Length</th>\n",
       "      <th>Merged_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>390110</td>\n",
       "      <td>PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS&lt;br/&gt;PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS&lt;br/&gt;PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS&lt;br/&gt;PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS&lt;br/&gt;PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS&lt;br/&gt;PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS&lt;br/&gt;PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS HS CODE: 390110 FREIGHT PREPAID&lt;br/&gt;</td>\n",
       "      <td>543</td>\n",
       "      <td>Polymers of ethylene, in primary forms ;Polyethylene having a specific gravity of less than 0.94 ;Having a relative viscosity of 1.44 or more;Other;Linear low density polyethylene;Low density polyethylene, except linear low density polyethylene;Medium density polyethylene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>390110</td>\n",
       "      <td>POLYETHYLENE COMMODITY CODE 390110 NET WE IGHT 21.420,0000 KG&lt;br/&gt;</td>\n",
       "      <td>66</td>\n",
       "      <td>Polymers of ethylene, in primary forms ;Polyethylene having a specific gravity of less than 0.94 ;Having a relative viscosity of 1.44 or more;Other;Linear low density polyethylene;Low density polyethylene, except linear low density polyethylene;Medium density polyethylene</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  \\\n",
       "0  390110   \n",
       "1  390110   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0  PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS<br/>PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS<br/>PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS<br/>PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS<br/>PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS<br/>PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS<br/>PACKAGE VISICO LE4421 1/2 SIZE OCT ABIN LD POLYETHYLENE =36 OCTABINS HS CODE: 390110 FREIGHT PREPAID<br/>   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               POLYETHYLENE COMMODITY CODE 390110 NET WE IGHT 21.420,0000 KG<br/>   \n",
       "\n",
       "   Product_Desc_Length  \\\n",
       "0                  543   \n",
       "1                   66   \n",
       "\n",
       "                                                                                                                                                                                                                                                                 Merged_Description  \n",
       "0  Polymers of ethylene, in primary forms ;Polyethylene having a specific gravity of less than 0.94 ;Having a relative viscosity of 1.44 or more;Other;Linear low density polyethylene;Low density polyethylene, except linear low density polyethylene;Medium density polyethylene  \n",
       "1  Polymers of ethylene, in primary forms ;Polyethylene having a specific gravity of less than 0.94 ;Having a relative viscosity of 1.44 or more;Other;Linear low density polyethylene;Low density polyethylene, except linear low density polyethylene;Medium density polyethylene  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "quick-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df['text'] = text_preprocessing(new_df['text'], ['remove_HS_Codes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-battle",
   "metadata": {},
   "source": [
    "#### Train - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "resident-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Val_X, Train_y, Val_y = model_selection.train_test_split(new_df['text'], new_df['label'], test_size=0.25, random_state=1, stratify=new_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "liquid-accessory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  101167 101167\n",
      "Val :  33723 33723\n"
     ]
    }
   ],
   "source": [
    "print(\"Train : \", len(Train_X), len(Train_y))\n",
    "print(\"Val : \", len(Val_X), len(Val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-amino",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "beneficial-pottery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model Name, Accuracy, Precision, Recall]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['Model Name', 'Accuracy', 'Precision', 'Recall']\n",
    "\n",
    "results = pd.DataFrame(columns = column_names)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-adrian",
   "metadata": {},
   "source": [
    "### With Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-challenge",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "combined-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model1a = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "acc = train(model1a, Train_X, Train_y, Val_X, Val_y, labels = new_df['label'].unique(), classifier_name='MNB_CNTVect_Chp39_40')\n",
    "\n",
    "results = results.append(pd.DataFrame([['MNB_CNTVect'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-singapore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev set results:\n"
     ]
    }
   ],
   "source": [
    "model1b = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "acc = train(model1b, Train_X, Train_y, Val_X, Val_y, labels = new_df['label'].unique(),classifier_name='MNB_CNTVect_NoSW_chp39_40')\n",
    "\n",
    "results = results.append(pd.DataFrame([['MNB_CNTVect_NoSW'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1c = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "acc = train(model1c, text_preprocessing(Train_X, ['remove_HS_Codes' ]), Train_y, text_preprocessing(Val_X, ['remove_HS_Codes']), Val_y,\n",
    "            labels = new_df['label'].unique(),\n",
    "           classifier_name='MNB_CNTVect_NoSW_NoHSCode_chp39_40')\n",
    "\n",
    "results = results.append(pd.DataFrame([['MNB_CNTVect_NoSW_NoHSCode'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2a = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "acc = train(model2a, Train_X, Train_y, Val_X, Val_y, labels = new_df['label'].unique(),\n",
    "           classifier_name='MNB_TFIDFVect_Chp39_40')\n",
    "\n",
    "results = results.append(pd.DataFrame([['MNB_TFIDFVect'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2b = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words = 'english')),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "acc = train(model2b, Train_X, Train_y, Val_X, Val_y, labels = new_df['label'].unique())\n",
    "\n",
    "results = results.append(pd.DataFrame([['MNB_TFIDFVect_NoSW'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2c = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words = 'english')),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "acc = train(model2c, text_preprocessing(Train_X, ['remove_HS_Codes' ]), Train_y, text_preprocessing(Val_X, ['remove_HS_Codes']), Val_y, labels = new_df['label'].unique())\n",
    "\n",
    "results = results.append(pd.DataFrame([['MNB_TFIDVect_NoSW_NoHSCode'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-algeria",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3a = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(model3a, Train_X, Train_y, Val_X, Val_y, labels = new_df['label'].unique())\n",
    "\n",
    "results = results.append(pd.DataFrame([['XGBoost_CNTVect'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3b = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(model3b, Train_X, Train_y, Val_X, Val_y, labels = new_df['label'].unique())\n",
    "\n",
    "results = results.append(pd.DataFrame([['XGBoost_CNTVect_NoSW'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3c = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words = 'english')),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(model3c, text_preprocessing(Train_X, ['remove_HS_Codes' ]), Train_y, text_preprocessing(Val_X, ['remove_HS_Codes']), Val_y, labels = new_df['label'].unique())\n",
    "\n",
    "results = results.append(pd.DataFrame([['XGBoost_CNTVect_NoSW_NoHSCode'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4a = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(model4a, Train_X, Train_y, Val_X, Val_y, labels = new_df['label'].unique())\n",
    "\n",
    "results = results.append(pd.DataFrame([['XGBoost_TFIDFVect'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4b = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words = 'english')),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])\n",
    " \n",
    " \n",
    "acc = train(model4b, Train_X, Train_y, Val_X, Val_y, labels = new_df['label'].unique())\n",
    "\n",
    "results = results.append(pd.DataFrame([['XGBoost_TFIDFVect_NoSW'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4c = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words = 'english')),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(model4c, text_preprocessing(Train_X, ['remove_HS_Codes' ]), Train_y, text_preprocessing(Val_X, ['remove_HS_Codes']), Val_y, labels = new_df['label'].unique())\n",
    "\n",
    "results = results.append(pd.DataFrame([['XGBoost_TFIDVect_NoSW_NoHSCode'] + acc], columns=column_names), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results from Chapter 30 and Chapeter 40 dataset\\n')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-masters",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-softball",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dt = datetime.datetime.now()\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = current_dt.strftime(\"%d%m%Y_%H%M\")\n",
    "print(dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('ModelResults_Chp39_40_'+ '02282021' + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
