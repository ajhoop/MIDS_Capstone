{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "saving-width",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (0.16)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.43-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.56.2)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.1-cp38-cp38-manylinux2010_x86_64.whl (3.2 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: tokenizers, sacremoses, filelock, transformers\n",
      "Successfully installed filelock-3.0.12 sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n",
      "Collecting bert\n",
      "  Using cached bert-2.2.0-py3-none-any.whl\n",
      "Collecting erlastic\n",
      "  Using cached erlastic-2.0.0-py3-none-any.whl\n",
      "Installing collected packages: erlastic, bert\n",
      "Successfully installed bert-2.2.0 erlastic-2.0.0\n",
      "Collecting bert-tensorflow\n",
      "  Using cached bert_tensorflow-1.0.4-py2.py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from bert-tensorflow) (1.15.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.4\n",
      "Collecting keras\n",
      "  Using cached Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.2.0-cp38-cp38-manylinux1_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras) (1.18.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.8/dist-packages (from keras) (1.6.0)\n",
      "Installing collected packages: h5py, keras\n",
      "Successfully installed h5py-3.2.0 keras-2.4.3\n",
      "Collecting dask_ml\n",
      "  Using cached dask_ml-1.8.0-py3-none-any.whl (141 kB)\n",
      "Collecting dask-glm>=0.2.0\n",
      "  Using cached dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.24.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from dask_ml) (20.9)\n",
      "Requirement already satisfied: distributed>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (2.30.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.18.1)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (2.17.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.6.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.2.2)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.52.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from dask-glm>=0.2.0->dask_ml) (1.6.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (5.4.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (0.11.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (0.8.5)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (1.1.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (7.1.2)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (6.1)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (2.0.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (2.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from distributed>=2.4.0->dask_ml) (45.2.0)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (5.8.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (1.0.2)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (1.7.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from multipledispatch>=0.4.9->dask_ml) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dask_ml) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dask_ml) (2.8.1)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask[array,dataframe]>=2.4.0->dask_ml) (0.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->dask_ml) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->dask_ml) (2.1.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2.4.0->dask_ml) (1.0.1)\n",
      "Requirement already satisfied: llvmlite<0.36,>=0.35.0 in /usr/local/lib/python3.8/dist-packages (from numba->dask_ml) (0.35.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->dask_ml) (2.4.7)\n",
      "Installing collected packages: dask-glm, dask-ml\n",
      "Successfully installed dask-glm-0.2.0 dask-ml-1.8.0\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-1.3.3-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.18.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.3\n",
      "Collecting datascroller\n",
      "  Using cached datascroller-1.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (from datascroller) (3.0.0)\n",
      "Collecting pandasql\n",
      "  Using cached pandasql-0.7.3-py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datascroller) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datascroller) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from pandas->datascroller) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datascroller) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datascroller) (1.15.0)\n",
      "Collecting sqlalchemy\n",
      "  Using cached SQLAlchemy-1.3.23-cp38-cp38-manylinux2010_x86_64.whl (1.3 MB)\n",
      "Installing collected packages: sqlalchemy, pandasql, datascroller\n",
      "Successfully installed datascroller-1.4.1 pandasql-0.7.3 sqlalchemy-1.3.23\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Using cached protobuf-3.15.3-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1-cp38-cp38-linux_x86_64.whl\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorboard~=2.4\n",
      "  Using cached tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting wheel~=0.35\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Using cached h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.4->tensorflow) (45.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.27.0-py2.py3-none-any.whl (135 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.4->tensorflow) (2.25.1)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, wheel, werkzeug, tensorboard-plugin-wit, protobuf, numpy, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.1\n",
      "    Uninstalling numpy-1.18.1:\n",
      "      Successfully uninstalled numpy-1.18.1\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.2.0\n",
      "    Uninstalling h5py-3.2.0:\n",
      "      Successfully uninstalled h5py-3.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "holoviz 0.11.6 requires numpy==1.18.1, but you have numpy 1.19.5 which is incompatible.\n",
      "holoviz 0.11.6 requires pandas==1.0.3, but you have pandas 1.2.2 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.1 flatbuffers-1.12 gast-0.3.3 google-auth-1.27.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 keras-preprocessing-1.1.2 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.15.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 werkzeug-1.0.1 wheel-0.36.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n",
    "!pip install transformers\n",
    "!pip install bert\n",
    "!pip install bert-tensorflow\n",
    "!pip install keras\n",
    "!pip install dask_ml\n",
    "!pip install xgboost\n",
    "!pip install datascroller\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "informed-fiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reasonable-elizabeth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "# Import dask packages\n",
    "# import dask.dataframe as ddf\n",
    "from math import nan\n",
    "import panel as pn\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dask_ml.model_selection import train_test_split\n",
    "import graphviz\n",
    "from datascroller import scroll\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "import transformers as ppb\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "import bert\n",
    "# from bert import run_classifier\n",
    "# from bert import optimization\n",
    "from bert import tokenization\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow import keras\n",
    "#### if use tensorflow=2.0.0, then import tensorflow.keras.model_selection \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-carroll",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Install BERT and BERT Tokenizer from the HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-candy",
   "metadata": {},
   "source": [
    "We can easily switch between variants of BERT by changing out which model we import from HuggingFace; the rest of the code just flows unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DistilBERT:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.TFDistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.TFBertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "## For BERT Large, use this:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.AutoModelWithLMHead, ppb.AutoTokenizer, 'bert-large-uncased')\n",
    "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# For ROBERTa base model, use this:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.TFRobertaModel, ppb.RobertaTokenizer, 'roberta-base')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-bowling",
   "metadata": {},
   "source": [
    "## Let's do an initial import on the sample dataset Padma created for HS4 codes 8712 and 8714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "associate-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hs_code_chap_to_keep.csv  sample_chap_39.parq  sample_chap_74.parq\n",
      "sample_chap_10.parq\t  sample_chap_40.parq  sample_chap_75.parq\n",
      "sample_chap_11.parq\t  sample_chap_47.parq  sample_chap_76.parq\n",
      "sample_chap_12.parq\t  sample_chap_48.parq  sample_chap_78.parq\n",
      "sample_chap_13.parq\t  sample_chap_49.parq  sample_chap_79.parq\n",
      "sample_chap_14.parq\t  sample_chap_50.parq  sample_chap_80.parq\n",
      "sample_chap_15.parq\t  sample_chap_51.parq  sample_chap_81.parq\n",
      "sample_chap_16.parq\t  sample_chap_52.parq  sample_chap_82.parq\n",
      "sample_chap_17.parq\t  sample_chap_53.parq  sample_chap_83.parq\n",
      "sample_chap_18.parq\t  sample_chap_54.parq  sample_chap_84.parq\n",
      "sample_chap_19.parq\t  sample_chap_55.parq  sample_chap_85.parq\n",
      "sample_chap_20.parq\t  sample_chap_56.parq  sample_chap_86.parq\n",
      "sample_chap_21.parq\t  sample_chap_57.parq  sample_chap_87.parq\n",
      "sample_chap_22.parq\t  sample_chap_58.parq  sample_chap_88.parq\n",
      "sample_chap_28.parq\t  sample_chap_59.parq  sample_chap_89.parq\n",
      "sample_chap_29.parq\t  sample_chap_60.parq  sample_chap_9.parq\n",
      "sample_chap_30.parq\t  sample_chap_61.parq  sample_chap_90.parq\n",
      "sample_chap_31.parq\t  sample_chap_62.parq  sample_chap_91.parq\n",
      "sample_chap_32.parq\t  sample_chap_63.parq  sample_chap_92.parq\n",
      "sample_chap_33.parq\t  sample_chap_64.parq  sample_chap_93.parq\n",
      "sample_chap_34.parq\t  sample_chap_65.parq  sample_chap_94.parq\n",
      "sample_chap_35.parq\t  sample_chap_66.parq  sample_chap_95.parq\n",
      "sample_chap_36.parq\t  sample_chap_67.parq  sample_chap_96.parq\n",
      "sample_chap_37.parq\t  sample_chap_72.parq\n",
      "sample_chap_38.parq\t  sample_chap_73.parq\n"
     ]
    }
   ],
   "source": [
    "!ls /data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "olive-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first sample\n",
    "# import_df = dd.read_parquet('/data/common/trade_data/2019/data_samples/sample_87128714.parq', engine='fastparquet', chunksize=\"100MB\")\n",
    "\n",
    "# updated sample with the multple hs codes entries removed\n",
    "# import_df = dd.read_parquet('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_ignore_multiple_hscode_chap39_40.parq', engine='fastparquet', chunksize=\"100MB\")\n",
    "files = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_1*.parq')\n",
    "import_df = pd.concat([pd.read_parquet(fp) for fp in files])\n",
    "\n",
    "# import_df = pd.read_parquet('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desirable-obligation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_15.parq',\n",
       " '/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_17.parq',\n",
       " '/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_12.parq',\n",
       " '/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_10.parq',\n",
       " '/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_14.parq',\n",
       " '/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_11.parq',\n",
       " '/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_16.parq',\n",
       " '/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_19.parq',\n",
       " '/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_18.parq',\n",
       " '/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_13.parq']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "communist-yield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['System Identity Id', 'Estimate Arrival Date', 'Actual Arrival Date',\n",
       "       'Bill of Lading', 'Master Bill of Lading', 'Bill Type Code',\n",
       "       'Carrier SASC Code', 'Vessel Country Code', 'Vessel Code',\n",
       "       'Vessel Name', 'Voyage', 'Inbond Type', 'Manifest No',\n",
       "       'Mode of Transportation', 'Loading Port', 'Last Vist Foreign Port',\n",
       "       'US Clearing District', 'Unloading Port', 'Place of Receipt', 'Country',\n",
       "       'Country Sure Level', 'Weight in KG', 'Weight', 'Weight Unit', 'TEU',\n",
       "       'Quantity', 'Quantity Unit', 'Measure in CM', 'Measure', 'Measure Unit',\n",
       "       'Container Id', 'Container Size', 'Container Type',\n",
       "       'Container Desc Code', 'Container Load Status',\n",
       "       'Container Type of Service', 'Shipper Name', 'Shipper Address ',\n",
       "       'Raw Shipper Name', 'Raw Shipper Addr1', 'Raw Shipper Addr2',\n",
       "       'Raw Shipper Addr3', 'Raw Shipper Addr4', 'Raw Shipper Addr Others',\n",
       "       'Consignee Name', 'Consignee Address ', 'Raw Consignee Name',\n",
       "       'Raw Consignee Addr1', 'Raw Consignee Addr2', 'Raw Consignee Addr3',\n",
       "       'Raw Consignee Addr4', 'Raw Consignee Addr Others', 'Notify Party Name',\n",
       "       'Notify Party Address ', 'Raw Notify Party Name',\n",
       "       'Raw Notify Party Addr1', 'Raw Notify Party Addr2',\n",
       "       'Raw Notify Party Addr3', 'Raw Notify Party Addr4',\n",
       "       'Raw Notify Party Addr Others', 'Product Desc', 'Marks & Numbers',\n",
       "       'HS Code', 'HS Code Sure Level', 'CIF', 'Indicator of true supplier',\n",
       "       'Indicator of true buyer', 'END', 'Cleaned_HS_Code', 'HS_Code',\n",
       "       'Merged_Description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "orange-pharmacology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System Identity Id</th>\n",
       "      <th>Estimate Arrival Date</th>\n",
       "      <th>Actual Arrival Date</th>\n",
       "      <th>Bill of Lading</th>\n",
       "      <th>Master Bill of Lading</th>\n",
       "      <th>Bill Type Code</th>\n",
       "      <th>Carrier SASC Code</th>\n",
       "      <th>Vessel Country Code</th>\n",
       "      <th>Vessel Code</th>\n",
       "      <th>Vessel Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Marks &amp; Numbers</th>\n",
       "      <th>HS Code</th>\n",
       "      <th>HS Code Sure Level</th>\n",
       "      <th>CIF</th>\n",
       "      <th>Indicator of true supplier</th>\n",
       "      <th>Indicator of true buyer</th>\n",
       "      <th>END</th>\n",
       "      <th>Cleaned_HS_Code</th>\n",
       "      <th>HS_Code</th>\n",
       "      <th>Merged_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>193322</th>\n",
       "      <td>6003201908300000674681</td>\n",
       "      <td>20190828</td>\n",
       "      <td>20190829</td>\n",
       "      <td>ZIMUTRT0102730</td>\n",
       "      <td>None</td>\n",
       "      <td>R</td>\n",
       "      <td>ZIMU, ZIM ISRAEL NAVIGATION CO</td>\n",
       "      <td>SG</td>\n",
       "      <td>9366500</td>\n",
       "      <td>ASIATIC HORIZON</td>\n",
       "      <td>...</td>\n",
       "      <td>NO MARKS&lt;br/&gt;</td>\n",
       "      <td>150110</td>\n",
       "      <td>8</td>\n",
       "      <td>533600.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>END</td>\n",
       "      <td>150110</td>\n",
       "      <td>150110</td>\n",
       "      <td>Pig fat (including lard) and poultry fat, other than that of heading 0209 or 1503 ;Lard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489756</th>\n",
       "      <td>6003201904090000598510</td>\n",
       "      <td>20190408</td>\n",
       "      <td>20190408</td>\n",
       "      <td>TSCW13645275</td>\n",
       "      <td>None</td>\n",
       "      <td>R</td>\n",
       "      <td>TSCW, TROPICAL SHIPPING &amp; CONSTRUCTION CO</td>\n",
       "      <td>PA</td>\n",
       "      <td>9809904</td>\n",
       "      <td>TROPIC HOPE</td>\n",
       "      <td>...</td>\n",
       "      <td>BSIU9477265&lt;br/&gt;</td>\n",
       "      <td>150110</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>END</td>\n",
       "      <td>150110</td>\n",
       "      <td>150110</td>\n",
       "      <td>Pig fat (including lard) and poultry fat, other than that of heading 0209 or 1503 ;Lard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            System Identity Id Estimate Arrival Date Actual Arrival Date  \\\n",
       "193322  6003201908300000674681  20190828              20190829             \n",
       "489756  6003201904090000598510  20190408              20190408             \n",
       "\n",
       "        Bill of Lading Master Bill of Lading Bill Type Code  \\\n",
       "193322  ZIMUTRT0102730  None                  R               \n",
       "489756  TSCW13645275    None                  R               \n",
       "\n",
       "                                Carrier SASC Code Vessel Country Code  \\\n",
       "193322  ZIMU, ZIM ISRAEL NAVIGATION CO             SG                   \n",
       "489756  TSCW, TROPICAL SHIPPING & CONSTRUCTION CO  PA                   \n",
       "\n",
       "       Vessel Code      Vessel Name  ...   Marks & Numbers HS Code  \\\n",
       "193322  9366500     ASIATIC HORIZON  ...  NO MARKS<br/>     150110   \n",
       "489756  9809904     TROPIC HOPE      ...  BSIU9477265<br/>  150110   \n",
       "\n",
       "       HS Code Sure Level       CIF Indicator of true supplier  \\\n",
       "193322  8                  533600.0  Y                           \n",
       "489756  8                  0.0       Y                           \n",
       "\n",
       "       Indicator of true buyer  END Cleaned_HS_Code HS_Code  \\\n",
       "193322  Y                       END  150110          150110   \n",
       "489756  Y                       END  150110          150110   \n",
       "\n",
       "                                                                             Merged_Description  \n",
       "193322  Pig fat (including lard) and poultry fat, other than that of heading 0209 or 1503 ;Lard  \n",
       "489756  Pig fat (including lard) and poultry fat, other than that of heading 0209 or 1503 ;Lard  \n",
       "\n",
       "[2 rows x 71 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't truncate text fields in the display\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "\n",
    "import_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "swiss-suicide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104866"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(import_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raised-healing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     Desc  HSCode\n",
      "193322  SUPREME ALL PURPOSE LARD IN PAILS 37LBS CAED N...  150110\n",
      "489756  SLAC: 840 PAILS NABORI ALL PURPOSE LARD HS COD...  150110\n",
      "104866\n"
     ]
    }
   ],
   "source": [
    "df1 = import_df[['Product Desc', 'Cleaned_HS_Code']]\n",
    "df1.columns = ['Desc', 'HSCode']\n",
    "print(df1.head(2))\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sitting-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HSCode                                               Desc\n",
      "193322  150110  Pig fat (including lard) and poultry fat, othe...\n",
      "485667  150120  Pig fat (including lard) and poultry fat, othe...\n",
      "247\n"
     ]
    }
   ],
   "source": [
    "hs_code_desc = import_df[['Cleaned_HS_Code', 'Merged_Description']]\n",
    "hs_code_desc = hs_code_desc.drop_duplicates()\n",
    "hs_code_desc.columns = ['HSCode', 'Desc']\n",
    "print(hs_code_desc.head(2))\n",
    "print(len(hs_code_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annual-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105113"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.append(hs_code_desc[['Desc', 'HSCode']]).reset_index()\n",
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "respected-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframes back to pandas\n",
    "\n",
    "# df1_pd = df1.compute()\n",
    "# len(df1_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sustainable-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_pd = df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-stuff",
   "metadata": {},
   "source": [
    "Remove long number sequences (that potentially contain HS Codes) from the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "oriented-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_pd['Desc'] = [re.sub('\\d{4,}', '', x) for x in df1_pd['Desc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-guard",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Let's tokenize the description fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-software",
   "metadata": {},
   "source": [
    "#### Create embeddings with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(data, column_name):\n",
    "#     '''\n",
    "#     Tokenize text\n",
    "#     '''\n",
    "#     tokens = data[column_name].apply((lambda x: nltk.word_tokenize(x)))\n",
    "    \n",
    "#     data['NLTK'+column_name] = np.array(tokens)\n",
    "    \n",
    "#     return data\n",
    "# #     return list(\n",
    "# #         filter(lambda word: word.isalnum(), tokens)\n",
    "# #     )\n",
    "\n",
    "# stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# def remove_stopwords(words):\n",
    "#     '''\n",
    "#     Remove stop words from the list of words\n",
    "#     '''\n",
    "    \n",
    "#     filtered = filter(lambda word: word not in stop_words, words)\n",
    "    \n",
    "#     return list(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-imperial",
   "metadata": {},
   "source": [
    "#### Create embeddings with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pre_process(data, column_name, max_length):\n",
    "#     \"\"\"\n",
    "#     Function takes inputs:\n",
    "#     - data in the form of a pandas dataframe\n",
    "#     - column_name containing the text to be embedded\n",
    "#     - max length\n",
    "#     and produces as output the input data BERT requires as an array consisting of:\n",
    "#     - Sentence IDs padded to the max length\n",
    "#     - BERT Masks that tell BERT which of the Sentence IDs are 0 and should be ignored\n",
    "#     - SequenceIDs which are all 0 for our classification task\n",
    "#     \"\"\"\n",
    "#     # Tokenize each item and add the special beginning/end tokens\n",
    "#     tokenized = data[column_name].apply((lambda x: tokenizer.encode(x, add_special_tokens=False, max_length=max_length, truncation=True)))\n",
    "#     data['BERT_'+column_name] = np.array(tokenized)\n",
    "# #     data['BERT_'+column_name] = tokenized\n",
    "      \n",
    "# #     # Create the padding based on the max length so all are same shape\n",
    "# #     bertSentenceIDs = np.array([i + [0]*(max_length-len(i)) for i in tokenized.values])\n",
    "    \n",
    "# #     # Create the attention mask so BERT knows which contain values and which are 0s that should be ignored\n",
    "# #     bertMasks = np.where(bertSentenceIDs != 0, 1, 0)\n",
    "\n",
    "# #     # Create the BERT sequence IDs. In this case they are all 0 since it's the same sentence input.\n",
    "# #     bertSequenceIDs = np.array([np.zeros(max_length) for i in tokenized.values], dtype=int)\n",
    "    \n",
    "# #     # Create and return the data array containing both the padded and the attention mask\n",
    "# #     X_data = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])\n",
    "\n",
    "# #     # Also look at the vocabulary size in the tokenizer\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-tunnel",
   "metadata": {},
   "source": [
    "In the past we have run into memory issues depending on the length of the input, so we set up a variable to truncate the tokens being input for each record.\n",
    "\n",
    "First, let's check the max length of the different tokenized columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_lst = ['Product Desc', 'Merged_Description']\n",
    "\n",
    "# max_length_dict = {}\n",
    "\n",
    "# for l in length_lst:\n",
    "#     tokenized = []\n",
    "#     tokenized = import_df_pd[l].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "#     # Find the max length for the tokenized examples\n",
    "#     max_length = 0\n",
    "#     for i in tokenized.values:\n",
    "#         if len(i) > max_length:\n",
    "#             max_length = len(i)\n",
    "            \n",
    "\n",
    "#     print('Max length of column', l, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-bathroom",
   "metadata": {},
   "source": [
    "This could be a problem if we have very, very short entries in the description fields.\n",
    "\n",
    "Some possible solutions:\n",
    "1. Augment each entry with the dictionary definition.\n",
    "2. Add extra training records of just the dictionary definition.\n",
    "3. Train on only the dictionary definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-celebrity",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Encode the description fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 180\n",
    "\n",
    "# # Create BERT embeddings for Product Desc and Merged_Description, append to pandas dataframe\n",
    "# X_pd = pre_process(df1_pd, 'Desc', max_length)\n",
    "\n",
    "\n",
    "\n",
    "# # Create NLTK embeddings for Product Desc and Merged_Description, append to pandas dataframe\n",
    "# X_pd = tokenize(X_pd, 'Desc')\n",
    "\n",
    "# X_pd.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-heath",
   "metadata": {},
   "source": [
    "### Create train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "juvenile-hunter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pd = df1_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "apparent-obligation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pd = X_pd['HSCode']\n",
    "type(y_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "young-option",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the train set is 84090\n",
      "Size of the dev set is 21023\n",
      "Size of the test set is 21023\n",
      "Size of the train label set is 84090\n",
      "Size of the dev label set is 21023\n",
      "Size of the test label set is 21023\n"
     ]
    }
   ],
   "source": [
    "# X = X_pd['Desc']\n",
    "y = X_pd['HSCode']\n",
    "\n",
    "# Split once to create the test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_pd, y, test_size=0.2, random_state=91)\n",
    "\n",
    "# Re-split the train set to create a dev set\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_pd, y, test_size=0.2, random_state=91, stratify=y)\n",
    "\n",
    "print('Size of the train set is', len(X_train))\n",
    "print('Size of the dev set is', len(X_dev))\n",
    "print('Size of the test set is', len(X_test))\n",
    "print('Size of the train label set is', len(y_train))\n",
    "print('Size of the dev label set is', len(y_dev))\n",
    "print('Size of the test label set is', len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "narrative-packing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Desc</th>\n",
       "      <th>HSCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52001</th>\n",
       "      <td>373089</td>\n",
       "      <td>RICE FLOUR (REGULAR) 1,600 CARTONS RICE FLOUR (REGULAR) H.S.CODE .90. N.W. 17,433.600 KGS., G.W. 18,560.000 KGS. 1,600 CARTONS RICE FLOUR (GLUTINOUS) H.S.CODE .90. N.W. 17,433.600 KGS., G.W. 18,560.000 KGS. INTENDED TRANSHIPMENT AT VIETNAM BY ESSEN EXPRESS V.027E&lt;br/&gt;RICE FLOUR (REGULAR) 1,600 CARTONS RICE FLOUR (REGULAR) H.S.CODE .90. N.W. 17,433.600 KGS., G.W. 18,560.000 KGS. 1,600 CARTONS RICE FLOUR (GLUTINOUS) H.S.CODE .90. N.W. 17,433.600 KGS., G.W. 18,560.000 KGS. INTENDED TRANSHIPMENT AT VIETNAM BY ESSEN EXPRESS V.027E&lt;br/&gt;</td>\n",
       "      <td>110290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45082</th>\n",
       "      <td>228105</td>\n",
       "      <td>HELLO KITTY RICE CRACKERS-STRAWBERRY SENBEI&lt;br/&gt;</td>\n",
       "      <td>100640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  \\\n",
       "52001  373089   \n",
       "45082  228105   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Desc  \\\n",
       "52001  RICE FLOUR (REGULAR) 1,600 CARTONS RICE FLOUR (REGULAR) H.S.CODE .90. N.W. 17,433.600 KGS., G.W. 18,560.000 KGS. 1,600 CARTONS RICE FLOUR (GLUTINOUS) H.S.CODE .90. N.W. 17,433.600 KGS., G.W. 18,560.000 KGS. INTENDED TRANSHIPMENT AT VIETNAM BY ESSEN EXPRESS V.027E<br/>RICE FLOUR (REGULAR) 1,600 CARTONS RICE FLOUR (REGULAR) H.S.CODE .90. N.W. 17,433.600 KGS., G.W. 18,560.000 KGS. 1,600 CARTONS RICE FLOUR (GLUTINOUS) H.S.CODE .90. N.W. 17,433.600 KGS., G.W. 18,560.000 KGS. INTENDED TRANSHIPMENT AT VIETNAM BY ESSEN EXPRESS V.027E<br/>   \n",
       "45082  HELLO KITTY RICE CRACKERS-STRAWBERRY SENBEI<br/>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "\n",
       "       HSCode  \n",
       "52001  110290  \n",
       "45082  100640  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "exempt-spare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Desc</th>\n",
       "      <th>HSCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38595</th>\n",
       "      <td>640868</td>\n",
       "      <td>FROZEN SUGAR WAFFLE NET WEIGHT:  KGS PO -001 / WILLIAMSPORT TEMP RECORDER:  FREIGHT PREPAID&lt;br/&gt;</td>\n",
       "      <td>121291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58356</th>\n",
       "      <td>522587</td>\n",
       "      <td>760 BAGS MALT SHIPPERS REF A 449 BSG PO REF PO160- HS-CODE    ALL CHARGES PREPAID INCL DTHC CHASSIS NOT REQUIRED&lt;br/&gt;</td>\n",
       "      <td>110720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  \\\n",
       "38595  640868   \n",
       "58356  522587   \n",
       "\n",
       "                                                                                                                        Desc  \\\n",
       "38595  FROZEN SUGAR WAFFLE NET WEIGHT:  KGS PO -001 / WILLIAMSPORT TEMP RECORDER:  FREIGHT PREPAID<br/>                        \n",
       "58356  760 BAGS MALT SHIPPERS REF A 449 BSG PO REF PO160- HS-CODE    ALL CHARGES PREPAID INCL DTHC CHASSIS NOT REQUIRED<br/>   \n",
       "\n",
       "       HSCode  \n",
       "38595  121291  \n",
       "58356  110720  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-check",
   "metadata": {},
   "source": [
    "## Create baseline of predicting the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "raising-boundary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when predicting majority class  0.00952550838387442\n"
     ]
    }
   ],
   "source": [
    "counts = X_train['HSCode'].value_counts().to_dict()\n",
    "# print(counts)\n",
    "max_value = max(counts.values())\n",
    "print('Accuracy when predicting majority class ', max_value/len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-times",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Construct a Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-trademark",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Use BOW on the words in the Product Desc column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer()\n",
    "\n",
    "# fit_transform() creates dictionary and return term-document matrix.\n",
    "X_train_counts = count_vector.fit_transform(X_train['Desc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = MultinomialNB().fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_counts = count_vector.transform(X_dev['Desc'])\n",
    "# X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "# Execute prediction(classification).\n",
    "predicted = clf1.predict(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score:', f1_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Precision:', precision_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Recall:', recall_score(y_dev, predicted, average=\"macro\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(clf1, X_new_counts, y_dev)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-scanning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rising-efficiency",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Use TF-IDF on the words in the Product Desc column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# fit_transform() creates dictionary and return term-document matrix.\n",
    "X_train_counts = count_vector.fit_transform(X_train['Desc'])\n",
    "\n",
    "# Import TfidfTransformer class.\n",
    "# TfidfTransformer transoforms count matrix to tf-idf representation.\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# fit_transform transforms count matrix to tf-idf representation(vector).\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model(naive bayes) and training. \n",
    "\n",
    "clf2 = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions\n",
    "# Transfroming.\n",
    "X_new_counts = count_vector.transform(X_dev['Desc'])\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "# Execute prediction(classification).\n",
    "predicted = clf2.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score:', f1_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Precision:', precision_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Recall:', recall_score(y_dev, predicted, average=\"macro\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_dev, predicted)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(clf2, X_new_tfidf, y_dev)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-forest",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Use BOW on the Merged Description field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform() creates dictionary and return term-document matrix.\n",
    "X_train_counts = count_vector.fit_transform(X_train['Merged_Description'])\n",
    "\n",
    "# # Import TfidfTransformer class.\n",
    "# # TfidfTransformer transoforms count matrix to tf-idf representation.\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# # fit_transform transforms count matrix to tf-idf representation(vector).\n",
    "# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model(naive bayes) and training. \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf3 = MultinomialNB().fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions\n",
    "# Transfroming.\n",
    "X_new_counts = count_vector.transform(X_dev['Merged_Description'])\n",
    "# X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "# Execute prediction(classification).\n",
    "predicted = clf3.predict(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score:', f1_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Precision:', precision_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Recall:', recall_score(y_dev, predicted, average=\"macro\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(clf3, X_new_counts, y_dev)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-knock",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Use TF-IDF on the Merged Description field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform() creates dictionary and return term-document matrix.\n",
    "X_train_counts = count_vector.fit_transform(X_train['Merged_Description'])\n",
    "\n",
    "# Import TfidfTransformer class.\n",
    "# TfidfTransformer transoforms count matrix to tf-idf representation.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# fit_transform transforms count matrix to tf-idf representation(vector).\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model(naive bayes) and training. \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf2 = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions\n",
    "# Transfroming.\n",
    "X_new_counts = count_vector.transform(X_dev['Merged_Description'])\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "# Execute prediction(classification).\n",
    "predicted = clf2.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score:', f1_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Precision:', precision_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Recall:', recall_score(y_dev, predicted, average=\"macro\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(clf2, X_new_tfidf, y_dev)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_dev, predicted)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-fossil",
   "metadata": {},
   "source": [
    "### Use the BERT embeddings in the Product Desc column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embedding column from list to string\n",
    "X_train['BERTProduct Desc'] = X_train['BERTProduct Desc'].apply(str).apply(', '.join)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer()\n",
    "\n",
    "# fit_transform() creates dictionary and return term-document matrix.\n",
    "X_train_counts = count_vector.fit_transform(X_train['Product Desc'])\n",
    "\n",
    "# Import TfidfTransformer class.\n",
    "# TfidfTransformer transoforms count matrix to tf-idf representation.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# fit_transform transforms count matrix to tf-idf representation(vector).\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model(naive bayes) and training. \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions\n",
    "# Transfroming.\n",
    "X_new_counts = count_vector.transform(X_dev['Product Desc'])\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "# Execute prediction(classification).\n",
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score:', f1_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Precision:', precision_score(y_dev, predicted, average=\"macro\"))\n",
    "print('Recall:', recall_score(y_dev, predicted, average=\"macro\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "plot_confusion_matrix(clf, X_new_tfidf, y_dev)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-parade",
   "metadata": {},
   "source": [
    "## Create functions to make model building easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "beautiful-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results = pd.DataFrame(index = ['Baseline', 'NB-BOW Desc','NB-tfidf Desc', 'KNN-BOW Desc', 'KNN-tfidf Desc', 'LogReg-BOW Desc', 'LogReg-tfidf Desc', \n",
    "                                'SVM-BOW Desc', 'SVM-tfidf Desc', 'XGBoost-BOW Desc', 'XGBoost-tfidf Desc', 'RF-BOW Desc', 'RF-tfidf Desc'],\n",
    "                       columns=['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "signed-judges",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB-BOW Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB-tfidf Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN-BOW Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN-tfidf Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg-BOW Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg-tfidf Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-BOW Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-tfidf Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost-BOW Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost-tfidf Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-BOW Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-tfidf Desc</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Accuracy\n",
       "Baseline                NaN\n",
       "NB-BOW Desc             NaN\n",
       "NB-tfidf Desc           NaN\n",
       "KNN-BOW Desc            NaN\n",
       "KNN-tfidf Desc          NaN\n",
       "LogReg-BOW Desc         NaN\n",
       "LogReg-tfidf Desc       NaN\n",
       "SVM-BOW Desc            NaN\n",
       "SVM-tfidf Desc          NaN\n",
       "XGBoost-BOW Desc        NaN\n",
       "XGBoost-tfidf Desc      NaN\n",
       "RF-BOW Desc             NaN\n",
       "RF-tfidf Desc           NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "infectious-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.at['Baseline', 'Accuracy'] = max_value/len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-rover",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aquatic-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, X, y, X_test, y_test, filename):\n",
    "    ### provide classifier, train and test set\n",
    "    ### get train/val split\n",
    "    ### fit on val\n",
    "    ### test on test\n",
    "    ### return accuracy score for test\n",
    "    tic = time()\n",
    "    mod = classifier.fit(X, y)\n",
    "    toc = time()\n",
    "    print(f\"Trained model in {toc - tic:0.4} seconds\")\n",
    "    \n",
    "    # save model parameters\n",
    "#     filename = classifier([('vectorizer')])+'_model.sav'\n",
    "    pickle.dump(mod, open(filename, 'wb'))\n",
    "    \n",
    "    print(\"Dev set results:\")\n",
    "    tic = time()\n",
    "    X_test_preds = mod.predict(X_test)\n",
    "    toc = time()\n",
    "    print(classification_report(y_test, X_test_preds) )\n",
    "    # plot confusion matrix\n",
    "#     plt.figure(figsize=(30,30))\n",
    "#     plot_confusion_matrix(mod, y_test, X_test_preds)\n",
    "#     plt.show()\n",
    "    # print confusion matrix\n",
    "    print(confusion_matrix(y_test, X_test_preds))\n",
    "    print(f\"Created predictions in {toc - tic:0.4} seconds\")\n",
    "    return accuracy_score(y_test,X_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "former-proposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 7.347 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       0.00      0.00      0.00         4\n",
      "      100199       0.68      0.99      0.81        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       0.00      0.00      0.00         2\n",
      "      100410       1.00      0.73      0.84        11\n",
      "      100490       0.96      0.68      0.79        37\n",
      "      100510       0.90      0.84      0.87        56\n",
      "      100590       0.89      0.56      0.69        73\n",
      "      100610       0.87      0.52      0.65        75\n",
      "      100620       0.79      0.69      0.74       200\n",
      "      100630       0.53      0.92      0.67       200\n",
      "      100640       0.78      0.58      0.67       200\n",
      "      100810       1.00      0.05      0.10        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       0.00      0.00      0.00         4\n",
      "      100830       0.74      0.77      0.76       180\n",
      "      100840       0.00      0.00      0.00         1\n",
      "      100850       0.78      0.93      0.85       179\n",
      "      100890       1.00      0.44      0.61        96\n",
      "      110100       0.59      0.78      0.67       200\n",
      "      110220       0.38      0.75      0.50       157\n",
      "      110290       0.63      0.80      0.70       115\n",
      "      110311       0.00      0.00      0.00        20\n",
      "      110313       1.00      0.39      0.56        31\n",
      "      110319       0.66      0.90      0.76       200\n",
      "      110320       0.90      0.29      0.44        66\n",
      "      110412       0.91      0.82      0.87        91\n",
      "      110419       0.68      0.70      0.69        61\n",
      "      110422       0.00      0.00      0.00         6\n",
      "      110423       0.73      0.63      0.68        38\n",
      "      110429       0.00      0.00      0.00        23\n",
      "      110430       0.50      0.14      0.22         7\n",
      "      110510       0.92      0.69      0.79        68\n",
      "      110520       1.00      0.25      0.41        55\n",
      "      110610       0.86      0.37      0.52        86\n",
      "      110620       0.77      0.32      0.45        63\n",
      "      110630       0.84      0.66      0.74       146\n",
      "      110710       0.78      0.72      0.75       200\n",
      "      110720       0.74      0.86      0.79       127\n",
      "      110811       1.00      0.21      0.34        53\n",
      "      110812       0.89      0.40      0.55        63\n",
      "      110813       0.80      0.83      0.82       200\n",
      "      110814       0.66      0.93      0.77       200\n",
      "      110819       0.92      0.35      0.51        63\n",
      "      110820       0.95      0.78      0.86        46\n",
      "      110900       0.69      0.93      0.79       200\n",
      "      120110       1.00      0.20      0.33        20\n",
      "      120190       0.42      0.94      0.58       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       1.00      0.83      0.90        23\n",
      "      120300       0.00      0.00      0.00         8\n",
      "      120400       0.77      0.70      0.73       100\n",
      "      120510       0.89      0.77      0.83        22\n",
      "      120590       0.75      0.38      0.50         8\n",
      "      120600       0.74      0.80      0.77       200\n",
      "      120710       0.89      0.73      0.81       200\n",
      "      120730       1.00      0.22      0.36        32\n",
      "      120740       0.85      0.95      0.89       112\n",
      "      120750       0.69      0.25      0.37        36\n",
      "      120760       0.00      0.00      0.00         3\n",
      "      120791       0.92      0.47      0.62        47\n",
      "      120799       0.80      0.82      0.81       123\n",
      "      120810       0.86      0.58      0.69       177\n",
      "      120890       0.00      0.00      0.00        15\n",
      "      120910       0.00      0.00      0.00        11\n",
      "      120921       1.00      0.33      0.50        33\n",
      "      120922       1.00      0.08      0.15        24\n",
      "      120923       0.00      0.00      0.00        10\n",
      "      120924       0.91      0.48      0.63       200\n",
      "      120925       1.00      0.66      0.79        44\n",
      "      120929       0.83      0.33      0.48        30\n",
      "      120930       1.00      0.68      0.81        25\n",
      "      120991       0.74      0.73      0.73       200\n",
      "      120999       1.00      0.86      0.92        71\n",
      "      121010       0.00      0.00      0.00         7\n",
      "      121020       0.93      0.75      0.83        89\n",
      "      121120       1.00      0.46      0.63        35\n",
      "      121130       1.00      0.50      0.67        40\n",
      "      121140       1.00      0.50      0.67        10\n",
      "      121190       0.62      0.80      0.70       200\n",
      "      121221       0.00      0.00      0.00        14\n",
      "      121229       1.00      0.57      0.72        30\n",
      "      121291       1.00      0.72      0.84        43\n",
      "      121293       0.00      0.00      0.00         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.83      0.17      0.28        60\n",
      "      121300       0.00      0.00      0.00         6\n",
      "      121410       0.84      0.74      0.78       103\n",
      "      121490       0.88      0.96      0.92       173\n",
      "      130120       0.86      0.85      0.86       200\n",
      "      130190       0.95      0.73      0.83        56\n",
      "      130211       1.00      0.10      0.18        31\n",
      "      130212       0.00      0.00      0.00        14\n",
      "      130213       0.00      0.00      0.00         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.73      0.71      0.72       139\n",
      "      130220       1.00      0.10      0.17        21\n",
      "      130231       1.00      0.21      0.35        38\n",
      "      130232       0.51      0.94      0.66       200\n",
      "      130239       0.91      0.60      0.72        70\n",
      "      140110       0.83      0.92      0.87       200\n",
      "      140120       0.96      0.55      0.70        44\n",
      "      140190       1.00      0.28      0.43        18\n",
      "      140420       0.95      0.82      0.88        97\n",
      "      140490       0.97      0.50      0.66        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       0.00      0.00      0.00         5\n",
      "      150290       0.00      0.00      0.00         2\n",
      "      150300       1.00      0.11      0.20         9\n",
      "      150410       0.86      0.55      0.67        33\n",
      "      150420       0.74      0.58      0.65        24\n",
      "      150430       0.00      0.00      0.00         3\n",
      "      150500       1.00      0.66      0.79        93\n",
      "      150600       0.00      0.00      0.00         2\n",
      "      150710       0.82      0.35      0.49        66\n",
      "      150790       0.88      0.65      0.75       135\n",
      "      150810       0.90      0.91      0.90       200\n",
      "      150890       0.86      0.75      0.80         8\n",
      "      150910       0.57      0.84      0.68       200\n",
      "      150990       0.43      0.52      0.47       200\n",
      "      151000       0.75      0.57      0.65       200\n",
      "      151110       0.97      0.97      0.97       200\n",
      "      151190       0.63      0.96      0.76       200\n",
      "      151211       0.96      0.70      0.81        37\n",
      "      151219       0.72      0.79      0.76       107\n",
      "      151221       1.00      0.45      0.62        33\n",
      "      151229       1.00      0.49      0.66        47\n",
      "      151311       1.00      0.50      0.67        24\n",
      "      151319       0.79      0.81      0.80        90\n",
      "      151321       0.00      0.00      0.00         1\n",
      "      151329       0.97      0.43      0.60        74\n",
      "      151411       0.00      0.00      0.00         7\n",
      "      151419       0.85      0.67      0.75        67\n",
      "      151491       0.00      0.00      0.00         4\n",
      "      151499       0.00      0.00      0.00         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       0.00      0.00      0.00        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       0.00      0.00      0.00         4\n",
      "      151530       0.84      0.89      0.86        99\n",
      "      151550       0.96      0.54      0.69        50\n",
      "      151590       0.59      0.86      0.70       191\n",
      "      151610       0.94      0.89      0.91       108\n",
      "      151620       0.81      0.68      0.74        91\n",
      "      151710       1.00      0.19      0.31        27\n",
      "      151790       0.64      0.54      0.58       173\n",
      "      151800       0.79      0.55      0.65        40\n",
      "      152000       0.91      0.91      0.91       131\n",
      "      152110       0.96      0.69      0.81        36\n",
      "      152190       1.00      0.37      0.54        35\n",
      "      152200       1.00      0.60      0.75        10\n",
      "      160100       0.78      0.58      0.67       144\n",
      "      160210       1.00      0.18      0.31        11\n",
      "      160220       0.99      0.85      0.92        89\n",
      "      160231       1.00      0.38      0.55        42\n",
      "      160232       0.00      0.00      0.00        12\n",
      "      160239       0.00      0.00      0.00         6\n",
      "      160241       0.98      0.92      0.95        86\n",
      "      160242       0.94      0.70      0.80       200\n",
      "      160249       0.84      0.36      0.50        73\n",
      "      160250       0.74      0.97      0.84       200\n",
      "      160290       0.91      0.24      0.38        41\n",
      "      160300       0.92      0.24      0.39        45\n",
      "      160411       0.59      0.76      0.66       200\n",
      "      160412       0.90      0.51      0.65        93\n",
      "      160413       0.71      0.83      0.76       200\n",
      "      160414       0.61      0.72      0.67       200\n",
      "      160415       0.95      0.54      0.69       100\n",
      "      160416       1.00      0.06      0.12        32\n",
      "      160417       0.00      0.00      0.00         3\n",
      "      160419       0.75      0.67      0.71       129\n",
      "      160420       0.44      0.86      0.59       200\n",
      "      160432       0.00      0.00      0.00         4\n",
      "      160510       0.73      0.75      0.74       200\n",
      "      160521       0.47      0.95      0.63       173\n",
      "      160529       0.59      0.86      0.70       119\n",
      "      160530       1.00      0.38      0.56        39\n",
      "      160540       1.00      0.04      0.08        51\n",
      "      160551       1.00      0.25      0.40         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       1.00      0.16      0.27        19\n",
      "      160554       1.00      0.14      0.24        29\n",
      "      160555       0.00      0.00      0.00        11\n",
      "      160556       0.00      0.00      0.00        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       0.00      0.00      0.00         5\n",
      "      160559       0.50      0.08      0.14        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.94      0.23      0.38        64\n",
      "      170113       0.00      0.00      0.00        17\n",
      "      170114       0.94      0.39      0.56        38\n",
      "      170191       0.44      0.69      0.54       200\n",
      "      170199       0.58      0.93      0.71       182\n",
      "      170211       0.83      0.62      0.71        55\n",
      "      170219       0.71      0.89      0.78       200\n",
      "      170220       0.78      0.51      0.62       191\n",
      "      170230       0.71      0.88      0.79       200\n",
      "      170240       0.35      0.46      0.39       200\n",
      "      170250       1.00      0.43      0.60        89\n",
      "      170260       0.57      0.48      0.53       200\n",
      "      170290       0.70      0.73      0.72       200\n",
      "      170310       0.91      0.59      0.72       130\n",
      "      170390       1.00      0.17      0.29         6\n",
      "      170410       1.00      0.69      0.82        75\n",
      "      170490       0.49      0.49      0.49       200\n",
      "      180100       0.67      0.88      0.76       200\n",
      "      180200       0.91      0.68      0.78        94\n",
      "      180310       0.80      0.82      0.81       188\n",
      "      180320       0.90      0.96      0.93        49\n",
      "      180400       0.64      0.77      0.70       200\n",
      "      180500       0.63      0.86      0.73       200\n",
      "      180610       0.52      0.51      0.51       200\n",
      "      180620       0.28      0.60      0.38       200\n",
      "      180631       0.68      0.69      0.68       189\n",
      "      180632       0.50      0.57      0.53       200\n",
      "      180690       0.55      0.43      0.49       200\n",
      "      190110       0.84      0.87      0.85       200\n",
      "      190120       0.44      0.53      0.48       200\n",
      "      190190       0.33      0.48      0.40       200\n",
      "      190211       0.80      0.36      0.50       113\n",
      "      190219       0.52      0.65      0.57       200\n",
      "      190220       0.88      0.51      0.65       120\n",
      "      190230       0.69      0.57      0.63       200\n",
      "      190240       1.00      0.32      0.48        41\n",
      "      190300       0.77      0.81      0.79       200\n",
      "      190410       0.70      0.65      0.67       200\n",
      "      190420       0.53      0.23      0.32        39\n",
      "      190430       0.88      0.79      0.83       200\n",
      "      190490       0.67      0.75      0.71       177\n",
      "      190510       1.00      0.19      0.32        32\n",
      "      190520       1.00      0.25      0.41        51\n",
      "      190531       0.69      0.60      0.65       200\n",
      "      190532       0.74      0.69      0.71       188\n",
      "      190540       0.82      0.64      0.72       200\n",
      "      190590       0.50      0.57      0.53       200\n",
      "\n",
      "    accuracy                           0.68     21023\n",
      "   macro avg       0.63      0.46      0.50     21023\n",
      "weighted avg       0.73      0.68      0.67     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   0   4 ...   0   0   0]\n",
      " [  0   0  92 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 129   0   2]\n",
      " [  0   0   0 ...   1 127  17]\n",
      " [  0   0   0 ...   4   0 114]]\n",
      "Created predictions in 1.332 seconds\n"
     ]
    }
   ],
   "source": [
    "trial1 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "acc = train(trial1, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/NB_BOW_model_1x.sav')\n",
    "\n",
    "results.at['NB-BOW Desc','Accuracy'] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hungarian-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 7.101 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       0.00      0.00      0.00         4\n",
      "      100199       0.77      0.94      0.84        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       0.00      0.00      0.00         2\n",
      "      100410       1.00      0.45      0.62        11\n",
      "      100490       1.00      0.41      0.58        37\n",
      "      100510       1.00      0.61      0.76        56\n",
      "      100590       1.00      0.41      0.58        73\n",
      "      100610       0.85      0.44      0.58        75\n",
      "      100620       0.81      0.70      0.75       200\n",
      "      100630       0.47      0.93      0.63       200\n",
      "      100640       0.74      0.62      0.68       200\n",
      "      100810       0.00      0.00      0.00        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       0.00      0.00      0.00         4\n",
      "      100830       0.73      0.83      0.78       180\n",
      "      100840       0.00      0.00      0.00         1\n",
      "      100850       0.67      0.97      0.79       179\n",
      "      100890       1.00      0.41      0.58        96\n",
      "      110100       0.57      0.83      0.67       200\n",
      "      110220       0.56      0.82      0.66       157\n",
      "      110290       0.85      0.59      0.70       115\n",
      "      110311       0.00      0.00      0.00        20\n",
      "      110313       1.00      0.29      0.45        31\n",
      "      110319       0.68      0.93      0.79       200\n",
      "      110320       0.94      0.23      0.37        66\n",
      "      110412       0.84      0.87      0.85        91\n",
      "      110419       0.93      0.43      0.58        61\n",
      "      110422       0.00      0.00      0.00         6\n",
      "      110423       0.87      0.34      0.49        38\n",
      "      110429       0.00      0.00      0.00        23\n",
      "      110430       0.00      0.00      0.00         7\n",
      "      110510       0.98      0.72      0.83        68\n",
      "      110520       1.00      0.25      0.41        55\n",
      "      110610       1.00      0.27      0.42        86\n",
      "      110620       0.85      0.27      0.41        63\n",
      "      110630       0.86      0.63      0.73       146\n",
      "      110710       0.78      0.74      0.76       200\n",
      "      110720       0.72      0.87      0.79       127\n",
      "      110811       1.00      0.19      0.32        53\n",
      "      110812       0.84      0.25      0.39        63\n",
      "      110813       0.74      0.90      0.81       200\n",
      "      110814       0.73      0.96      0.83       200\n",
      "      110819       1.00      0.16      0.27        63\n",
      "      110820       0.95      0.78      0.86        46\n",
      "      110900       0.71      0.95      0.81       200\n",
      "      120110       0.00      0.00      0.00        20\n",
      "      120190       0.46      0.94      0.62       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       1.00      0.70      0.82        23\n",
      "      120300       0.00      0.00      0.00         8\n",
      "      120400       0.94      0.62      0.75       100\n",
      "      120510       0.00      0.00      0.00        22\n",
      "      120590       0.00      0.00      0.00         8\n",
      "      120600       0.64      0.89      0.75       200\n",
      "      120710       0.83      0.77      0.80       200\n",
      "      120730       1.00      0.25      0.40        32\n",
      "      120740       0.87      0.93      0.90       112\n",
      "      120750       1.00      0.25      0.40        36\n",
      "      120760       0.00      0.00      0.00         3\n",
      "      120791       0.92      0.47      0.62        47\n",
      "      120799       0.84      0.81      0.83       123\n",
      "      120810       0.83      0.56      0.67       177\n",
      "      120890       0.00      0.00      0.00        15\n",
      "      120910       0.00      0.00      0.00        11\n",
      "      120921       1.00      0.21      0.35        33\n",
      "      120922       1.00      0.04      0.08        24\n",
      "      120923       0.00      0.00      0.00        10\n",
      "      120924       0.90      0.56      0.69       200\n",
      "      120925       1.00      0.34      0.51        44\n",
      "      120929       0.67      0.07      0.12        30\n",
      "      120930       1.00      0.64      0.78        25\n",
      "      120991       0.68      0.81      0.74       200\n",
      "      120999       1.00      0.83      0.91        71\n",
      "      121010       0.00      0.00      0.00         7\n",
      "      121020       0.92      0.81      0.86        89\n",
      "      121120       1.00      0.34      0.51        35\n",
      "      121130       1.00      0.23      0.37        40\n",
      "      121140       1.00      0.50      0.67        10\n",
      "      121190       0.57      0.83      0.68       200\n",
      "      121221       0.00      0.00      0.00        14\n",
      "      121229       1.00      0.57      0.72        30\n",
      "      121291       0.97      0.65      0.78        43\n",
      "      121293       0.00      0.00      0.00         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.75      0.10      0.18        60\n",
      "      121300       0.00      0.00      0.00         6\n",
      "      121410       0.86      0.70      0.77       103\n",
      "      121490       0.88      0.97      0.93       173\n",
      "      130120       0.87      0.85      0.86       200\n",
      "      130190       1.00      0.50      0.67        56\n",
      "      130211       0.00      0.00      0.00        31\n",
      "      130212       0.00      0.00      0.00        14\n",
      "      130213       0.00      0.00      0.00         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.70      0.71      0.70       139\n",
      "      130220       0.00      0.00      0.00        21\n",
      "      130231       1.00      0.45      0.62        38\n",
      "      130232       0.51      0.96      0.67       200\n",
      "      130239       0.93      0.76      0.83        70\n",
      "      140110       0.79      0.95      0.86       200\n",
      "      140120       1.00      0.45      0.62        44\n",
      "      140190       1.00      0.11      0.20        18\n",
      "      140420       0.98      0.91      0.94        97\n",
      "      140490       1.00      0.43      0.60        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       0.00      0.00      0.00         5\n",
      "      150290       0.00      0.00      0.00         2\n",
      "      150300       0.00      0.00      0.00         9\n",
      "      150410       0.88      0.42      0.57        33\n",
      "      150420       1.00      0.04      0.08        24\n",
      "      150430       0.00      0.00      0.00         3\n",
      "      150500       1.00      0.69      0.82        93\n",
      "      150600       0.00      0.00      0.00         2\n",
      "      150710       1.00      0.15      0.26        66\n",
      "      150790       0.88      0.63      0.73       135\n",
      "      150810       0.85      0.93      0.89       200\n",
      "      150890       0.00      0.00      0.00         8\n",
      "      150910       0.59      0.84      0.69       200\n",
      "      150990       0.34      0.55      0.42       200\n",
      "      151000       0.78      0.62      0.69       200\n",
      "      151110       0.87      0.97      0.92       200\n",
      "      151190       0.58      0.95      0.72       200\n",
      "      151211       1.00      0.38      0.55        37\n",
      "      151219       0.80      0.67      0.73       107\n",
      "      151221       1.00      0.33      0.50        33\n",
      "      151229       1.00      0.36      0.53        47\n",
      "      151311       0.00      0.00      0.00        24\n",
      "      151319       0.84      0.84      0.84        90\n",
      "      151321       0.00      0.00      0.00         1\n",
      "      151329       0.97      0.38      0.54        74\n",
      "      151411       0.00      0.00      0.00         7\n",
      "      151419       0.81      0.64      0.72        67\n",
      "      151491       0.00      0.00      0.00         4\n",
      "      151499       0.00      0.00      0.00         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       0.00      0.00      0.00        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       0.00      0.00      0.00         4\n",
      "      151530       0.80      0.90      0.85        99\n",
      "      151550       1.00      0.42      0.59        50\n",
      "      151590       0.55      0.88      0.68       191\n",
      "      151610       0.92      0.91      0.91       108\n",
      "      151620       0.95      0.60      0.74        91\n",
      "      151710       1.00      0.15      0.26        27\n",
      "      151790       0.66      0.61      0.63       173\n",
      "      151800       0.78      0.17      0.29        40\n",
      "      152000       0.88      0.92      0.90       131\n",
      "      152110       0.96      0.64      0.77        36\n",
      "      152190       1.00      0.37      0.54        35\n",
      "      152200       0.00      0.00      0.00        10\n",
      "      160100       0.74      0.62      0.67       144\n",
      "      160210       0.00      0.00      0.00        11\n",
      "      160220       0.94      0.89      0.91        89\n",
      "      160231       1.00      0.29      0.44        42\n",
      "      160232       0.00      0.00      0.00        12\n",
      "      160239       0.00      0.00      0.00         6\n",
      "      160241       0.97      0.91      0.94        86\n",
      "      160242       0.81      0.81      0.81       200\n",
      "      160249       0.90      0.37      0.52        73\n",
      "      160250       0.52      0.98      0.68       200\n",
      "      160290       1.00      0.22      0.36        41\n",
      "      160300       1.00      0.11      0.20        45\n",
      "      160411       0.67      0.84      0.75       200\n",
      "      160412       0.89      0.53      0.66        93\n",
      "      160413       0.65      0.85      0.73       200\n",
      "      160414       0.60      0.80      0.69       200\n",
      "      160415       0.96      0.54      0.69       100\n",
      "      160416       0.00      0.00      0.00        32\n",
      "      160417       0.00      0.00      0.00         3\n",
      "      160419       0.79      0.65      0.71       129\n",
      "      160420       0.47      0.90      0.61       200\n",
      "      160432       0.00      0.00      0.00         4\n",
      "      160510       0.74      0.84      0.79       200\n",
      "      160521       0.41      0.96      0.58       173\n",
      "      160529       0.79      0.76      0.78       119\n",
      "      160530       1.00      0.28      0.44        39\n",
      "      160540       1.00      0.02      0.04        51\n",
      "      160551       0.00      0.00      0.00         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       0.00      0.00      0.00        19\n",
      "      160554       1.00      0.07      0.13        29\n",
      "      160555       0.00      0.00      0.00        11\n",
      "      160556       0.00      0.00      0.00        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       0.00      0.00      0.00         5\n",
      "      160559       0.00      0.00      0.00        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       1.00      0.17      0.29        64\n",
      "      170113       0.00      0.00      0.00        17\n",
      "      170114       1.00      0.24      0.38        38\n",
      "      170191       0.45      0.68      0.54       200\n",
      "      170199       0.63      0.92      0.75       182\n",
      "      170211       0.94      0.56      0.70        55\n",
      "      170219       0.72      0.90      0.80       200\n",
      "      170220       0.83      0.48      0.61       191\n",
      "      170230       0.60      0.91      0.72       200\n",
      "      170240       0.36      0.44      0.40       200\n",
      "      170250       1.00      0.43      0.60        89\n",
      "      170260       0.53      0.56      0.54       200\n",
      "      170290       0.63      0.73      0.68       200\n",
      "      170310       0.97      0.58      0.72       130\n",
      "      170390       0.00      0.00      0.00         6\n",
      "      170410       1.00      0.73      0.85        75\n",
      "      170490       0.61      0.43      0.50       200\n",
      "      180100       0.67      0.92      0.78       200\n",
      "      180200       0.94      0.62      0.74        94\n",
      "      180310       0.82      0.86      0.84       188\n",
      "      180320       1.00      0.61      0.76        49\n",
      "      180400       0.67      0.80      0.73       200\n",
      "      180500       0.65      0.90      0.75       200\n",
      "      180610       0.65      0.55      0.59       200\n",
      "      180620       0.43      0.57      0.49       200\n",
      "      180631       0.58      0.76      0.66       189\n",
      "      180632       0.65      0.48      0.55       200\n",
      "      180690       0.56      0.45      0.50       200\n",
      "      190110       0.81      0.89      0.85       200\n",
      "      190120       0.59      0.48      0.53       200\n",
      "      190190       0.49      0.50      0.50       200\n",
      "      190211       0.94      0.39      0.55       113\n",
      "      190219       0.53      0.74      0.62       200\n",
      "      190220       0.94      0.48      0.64       120\n",
      "      190230       0.70      0.61      0.65       200\n",
      "      190240       1.00      0.24      0.39        41\n",
      "      190300       0.74      0.85      0.80       200\n",
      "      190410       0.66      0.78      0.71       200\n",
      "      190420       0.75      0.15      0.26        39\n",
      "      190430       0.67      0.81      0.73       200\n",
      "      190490       0.73      0.81      0.76       177\n",
      "      190510       1.00      0.09      0.17        32\n",
      "      190520       1.00      0.25      0.41        51\n",
      "      190531       0.64      0.74      0.68       200\n",
      "      190532       0.73      0.73      0.73       188\n",
      "      190540       0.79      0.78      0.78       200\n",
      "      190590       0.49      0.64      0.55       200\n",
      "\n",
      "    accuracy                           0.68     21023\n",
      "   macro avg       0.58      0.42      0.45     21023\n",
      "weighted avg       0.72      0.68      0.66     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   0   4 ...   0   0   0]\n",
      " [  0   0  87 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 137   0   2]\n",
      " [  0   0   0 ...   1 155  15]\n",
      " [  0   0   0 ...   4   2 127]]\n",
      "Created predictions in 1.335 seconds\n"
     ]
    }
   ],
   "source": [
    "trial2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    " \n",
    "acc = train(trial2, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/NB_tfidf_model_1x.sav')\n",
    "\n",
    "results.at['NB-tfidf Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adjacent-party",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 4.452 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       1.00      0.25      0.40         4\n",
      "      100199       0.89      0.97      0.93        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       0.00      0.00      0.00         2\n",
      "      100410       0.80      0.73      0.76        11\n",
      "      100490       0.52      0.92      0.67        37\n",
      "      100510       0.83      0.88      0.85        56\n",
      "      100590       0.56      0.79      0.66        73\n",
      "      100610       0.72      0.84      0.78        75\n",
      "      100620       0.80      0.86      0.83       200\n",
      "      100630       0.78      0.80      0.79       200\n",
      "      100640       0.59      0.89      0.71       200\n",
      "      100810       0.62      0.80      0.70        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       0.67      0.50      0.57         4\n",
      "      100830       0.69      0.91      0.79       180\n",
      "      100840       1.00      1.00      1.00         1\n",
      "      100850       0.88      0.95      0.91       179\n",
      "      100890       0.80      0.90      0.84        96\n",
      "      110100       0.53      0.82      0.65       200\n",
      "      110220       0.64      0.85      0.73       157\n",
      "      110290       0.71      0.71      0.71       115\n",
      "      110311       0.22      0.55      0.32        20\n",
      "      110313       0.67      0.77      0.72        31\n",
      "      110319       0.81      0.91      0.86       200\n",
      "      110320       0.72      0.91      0.81        66\n",
      "      110412       0.81      0.96      0.88        91\n",
      "      110419       0.76      0.77      0.76        61\n",
      "      110422       1.00      0.67      0.80         6\n",
      "      110423       0.88      0.76      0.82        38\n",
      "      110429       0.69      0.39      0.50        23\n",
      "      110430       1.00      0.43      0.60         7\n",
      "      110510       0.80      0.75      0.77        68\n",
      "      110520       0.72      0.85      0.78        55\n",
      "      110610       0.62      0.56      0.59        86\n",
      "      110620       0.63      0.75      0.68        63\n",
      "      110630       0.81      0.82      0.81       146\n",
      "      110710       0.57      0.91      0.70       200\n",
      "      110720       0.46      0.37      0.41       127\n",
      "      110811       0.84      0.77      0.80        53\n",
      "      110812       0.78      0.78      0.78        63\n",
      "      110813       0.87      0.90      0.88       200\n",
      "      110814       0.82      0.91      0.86       200\n",
      "      110819       0.79      0.78      0.78        63\n",
      "      110820       0.88      0.93      0.91        46\n",
      "      110900       0.91      0.94      0.93       200\n",
      "      120110       0.95      0.90      0.92        20\n",
      "      120190       0.84      0.73      0.78       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.86      0.83      0.84        23\n",
      "      120300       1.00      0.62      0.77         8\n",
      "      120400       0.76      0.82      0.79       100\n",
      "      120510       0.92      0.50      0.65        22\n",
      "      120590       0.50      0.50      0.50         8\n",
      "      120600       0.85      0.89      0.87       200\n",
      "      120710       0.78      0.90      0.83       200\n",
      "      120730       0.84      0.97      0.90        32\n",
      "      120740       0.91      0.84      0.87       112\n",
      "      120750       0.94      0.83      0.88        36\n",
      "      120760       0.00      0.00      0.00         3\n",
      "      120791       0.82      0.87      0.85        47\n",
      "      120799       0.88      0.81      0.85       123\n",
      "      120810       0.75      0.89      0.81       177\n",
      "      120890       0.82      0.60      0.69        15\n",
      "      120910       0.73      0.73      0.73        11\n",
      "      120921       0.54      0.76      0.63        33\n",
      "      120922       0.62      0.21      0.31        24\n",
      "      120923       1.00      0.40      0.57        10\n",
      "      120924       0.57      0.66      0.61       200\n",
      "      120925       0.97      0.73      0.83        44\n",
      "      120929       0.85      0.73      0.79        30\n",
      "      120930       0.62      0.84      0.71        25\n",
      "      120991       0.79      0.81      0.80       200\n",
      "      120999       0.90      0.86      0.88        71\n",
      "      121010       0.00      0.00      0.00         7\n",
      "      121020       0.78      0.85      0.82        89\n",
      "      121120       0.84      0.77      0.81        35\n",
      "      121130       0.80      0.90      0.85        40\n",
      "      121140       0.89      0.80      0.84        10\n",
      "      121190       0.70      0.67      0.68       200\n",
      "      121221       0.47      0.64      0.55        14\n",
      "      121229       0.85      0.77      0.81        30\n",
      "      121291       0.86      0.98      0.91        43\n",
      "      121293       0.50      0.50      0.50         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.62      0.50      0.56        60\n",
      "      121300       0.50      0.17      0.25         6\n",
      "      121410       0.85      0.88      0.87       103\n",
      "      121490       0.94      0.93      0.93       173\n",
      "      130120       0.93      0.92      0.92       200\n",
      "      130190       0.85      0.84      0.85        56\n",
      "      130211       0.71      0.94      0.81        31\n",
      "      130212       0.55      0.86      0.67        14\n",
      "      130213       0.62      0.62      0.62         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.78      0.77      0.78       139\n",
      "      130220       0.59      0.90      0.72        21\n",
      "      130231       0.38      0.97      0.55        38\n",
      "      130232       0.90      0.92      0.91       200\n",
      "      130239       0.73      0.74      0.74        70\n",
      "      140110       0.79      0.94      0.85       200\n",
      "      140120       0.89      0.57      0.69        44\n",
      "      140190       0.88      0.78      0.82        18\n",
      "      140420       0.79      0.98      0.87        97\n",
      "      140490       0.88      0.74      0.80        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       1.00      0.40      0.57         5\n",
      "      150290       1.00      0.50      0.67         2\n",
      "      150300       1.00      0.56      0.71         9\n",
      "      150410       0.80      0.73      0.76        33\n",
      "      150420       0.90      0.79      0.84        24\n",
      "      150430       1.00      0.67      0.80         3\n",
      "      150500       0.87      0.94      0.90        93\n",
      "      150600       1.00      0.50      0.67         2\n",
      "      150710       0.63      0.68      0.66        66\n",
      "      150790       0.73      0.76      0.74       135\n",
      "      150810       0.83      0.99      0.90       200\n",
      "      150890       0.60      0.75      0.67         8\n",
      "      150910       0.77      0.78      0.78       200\n",
      "      150990       0.65      0.65      0.65       200\n",
      "      151000       0.71      0.82      0.76       200\n",
      "      151110       0.98      0.97      0.97       200\n",
      "      151190       0.84      0.94      0.88       200\n",
      "      151211       0.83      0.78      0.81        37\n",
      "      151219       0.71      0.75      0.73       107\n",
      "      151221       0.76      0.76      0.76        33\n",
      "      151229       0.90      0.79      0.84        47\n",
      "      151311       0.78      0.58      0.67        24\n",
      "      151319       0.91      0.88      0.89        90\n",
      "      151321       0.00      0.00      0.00         1\n",
      "      151329       0.96      0.88      0.92        74\n",
      "      151411       0.83      0.71      0.77         7\n",
      "      151419       0.84      0.79      0.82        67\n",
      "      151491       1.00      0.50      0.67         4\n",
      "      151499       0.00      0.00      0.00         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       0.83      0.50      0.62        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       0.00      0.00      0.00         4\n",
      "      151530       0.89      0.92      0.91        99\n",
      "      151550       0.85      0.90      0.87        50\n",
      "      151590       0.82      0.84      0.83       191\n",
      "      151610       0.94      0.94      0.94       108\n",
      "      151620       0.88      0.74      0.80        91\n",
      "      151710       0.72      0.67      0.69        27\n",
      "      151790       0.81      0.71      0.76       173\n",
      "      151800       0.79      0.68      0.73        40\n",
      "      152000       0.84      0.95      0.89       131\n",
      "      152110       0.84      0.89      0.86        36\n",
      "      152190       0.80      0.91      0.85        35\n",
      "      152200       0.88      0.70      0.78        10\n",
      "      160100       0.64      0.86      0.73       144\n",
      "      160210       1.00      0.36      0.53        11\n",
      "      160220       0.83      0.93      0.88        89\n",
      "      160231       0.85      0.79      0.81        42\n",
      "      160232       0.50      0.42      0.45        12\n",
      "      160239       1.00      0.67      0.80         6\n",
      "      160241       0.91      0.92      0.91        86\n",
      "      160242       0.83      0.91      0.87       200\n",
      "      160249       0.80      0.81      0.80        73\n",
      "      160250       0.94      0.96      0.95       200\n",
      "      160290       0.64      0.66      0.65        41\n",
      "      160300       0.82      0.69      0.75        45\n",
      "      160411       0.83      0.88      0.85       200\n",
      "      160412       0.67      0.71      0.69        93\n",
      "      160413       0.81      0.84      0.82       200\n",
      "      160414       0.84      0.70      0.77       200\n",
      "      160415       0.80      0.92      0.86       100\n",
      "      160416       0.85      0.53      0.65        32\n",
      "      160417       0.00      0.00      0.00         3\n",
      "      160419       0.80      0.80      0.80       129\n",
      "      160420       0.88      0.80      0.84       200\n",
      "      160432       0.00      0.00      0.00         4\n",
      "      160510       0.91      0.83      0.87       200\n",
      "      160521       0.96      0.90      0.93       173\n",
      "      160529       0.89      0.86      0.87       119\n",
      "      160530       0.86      0.82      0.84        39\n",
      "      160540       0.78      0.88      0.83        51\n",
      "      160551       0.50      0.50      0.50         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       1.00      0.53      0.69        19\n",
      "      160554       0.67      0.69      0.68        29\n",
      "      160555       0.83      0.45      0.59        11\n",
      "      160556       0.75      0.23      0.35        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       0.67      0.80      0.73         5\n",
      "      160559       0.67      0.17      0.27        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.53      0.39      0.45        64\n",
      "      170113       0.67      0.12      0.20        17\n",
      "      170114       0.73      0.71      0.72        38\n",
      "      170191       0.74      0.59      0.66       200\n",
      "      170199       0.89      0.83      0.86       182\n",
      "      170211       0.89      0.76      0.82        55\n",
      "      170219       0.86      0.94      0.90       200\n",
      "      170220       0.77      0.58      0.66       191\n",
      "      170230       0.93      0.89      0.91       200\n",
      "      170240       0.65      0.39      0.49       200\n",
      "      170250       0.87      0.89      0.88        89\n",
      "      170260       0.71      0.56      0.63       200\n",
      "      170290       0.94      0.67      0.78       200\n",
      "      170310       0.82      0.92      0.86       130\n",
      "      170390       1.00      0.67      0.80         6\n",
      "      170410       0.89      0.79      0.84        75\n",
      "      170490       0.77      0.42      0.54       200\n",
      "      180100       0.90      0.85      0.88       200\n",
      "      180200       0.88      0.87      0.88        94\n",
      "      180310       0.88      0.91      0.90       188\n",
      "      180320       0.94      0.92      0.93        49\n",
      "      180400       0.92      0.85      0.89       200\n",
      "      180500       0.81      0.90      0.85       200\n",
      "      180610       0.69      0.47      0.56       200\n",
      "      180620       0.67      0.48      0.56       200\n",
      "      180631       0.68      0.74      0.71       189\n",
      "      180632       0.82      0.42      0.55       200\n",
      "      180690       0.49      0.53      0.51       200\n",
      "      190110       0.86      0.91      0.88       200\n",
      "      190120       0.88      0.51      0.65       200\n",
      "      190190       0.76      0.40      0.52       200\n",
      "      190211       0.55      0.88      0.68       113\n",
      "      190219       0.74      0.69      0.72       200\n",
      "      190220       0.84      0.78      0.81       120\n",
      "      190230       0.80      0.71      0.75       200\n",
      "      190240       0.84      0.63      0.72        41\n",
      "      190300       0.87      0.89      0.88       200\n",
      "      190410       0.69      0.72      0.71       200\n",
      "      190420       0.82      0.46      0.59        39\n",
      "      190430       0.93      0.82      0.87       200\n",
      "      190490       0.88      0.80      0.84       177\n",
      "      190510       0.62      0.62      0.62        32\n",
      "      190520       0.77      0.71      0.73        51\n",
      "      190531       0.71      0.74      0.73       200\n",
      "      190532       0.72      0.73      0.72       188\n",
      "      190540       0.75      0.84      0.79       200\n",
      "      190590       0.73      0.47      0.57       200\n",
      "\n",
      "    accuracy                           0.78     21023\n",
      "   macro avg       0.71      0.66      0.67     21023\n",
      "weighted avg       0.79      0.78      0.78     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   1   2 ...   0   0   0]\n",
      " [  0   0  90 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 137   0   3]\n",
      " [  0   0   0 ...   0 168   4]\n",
      " [  0   0   0 ...   1   6  94]]\n",
      "Created predictions in 83.62 seconds\n"
     ]
    }
   ],
   "source": [
    "trial3 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', KNeighborsClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(trial3, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/KNN_BOW_model_1x.sav')\n",
    "\n",
    "results.at['KNN-BOW Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "african-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 4.412 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       0.67      0.50      0.57         4\n",
      "      100199       0.74      0.96      0.84        93\n",
      "      100210       1.00      1.00      1.00         1\n",
      "      100290       0.50      0.50      0.50         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       0.00      0.00      0.00         2\n",
      "      100410       0.91      0.91      0.91        11\n",
      "      100490       0.92      0.92      0.92        37\n",
      "      100510       0.81      0.89      0.85        56\n",
      "      100590       0.58      0.84      0.68        73\n",
      "      100610       0.69      0.87      0.77        75\n",
      "      100620       0.66      0.72      0.69       200\n",
      "      100630       0.81      0.77      0.79       200\n",
      "      100640       0.52      0.76      0.62       200\n",
      "      100810       0.60      0.90      0.72        20\n",
      "      100821       1.00      0.50      0.67         2\n",
      "      100829       0.75      0.75      0.75         4\n",
      "      100830       0.78      0.87      0.82       180\n",
      "      100840       1.00      1.00      1.00         1\n",
      "      100850       0.90      0.96      0.92       179\n",
      "      100890       0.75      0.93      0.83        96\n",
      "      110100       0.66      0.85      0.74       200\n",
      "      110220       0.69      0.85      0.76       157\n",
      "      110290       0.76      0.77      0.77       115\n",
      "      110311       0.80      0.60      0.69        20\n",
      "      110313       0.69      0.81      0.75        31\n",
      "      110319       0.80      0.88      0.83       200\n",
      "      110320       0.75      0.88      0.81        66\n",
      "      110412       0.90      0.92      0.91        91\n",
      "      110419       0.74      0.80      0.77        61\n",
      "      110422       0.44      0.67      0.53         6\n",
      "      110423       0.76      0.89      0.82        38\n",
      "      110429       0.53      0.43      0.48        23\n",
      "      110430       0.67      0.57      0.62         7\n",
      "      110510       0.83      0.85      0.84        68\n",
      "      110520       0.65      0.82      0.73        55\n",
      "      110610       0.62      0.58      0.60        86\n",
      "      110620       0.73      0.68      0.70        63\n",
      "      110630       0.76      0.85      0.80       146\n",
      "      110710       0.62      0.83      0.71       200\n",
      "      110720       0.71      0.45      0.55       127\n",
      "      110811       0.73      0.77      0.75        53\n",
      "      110812       0.76      0.76      0.76        63\n",
      "      110813       0.81      0.84      0.83       200\n",
      "      110814       0.91      0.90      0.90       200\n",
      "      110819       0.75      0.78      0.77        63\n",
      "      110820       0.88      0.93      0.91        46\n",
      "      110900       0.90      0.93      0.92       200\n",
      "      120110       0.95      0.95      0.95        20\n",
      "      120190       0.82      0.76      0.79       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.81      0.96      0.88        23\n",
      "      120300       0.71      0.62      0.67         8\n",
      "      120400       0.82      0.76      0.79       100\n",
      "      120510       0.80      0.73      0.76        22\n",
      "      120590       0.71      0.62      0.67         8\n",
      "      120600       0.77      0.86      0.81       200\n",
      "      120710       0.76      0.82      0.79       200\n",
      "      120730       0.69      0.75      0.72        32\n",
      "      120740       0.85      0.89      0.87       112\n",
      "      120750       0.74      0.86      0.79        36\n",
      "      120760       0.00      0.00      0.00         3\n",
      "      120791       0.87      0.83      0.85        47\n",
      "      120799       0.82      0.88      0.85       123\n",
      "      120810       0.76      0.86      0.81       177\n",
      "      120890       0.77      0.67      0.71        15\n",
      "      120910       0.57      0.73      0.64        11\n",
      "      120921       0.76      0.85      0.80        33\n",
      "      120922       0.80      0.33      0.47        24\n",
      "      120923       1.00      0.30      0.46        10\n",
      "      120924       0.77      0.63      0.69       200\n",
      "      120925       0.80      0.89      0.84        44\n",
      "      120929       0.85      0.73      0.79        30\n",
      "      120930       0.76      0.88      0.81        25\n",
      "      120991       0.74      0.84      0.79       200\n",
      "      120999       0.94      0.89      0.91        71\n",
      "      121010       0.25      0.29      0.27         7\n",
      "      121020       0.79      0.91      0.85        89\n",
      "      121120       0.91      0.89      0.90        35\n",
      "      121130       0.82      0.90      0.86        40\n",
      "      121140       0.89      0.80      0.84        10\n",
      "      121190       0.78      0.74      0.76       200\n",
      "      121221       0.47      0.64      0.55        14\n",
      "      121229       0.96      0.80      0.87        30\n",
      "      121291       0.95      0.98      0.97        43\n",
      "      121293       0.00      0.00      0.00         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.67      0.62      0.64        60\n",
      "      121300       0.60      0.50      0.55         6\n",
      "      121410       0.88      0.89      0.88       103\n",
      "      121490       0.93      0.97      0.95       173\n",
      "      130120       0.86      0.89      0.88       200\n",
      "      130190       0.78      0.91      0.84        56\n",
      "      130211       0.56      0.61      0.58        31\n",
      "      130212       0.68      0.93      0.79        14\n",
      "      130213       0.00      0.00      0.00         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.77      0.84      0.80       139\n",
      "      130220       0.79      0.90      0.84        21\n",
      "      130231       0.77      0.97      0.86        38\n",
      "      130232       0.85      0.92      0.88       200\n",
      "      130239       0.82      0.84      0.83        70\n",
      "      140110       0.81      0.93      0.86       200\n",
      "      140120       0.82      0.75      0.79        44\n",
      "      140190       0.78      0.78      0.78        18\n",
      "      140420       0.85      0.96      0.90        97\n",
      "      140490       0.86      0.71      0.78        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       1.00      0.40      0.57         5\n",
      "      150290       1.00      1.00      1.00         2\n",
      "      150300       0.75      0.67      0.71         9\n",
      "      150410       0.77      0.73      0.75        33\n",
      "      150420       0.80      0.83      0.82        24\n",
      "      150430       1.00      0.67      0.80         3\n",
      "      150500       0.87      0.95      0.91        93\n",
      "      150600       0.50      0.50      0.50         2\n",
      "      150710       0.70      0.73      0.71        66\n",
      "      150790       0.75      0.70      0.72       135\n",
      "      150810       0.82      0.93      0.87       200\n",
      "      150890       0.67      0.75      0.71         8\n",
      "      150910       0.74      0.81      0.77       200\n",
      "      150990       0.60      0.54      0.57       200\n",
      "      151000       0.63      0.78      0.69       200\n",
      "      151110       0.97      0.97      0.97       200\n",
      "      151190       0.85      0.89      0.87       200\n",
      "      151211       0.86      0.81      0.83        37\n",
      "      151219       0.79      0.72      0.75       107\n",
      "      151221       0.79      0.91      0.85        33\n",
      "      151229       0.82      0.85      0.83        47\n",
      "      151311       0.88      0.58      0.70        24\n",
      "      151319       0.87      0.89      0.88        90\n",
      "      151321       0.00      0.00      0.00         1\n",
      "      151329       0.94      0.89      0.92        74\n",
      "      151411       0.75      0.86      0.80         7\n",
      "      151419       0.88      0.90      0.89        67\n",
      "      151491       0.50      0.25      0.33         4\n",
      "      151499       0.00      0.00      0.00         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       1.00      0.50      0.67        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       1.00      0.25      0.40         4\n",
      "      151530       0.86      0.96      0.91        99\n",
      "      151550       0.87      0.82      0.85        50\n",
      "      151590       0.77      0.86      0.81       191\n",
      "      151610       0.98      0.94      0.96       108\n",
      "      151620       0.91      0.77      0.83        91\n",
      "      151710       0.83      0.74      0.78        27\n",
      "      151790       0.77      0.72      0.74       173\n",
      "      151800       0.78      0.70      0.74        40\n",
      "      152000       0.89      0.97      0.93       131\n",
      "      152110       0.79      0.92      0.85        36\n",
      "      152190       0.91      0.91      0.91        35\n",
      "      152200       0.78      0.70      0.74        10\n",
      "      160100       0.69      0.80      0.74       144\n",
      "      160210       0.80      0.36      0.50        11\n",
      "      160220       0.98      0.97      0.97        89\n",
      "      160231       0.83      0.83      0.83        42\n",
      "      160232       0.86      0.50      0.63        12\n",
      "      160239       1.00      0.83      0.91         6\n",
      "      160241       0.93      0.91      0.92        86\n",
      "      160242       0.79      0.71      0.75       200\n",
      "      160249       0.72      0.81      0.76        73\n",
      "      160250       0.93      0.97      0.95       200\n",
      "      160290       0.64      0.66      0.65        41\n",
      "      160300       0.87      0.76      0.81        45\n",
      "      160411       0.84      0.93      0.88       200\n",
      "      160412       0.77      0.71      0.74        93\n",
      "      160413       0.83      0.85      0.84       200\n",
      "      160414       0.85      0.77      0.81       200\n",
      "      160415       0.83      0.92      0.87       100\n",
      "      160416       0.71      0.53      0.61        32\n",
      "      160417       1.00      0.33      0.50         3\n",
      "      160419       0.79      0.84      0.82       129\n",
      "      160420       0.83      0.81      0.82       200\n",
      "      160432       0.33      0.25      0.29         4\n",
      "      160510       0.86      0.85      0.86       200\n",
      "      160521       0.91      0.92      0.92       173\n",
      "      160529       0.83      0.92      0.88       119\n",
      "      160530       0.76      0.79      0.77        39\n",
      "      160540       0.78      0.92      0.85        51\n",
      "      160551       0.40      0.50      0.44         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       0.79      0.58      0.67        19\n",
      "      160554       0.71      0.86      0.78        29\n",
      "      160555       0.50      0.64      0.56        11\n",
      "      160556       1.00      0.38      0.56        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       0.57      0.80      0.67         5\n",
      "      160559       0.71      0.42      0.53        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.54      0.41      0.46        64\n",
      "      170113       0.67      0.35      0.46        17\n",
      "      170114       0.74      0.76      0.75        38\n",
      "      170191       0.72      0.62      0.66       200\n",
      "      170199       0.91      0.87      0.89       182\n",
      "      170211       0.90      0.84      0.87        55\n",
      "      170219       0.81      0.89      0.84       200\n",
      "      170220       0.77      0.58      0.66       191\n",
      "      170230       0.89      0.91      0.90       200\n",
      "      170240       0.60      0.40      0.48       200\n",
      "      170250       0.85      0.69      0.76        89\n",
      "      170260       0.71      0.56      0.62       200\n",
      "      170290       0.85      0.72      0.78       200\n",
      "      170310       0.85      0.92      0.88       130\n",
      "      170390       1.00      0.83      0.91         6\n",
      "      170410       0.90      0.83      0.86        75\n",
      "      170490       0.67      0.48      0.56       200\n",
      "      180100       0.84      0.90      0.87       200\n",
      "      180200       0.79      0.85      0.82        94\n",
      "      180310       0.78      0.87      0.82       188\n",
      "      180320       0.77      0.96      0.85        49\n",
      "      180400       0.83      0.85      0.84       200\n",
      "      180500       0.81      0.87      0.84       200\n",
      "      180610       0.68      0.48      0.57       200\n",
      "      180620       0.61      0.46      0.52       200\n",
      "      180631       0.73      0.72      0.72       189\n",
      "      180632       0.75      0.47      0.58       200\n",
      "      180690       0.58      0.51      0.54       200\n",
      "      190110       0.88      0.92      0.90       200\n",
      "      190120       0.79      0.56      0.65       200\n",
      "      190190       0.71      0.50      0.59       200\n",
      "      190211       0.70      0.81      0.75       113\n",
      "      190219       0.77      0.79      0.78       200\n",
      "      190220       0.80      0.75      0.77       120\n",
      "      190230       0.85      0.66      0.74       200\n",
      "      190240       0.80      0.68      0.74        41\n",
      "      190300       0.81      0.83      0.82       200\n",
      "      190410       0.76      0.76      0.76       200\n",
      "      190420       0.69      0.46      0.55        39\n",
      "      190430       0.88      0.81      0.84       200\n",
      "      190490       0.85      0.83      0.84       177\n",
      "      190510       0.64      0.66      0.65        32\n",
      "      190520       0.84      0.73      0.78        51\n",
      "      190531       0.71      0.79      0.75       200\n",
      "      190532       0.73      0.78      0.76       188\n",
      "      190540       0.80      0.81      0.81       200\n",
      "      190590       0.74      0.52      0.61       200\n",
      "\n",
      "    accuracy                           0.79     21023\n",
      "   macro avg       0.72      0.69      0.70     21023\n",
      "weighted avg       0.79      0.79      0.78     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   2   2 ...   0   0   0]\n",
      " [  0   0  89 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 147   0   2]\n",
      " [  0   0   0 ...   1 163   1]\n",
      " [  0   0   0 ...   5  12 104]]\n",
      "Created predictions in 87.62 seconds\n"
     ]
    }
   ],
   "source": [
    "trial4 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', KNeighborsClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(trial4, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/KNN_tfidf_model_1x.sav')\n",
    "\n",
    "results.at['KNN-tfidf Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "raised-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 606.9 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       0.75      0.75      0.75         4\n",
      "      100199       0.96      0.97      0.96        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       1.00      0.50      0.67         2\n",
      "      100410       1.00      0.82      0.90        11\n",
      "      100490       1.00      0.92      0.96        37\n",
      "      100510       0.94      0.89      0.92        56\n",
      "      100590       0.93      0.86      0.89        73\n",
      "      100610       0.91      0.95      0.93        75\n",
      "      100620       0.93      0.90      0.92       200\n",
      "      100630       0.85      0.91      0.88       200\n",
      "      100640       0.92      0.97      0.94       200\n",
      "      100810       0.83      0.95      0.88        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       0.75      0.75      0.75         4\n",
      "      100830       0.93      0.94      0.94       180\n",
      "      100840       1.00      1.00      1.00         1\n",
      "      100850       0.96      0.97      0.97       179\n",
      "      100890       0.89      0.96      0.92        96\n",
      "      110100       0.84      0.87      0.86       200\n",
      "      110220       0.86      0.95      0.90       157\n",
      "      110290       0.86      0.76      0.81       115\n",
      "      110311       0.69      0.45      0.55        20\n",
      "      110313       0.86      0.77      0.81        31\n",
      "      110319       0.92      0.98      0.95       200\n",
      "      110320       0.98      0.97      0.98        66\n",
      "      110412       0.96      0.96      0.96        91\n",
      "      110419       0.93      0.82      0.87        61\n",
      "      110422       1.00      0.83      0.91         6\n",
      "      110423       0.92      0.92      0.92        38\n",
      "      110429       0.79      0.48      0.59        23\n",
      "      110430       1.00      0.57      0.73         7\n",
      "      110510       0.93      0.91      0.92        68\n",
      "      110520       0.94      0.91      0.93        55\n",
      "      110610       0.78      0.72      0.75        86\n",
      "      110620       0.80      0.71      0.76        63\n",
      "      110630       0.86      0.89      0.87       146\n",
      "      110710       0.88      0.80      0.84       200\n",
      "      110720       0.74      0.90      0.81       127\n",
      "      110811       0.94      0.83      0.88        53\n",
      "      110812       0.90      0.86      0.88        63\n",
      "      110813       0.94      0.93      0.93       200\n",
      "      110814       0.94      0.96      0.95       200\n",
      "      110819       0.87      0.84      0.85        63\n",
      "      110820       1.00      0.98      0.99        46\n",
      "      110900       0.95      0.95      0.95       200\n",
      "      120110       0.89      0.85      0.87        20\n",
      "      120190       0.91      0.88      0.89       200\n",
      "      120230       0.00      0.00      0.00         0\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.96      0.96      0.96        23\n",
      "      120300       1.00      0.50      0.67         8\n",
      "      120400       0.87      0.92      0.89       100\n",
      "      120510       1.00      0.82      0.90        22\n",
      "      120590       0.80      0.50      0.62         8\n",
      "      120600       0.93      0.95      0.94       200\n",
      "      120710       0.96      0.96      0.96       200\n",
      "      120730       0.97      0.97      0.97        32\n",
      "      120740       0.92      0.93      0.92       112\n",
      "      120750       0.94      0.92      0.93        36\n",
      "      120760       1.00      0.33      0.50         3\n",
      "      120791       0.98      0.94      0.96        47\n",
      "      120799       0.92      0.91      0.91       123\n",
      "      120810       0.94      0.92      0.93       177\n",
      "      120890       0.92      0.80      0.86        15\n",
      "      120910       0.90      0.82      0.86        11\n",
      "      120921       0.94      1.00      0.97        33\n",
      "      120922       0.88      0.62      0.73        24\n",
      "      120923       1.00      0.50      0.67        10\n",
      "      120924       0.70      0.85      0.77       200\n",
      "      120925       0.98      0.93      0.95        44\n",
      "      120929       0.88      0.93      0.90        30\n",
      "      120930       0.92      0.88      0.90        25\n",
      "      120991       0.91      0.95      0.93       200\n",
      "      120999       0.97      0.92      0.94        71\n",
      "      121010       1.00      0.14      0.25         7\n",
      "      121020       0.89      0.93      0.91        89\n",
      "      121120       1.00      0.97      0.99        35\n",
      "      121130       0.87      1.00      0.93        40\n",
      "      121140       1.00      0.70      0.82        10\n",
      "      121190       0.77      0.85      0.81       200\n",
      "      121221       0.69      0.64      0.67        14\n",
      "      121229       1.00      0.87      0.93        30\n",
      "      121291       0.98      1.00      0.99        43\n",
      "      121293       1.00      0.50      0.67         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.80      0.68      0.74        60\n",
      "      121300       1.00      0.33      0.50         6\n",
      "      121410       0.96      0.98      0.97       103\n",
      "      121490       0.99      0.97      0.98       173\n",
      "      130120       0.97      0.97      0.97       200\n",
      "      130190       0.89      0.88      0.88        56\n",
      "      130211       0.90      0.87      0.89        31\n",
      "      130212       0.76      0.93      0.84        14\n",
      "      130213       0.80      0.50      0.62         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.80      0.92      0.86       139\n",
      "      130220       0.94      0.81      0.87        21\n",
      "      130231       1.00      0.97      0.99        38\n",
      "      130232       0.95      0.95      0.95       200\n",
      "      130239       0.91      0.87      0.89        70\n",
      "      140110       0.95      0.99      0.97       200\n",
      "      140120       0.97      0.73      0.83        44\n",
      "      140190       1.00      0.78      0.88        18\n",
      "      140420       0.91      0.98      0.95        97\n",
      "      140490       0.89      0.82      0.86        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       1.00      0.60      0.75         5\n",
      "      150290       1.00      1.00      1.00         2\n",
      "      150300       0.75      0.33      0.46         9\n",
      "      150410       0.93      0.82      0.87        33\n",
      "      150420       0.87      0.83      0.85        24\n",
      "      150430       1.00      0.67      0.80         3\n",
      "      150500       0.98      0.99      0.98        93\n",
      "      150600       1.00      0.50      0.67         2\n",
      "      150710       0.88      0.91      0.90        66\n",
      "      150790       0.88      0.89      0.89       135\n",
      "      150810       0.98      0.98      0.98       200\n",
      "      150890       0.86      0.75      0.80         8\n",
      "      150910       0.89      0.81      0.85       200\n",
      "      150990       0.78      0.87      0.82       200\n",
      "      151000       0.87      0.93      0.90       200\n",
      "      151110       0.99      0.99      0.99       200\n",
      "      151190       0.97      0.97      0.97       200\n",
      "      151211       0.85      0.89      0.87        37\n",
      "      151219       0.91      0.83      0.87       107\n",
      "      151221       0.93      0.79      0.85        33\n",
      "      151229       0.92      0.96      0.94        47\n",
      "      151311       0.94      0.67      0.78        24\n",
      "      151319       0.91      0.92      0.92        90\n",
      "      151321       1.00      1.00      1.00         1\n",
      "      151329       0.97      0.97      0.97        74\n",
      "      151411       1.00      0.86      0.92         7\n",
      "      151419       0.93      0.93      0.93        67\n",
      "      151491       0.75      0.75      0.75         4\n",
      "      151499       0.50      0.25      0.33         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       0.88      0.70      0.78        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       1.00      0.25      0.40         4\n",
      "      151530       0.97      0.95      0.96        99\n",
      "      151550       0.90      0.92      0.91        50\n",
      "      151590       0.83      0.91      0.87       191\n",
      "      151610       0.99      0.94      0.97       108\n",
      "      151620       0.89      0.86      0.87        91\n",
      "      151710       0.92      0.81      0.86        27\n",
      "      151790       0.80      0.79      0.79       173\n",
      "      151800       0.82      0.70      0.76        40\n",
      "      152000       0.97      0.97      0.97       131\n",
      "      152110       0.92      0.94      0.93        36\n",
      "      152190       1.00      0.97      0.99        35\n",
      "      152200       1.00      0.70      0.82        10\n",
      "      160100       0.85      0.90      0.87       144\n",
      "      160210       0.88      0.64      0.74        11\n",
      "      160220       1.00      0.97      0.98        89\n",
      "      160231       0.97      0.90      0.94        42\n",
      "      160232       0.90      0.75      0.82        12\n",
      "      160239       0.71      0.83      0.77         6\n",
      "      160241       0.99      0.93      0.96        86\n",
      "      160242       0.88      0.94      0.91       200\n",
      "      160249       0.85      0.88      0.86        73\n",
      "      160250       0.96      0.96      0.96       200\n",
      "      160290       0.89      0.80      0.85        41\n",
      "      160300       0.97      0.78      0.86        45\n",
      "      160411       0.88      0.98      0.93       200\n",
      "      160412       0.82      0.86      0.84        93\n",
      "      160413       0.88      0.89      0.88       200\n",
      "      160414       0.90      0.80      0.85       200\n",
      "      160415       0.92      0.97      0.94       100\n",
      "      160416       0.81      0.66      0.72        32\n",
      "      160417       1.00      0.33      0.50         3\n",
      "      160419       0.84      0.87      0.85       129\n",
      "      160420       0.90      0.88      0.89       200\n",
      "      160432       0.00      0.00      0.00         4\n",
      "      160510       0.94      0.91      0.92       200\n",
      "      160521       0.95      0.98      0.96       173\n",
      "      160529       0.96      0.94      0.95       119\n",
      "      160530       0.97      0.95      0.96        39\n",
      "      160540       0.94      0.92      0.93        51\n",
      "      160551       0.60      0.75      0.67         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       0.75      0.79      0.77        19\n",
      "      160554       0.77      0.79      0.78        29\n",
      "      160555       0.80      0.73      0.76        11\n",
      "      160556       1.00      0.62      0.76        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       0.67      0.80      0.73         5\n",
      "      160559       0.86      0.50      0.63        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.71      0.64      0.67        64\n",
      "      170113       0.83      0.59      0.69        17\n",
      "      170114       0.76      0.68      0.72        38\n",
      "      170191       0.74      0.77      0.75       200\n",
      "      170199       0.92      0.88      0.90       182\n",
      "      170211       0.93      0.91      0.92        55\n",
      "      170219       0.94      0.96      0.95       200\n",
      "      170220       0.80      0.82      0.81       191\n",
      "      170230       0.95      0.93      0.94       200\n",
      "      170240       0.63      0.68      0.65       200\n",
      "      170250       0.87      0.92      0.90        89\n",
      "      170260       0.70      0.78      0.74       200\n",
      "      170290       0.84      0.84      0.84       200\n",
      "      170310       0.93      0.96      0.94       130\n",
      "      170390       1.00      1.00      1.00         6\n",
      "      170410       0.94      0.88      0.91        75\n",
      "      170490       0.60      0.58      0.59       200\n",
      "      180100       0.95      0.93      0.94       200\n",
      "      180200       0.94      0.93      0.93        94\n",
      "      180310       0.94      0.96      0.95       188\n",
      "      180320       0.98      0.96      0.97        49\n",
      "      180400       0.90      0.91      0.90       200\n",
      "      180500       0.91      0.88      0.90       200\n",
      "      180610       0.78      0.71      0.75       200\n",
      "      180620       0.66      0.68      0.67       200\n",
      "      180631       0.76      0.78      0.77       189\n",
      "      180632       0.71      0.75      0.73       200\n",
      "      180690       0.58      0.66      0.62       200\n",
      "      190110       0.95      0.94      0.94       200\n",
      "      190120       0.71      0.74      0.73       200\n",
      "      190190       0.71      0.66      0.68       200\n",
      "      190211       0.79      0.82      0.81       113\n",
      "      190219       0.82      0.79      0.80       200\n",
      "      190220       0.85      0.89      0.87       120\n",
      "      190230       0.83      0.83      0.83       200\n",
      "      190240       0.80      0.80      0.80        41\n",
      "      190300       0.89      0.89      0.89       200\n",
      "      190410       0.75      0.86      0.80       200\n",
      "      190420       0.88      0.54      0.67        39\n",
      "      190430       0.92      0.96      0.94       200\n",
      "      190490       0.88      0.82      0.85       177\n",
      "      190510       0.91      0.66      0.76        32\n",
      "      190520       0.87      0.78      0.82        51\n",
      "      190531       0.78      0.82      0.80       200\n",
      "      190532       0.83      0.79      0.81       188\n",
      "      190540       0.88      0.88      0.88       200\n",
      "      190590       0.70      0.65      0.68       200\n",
      "\n",
      "    accuracy                           0.87     21023\n",
      "   macro avg       0.82      0.76      0.78     21023\n",
      "weighted avg       0.87      0.87      0.87     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   3   1 ...   0   0   0]\n",
      " [  0   1  90 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 149   0   1]\n",
      " [  0   0   0 ...   0 175   6]\n",
      " [  0   0   0 ...   4   6 130]]\n",
      "Created predictions in 1.368 seconds\n"
     ]
    }
   ],
   "source": [
    "trial5 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier',LogisticRegression()),\n",
    "])\n",
    " \n",
    "acc = train(trial5, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/LogReg_BOW_model_1x.sav')\n",
    "\n",
    "results.at['LogReg-BOW Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "universal-exclusive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 592.7 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       0.00      0.00      0.00         4\n",
      "      100199       0.89      0.97      0.93        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       0.00      0.00      0.00         2\n",
      "      100410       1.00      0.73      0.84        11\n",
      "      100490       1.00      0.92      0.96        37\n",
      "      100510       0.98      0.86      0.91        56\n",
      "      100590       0.98      0.82      0.90        73\n",
      "      100610       0.88      0.85      0.86        75\n",
      "      100620       0.88      0.91      0.89       200\n",
      "      100630       0.85      0.90      0.87       200\n",
      "      100640       0.82      0.94      0.88       200\n",
      "      100810       0.89      0.80      0.84        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       0.00      0.00      0.00         4\n",
      "      100830       0.91      0.92      0.92       180\n",
      "      100840       0.00      0.00      0.00         1\n",
      "      100850       0.93      0.98      0.95       179\n",
      "      100890       0.91      0.94      0.92        96\n",
      "      110100       0.81      0.85      0.83       200\n",
      "      110220       0.76      0.96      0.85       157\n",
      "      110290       0.85      0.75      0.80       115\n",
      "      110311       1.00      0.20      0.33        20\n",
      "      110313       0.87      0.65      0.74        31\n",
      "      110319       0.85      0.96      0.90       200\n",
      "      110320       0.92      0.92      0.92        66\n",
      "      110412       0.94      0.91      0.93        91\n",
      "      110419       0.94      0.77      0.85        61\n",
      "      110422       1.00      0.17      0.29         6\n",
      "      110423       0.92      0.87      0.89        38\n",
      "      110429       0.75      0.26      0.39        23\n",
      "      110430       1.00      0.29      0.44         7\n",
      "      110510       0.97      0.87      0.91        68\n",
      "      110520       0.96      0.78      0.86        55\n",
      "      110610       0.75      0.66      0.70        86\n",
      "      110620       0.87      0.65      0.75        63\n",
      "      110630       0.88      0.83      0.86       146\n",
      "      110710       0.87      0.79      0.83       200\n",
      "      110720       0.72      0.90      0.80       127\n",
      "      110811       1.00      0.72      0.84        53\n",
      "      110812       0.94      0.76      0.84        63\n",
      "      110813       0.90      0.95      0.92       200\n",
      "      110814       0.89      0.94      0.92       200\n",
      "      110819       0.84      0.81      0.82        63\n",
      "      110820       0.95      0.89      0.92        46\n",
      "      110900       0.95      0.94      0.95       200\n",
      "      120110       1.00      0.45      0.62        20\n",
      "      120190       0.87      0.83      0.85       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.95      0.91      0.93        23\n",
      "      120300       1.00      0.62      0.77         8\n",
      "      120400       0.94      0.94      0.94       100\n",
      "      120510       1.00      0.68      0.81        22\n",
      "      120590       0.50      0.12      0.20         8\n",
      "      120600       0.92      0.95      0.93       200\n",
      "      120710       0.93      0.95      0.94       200\n",
      "      120730       1.00      0.84      0.92        32\n",
      "      120740       0.94      0.91      0.93       112\n",
      "      120750       0.89      0.94      0.92        36\n",
      "      120760       0.00      0.00      0.00         3\n",
      "      120791       0.98      0.89      0.93        47\n",
      "      120799       0.87      0.90      0.88       123\n",
      "      120810       0.92      0.89      0.91       177\n",
      "      120890       0.89      0.53      0.67        15\n",
      "      120910       0.89      0.73      0.80        11\n",
      "      120921       0.84      0.94      0.89        33\n",
      "      120922       1.00      0.42      0.59        24\n",
      "      120923       1.00      0.30      0.46        10\n",
      "      120924       0.58      0.87      0.70       200\n",
      "      120925       0.97      0.86      0.92        44\n",
      "      120929       0.96      0.73      0.83        30\n",
      "      120930       0.95      0.80      0.87        25\n",
      "      120991       0.87      0.93      0.90       200\n",
      "      120999       1.00      0.92      0.96        71\n",
      "      121010       0.00      0.00      0.00         7\n",
      "      121020       0.93      0.92      0.93        89\n",
      "      121120       1.00      0.89      0.94        35\n",
      "      121130       0.88      0.95      0.92        40\n",
      "      121140       1.00      0.70      0.82        10\n",
      "      121190       0.77      0.84      0.81       200\n",
      "      121221       1.00      0.43      0.60        14\n",
      "      121229       1.00      0.70      0.82        30\n",
      "      121291       0.98      0.93      0.95        43\n",
      "      121293       0.00      0.00      0.00         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.87      0.55      0.67        60\n",
      "      121300       0.00      0.00      0.00         6\n",
      "      121410       0.95      0.93      0.94       103\n",
      "      121490       0.98      0.97      0.97       173\n",
      "      130120       0.96      0.93      0.94       200\n",
      "      130190       0.94      0.84      0.89        56\n",
      "      130211       0.93      0.81      0.86        31\n",
      "      130212       0.90      0.64      0.75        14\n",
      "      130213       0.80      0.50      0.62         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.84      0.91      0.87       139\n",
      "      130220       1.00      0.71      0.83        21\n",
      "      130231       1.00      0.97      0.99        38\n",
      "      130232       0.93      0.95      0.94       200\n",
      "      130239       0.90      0.86      0.88        70\n",
      "      140110       0.90      0.99      0.94       200\n",
      "      140120       1.00      0.66      0.79        44\n",
      "      140190       1.00      0.78      0.88        18\n",
      "      140420       0.92      0.98      0.95        97\n",
      "      140490       1.00      0.67      0.80        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       1.00      0.20      0.33         5\n",
      "      150290       0.00      0.00      0.00         2\n",
      "      150300       1.00      0.33      0.50         9\n",
      "      150410       0.85      0.70      0.77        33\n",
      "      150420       0.86      0.79      0.83        24\n",
      "      150430       0.00      0.00      0.00         3\n",
      "      150500       1.00      0.98      0.99        93\n",
      "      150600       0.00      0.00      0.00         2\n",
      "      150710       0.74      0.83      0.79        66\n",
      "      150790       0.84      0.87      0.85       135\n",
      "      150810       0.97      0.97      0.97       200\n",
      "      150890       1.00      0.75      0.86         8\n",
      "      150910       0.89      0.79      0.84       200\n",
      "      150990       0.63      0.84      0.72       200\n",
      "      151000       0.84      0.90      0.87       200\n",
      "      151110       0.96      0.99      0.98       200\n",
      "      151190       0.93      0.96      0.94       200\n",
      "      151211       0.94      0.81      0.87        37\n",
      "      151219       0.90      0.79      0.84       107\n",
      "      151221       1.00      0.67      0.80        33\n",
      "      151229       0.80      0.83      0.81        47\n",
      "      151311       1.00      0.50      0.67        24\n",
      "      151319       0.82      0.89      0.86        90\n",
      "      151321       0.00      0.00      0.00         1\n",
      "      151329       0.99      0.96      0.97        74\n",
      "      151411       0.00      0.00      0.00         7\n",
      "      151419       0.86      0.81      0.83        67\n",
      "      151491       0.00      0.00      0.00         4\n",
      "      151499       0.00      0.00      0.00         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       1.00      0.40      0.57        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       0.00      0.00      0.00         4\n",
      "      151530       0.96      0.95      0.95        99\n",
      "      151550       0.91      0.82      0.86        50\n",
      "      151590       0.79      0.91      0.84       191\n",
      "      151610       1.00      0.92      0.96       108\n",
      "      151620       0.94      0.82      0.88        91\n",
      "      151710       0.95      0.74      0.83        27\n",
      "      151790       0.77      0.76      0.77       173\n",
      "      151800       0.88      0.55      0.68        40\n",
      "      152000       0.98      0.97      0.98       131\n",
      "      152110       0.94      0.89      0.91        36\n",
      "      152190       1.00      0.97      0.99        35\n",
      "      152200       1.00      0.60      0.75        10\n",
      "      160100       0.80      0.84      0.82       144\n",
      "      160210       1.00      0.36      0.53        11\n",
      "      160220       1.00      0.93      0.97        89\n",
      "      160231       1.00      0.86      0.92        42\n",
      "      160232       1.00      0.50      0.67        12\n",
      "      160239       1.00      0.83      0.91         6\n",
      "      160241       0.99      0.92      0.95        86\n",
      "      160242       0.85      0.97      0.91       200\n",
      "      160249       0.77      0.86      0.81        73\n",
      "      160250       0.92      0.96      0.94       200\n",
      "      160290       0.96      0.61      0.75        41\n",
      "      160300       0.83      0.76      0.79        45\n",
      "      160411       0.84      0.95      0.89       200\n",
      "      160412       0.85      0.77      0.81        93\n",
      "      160413       0.90      0.85      0.88       200\n",
      "      160414       0.84      0.82      0.83       200\n",
      "      160415       0.83      0.95      0.89       100\n",
      "      160416       0.94      0.53      0.68        32\n",
      "      160417       0.00      0.00      0.00         3\n",
      "      160419       0.77      0.77      0.77       129\n",
      "      160420       0.84      0.88      0.86       200\n",
      "      160432       0.00      0.00      0.00         4\n",
      "      160510       0.90      0.88      0.89       200\n",
      "      160521       0.87      0.95      0.91       173\n",
      "      160529       0.86      0.96      0.91       119\n",
      "      160530       1.00      0.85      0.92        39\n",
      "      160540       0.97      0.69      0.80        51\n",
      "      160551       1.00      0.50      0.67         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       1.00      0.63      0.77        19\n",
      "      160554       0.76      0.76      0.76        29\n",
      "      160555       1.00      0.09      0.17        11\n",
      "      160556       1.00      0.23      0.38        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       1.00      0.80      0.89         5\n",
      "      160559       0.80      0.33      0.47        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.78      0.59      0.67        64\n",
      "      170113       1.00      0.41      0.58        17\n",
      "      170114       1.00      0.63      0.77        38\n",
      "      170191       0.62      0.78      0.69       200\n",
      "      170199       0.92      0.89      0.91       182\n",
      "      170211       0.90      0.82      0.86        55\n",
      "      170219       0.88      0.94      0.91       200\n",
      "      170220       0.74      0.76      0.75       191\n",
      "      170230       0.93      0.93      0.93       200\n",
      "      170240       0.53      0.67      0.59       200\n",
      "      170250       0.85      0.90      0.87        89\n",
      "      170260       0.63      0.68      0.66       200\n",
      "      170290       0.74      0.84      0.79       200\n",
      "      170310       0.87      0.97      0.92       130\n",
      "      170390       1.00      0.67      0.80         6\n",
      "      170410       0.97      0.84      0.90        75\n",
      "      170490       0.59      0.55      0.57       200\n",
      "      180100       0.93      0.92      0.92       200\n",
      "      180200       0.97      0.90      0.93        94\n",
      "      180310       0.87      0.96      0.91       188\n",
      "      180320       0.98      1.00      0.99        49\n",
      "      180400       0.86      0.93      0.89       200\n",
      "      180500       0.85      0.87      0.86       200\n",
      "      180610       0.68      0.73      0.70       200\n",
      "      180620       0.61      0.64      0.62       200\n",
      "      180631       0.80      0.75      0.77       189\n",
      "      180632       0.63      0.73      0.68       200\n",
      "      180690       0.61      0.65      0.62       200\n",
      "      190110       0.97      0.89      0.93       200\n",
      "      190120       0.67      0.72      0.69       200\n",
      "      190190       0.59      0.58      0.59       200\n",
      "      190211       0.81      0.81      0.81       113\n",
      "      190219       0.77      0.79      0.78       200\n",
      "      190220       0.84      0.77      0.80       120\n",
      "      190230       0.78      0.79      0.78       200\n",
      "      190240       0.82      0.78      0.80        41\n",
      "      190300       0.87      0.90      0.88       200\n",
      "      190410       0.66      0.79      0.72       200\n",
      "      190420       0.74      0.36      0.48        39\n",
      "      190430       0.92      0.96      0.94       200\n",
      "      190490       0.90      0.82      0.86       177\n",
      "      190510       0.96      0.69      0.80        32\n",
      "      190520       0.90      0.69      0.78        51\n",
      "      190531       0.80      0.79      0.79       200\n",
      "      190532       0.80      0.77      0.78       188\n",
      "      190540       0.91      0.88      0.89       200\n",
      "      190590       0.66      0.65      0.65       200\n",
      "\n",
      "    accuracy                           0.84     21023\n",
      "   macro avg       0.76      0.66      0.69     21023\n",
      "weighted avg       0.85      0.84      0.84     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   0   4 ...   0   0   0]\n",
      " [  0   0  90 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 144   0   1]\n",
      " [  0   0   0 ...   1 175   6]\n",
      " [  0   0   0 ...   5   4 129]]\n",
      "Created predictions in 1.385 seconds\n"
     ]
    }
   ],
   "source": [
    "trial6 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier',LogisticRegression()),\n",
    "])\n",
    " \n",
    "acc = train(trial6, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/LogReg_tfidf_model_1x.sav')\n",
    "\n",
    "results.at['LogReg-tfidf Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "characteristic-victory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 1.319e+03 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       0.00      0.00      0.00         4\n",
      "      100199       0.20      0.97      0.34        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       0.00      0.00      0.00         2\n",
      "      100410       1.00      0.73      0.84        11\n",
      "      100490       1.00      0.76      0.86        37\n",
      "      100510       0.97      0.57      0.72        56\n",
      "      100590       0.97      0.42      0.59        73\n",
      "      100610       0.86      0.24      0.38        75\n",
      "      100620       0.92      0.58      0.71       200\n",
      "      100630       0.83      0.76      0.79       200\n",
      "      100640       0.63      0.93      0.75       200\n",
      "      100810       0.00      0.00      0.00        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       0.00      0.00      0.00         4\n",
      "      100830       0.87      0.73      0.79       180\n",
      "      100840       0.00      0.00      0.00         1\n",
      "      100850       0.91      0.93      0.92       179\n",
      "      100890       0.95      0.36      0.53        96\n",
      "      110100       0.74      0.67      0.70       200\n",
      "      110220       0.70      0.72      0.71       157\n",
      "      110290       0.93      0.64      0.76       115\n",
      "      110311       0.00      0.00      0.00        20\n",
      "      110313       0.83      0.16      0.27        31\n",
      "      110319       0.85      0.66      0.74       200\n",
      "      110320       0.87      0.20      0.32        66\n",
      "      110412       0.95      0.62      0.75        91\n",
      "      110419       0.95      0.57      0.71        61\n",
      "      110422       0.00      0.00      0.00         6\n",
      "      110423       1.00      0.45      0.62        38\n",
      "      110429       1.00      0.04      0.08        23\n",
      "      110430       0.00      0.00      0.00         7\n",
      "      110510       1.00      0.51      0.68        68\n",
      "      110520       0.90      0.16      0.28        55\n",
      "      110610       0.77      0.31      0.45        86\n",
      "      110620       0.90      0.29      0.43        63\n",
      "      110630       0.96      0.60      0.74       146\n",
      "      110710       0.86      0.43      0.58       200\n",
      "      110720       0.69      0.88      0.78       127\n",
      "      110811       1.00      0.36      0.53        53\n",
      "      110812       0.93      0.40      0.56        63\n",
      "      110813       0.91      0.80      0.85       200\n",
      "      110814       0.89      0.72      0.80       200\n",
      "      110819       0.84      0.25      0.39        63\n",
      "      110820       0.96      0.52      0.68        46\n",
      "      110900       0.97      0.81      0.89       200\n",
      "      120110       1.00      0.25      0.40        20\n",
      "      120190       0.86      0.78      0.82       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.94      0.74      0.83        23\n",
      "      120300       1.00      0.50      0.67         8\n",
      "      120400       0.89      0.58      0.70       100\n",
      "      120510       0.88      0.32      0.47        22\n",
      "      120590       1.00      0.25      0.40         8\n",
      "      120600       0.84      0.86      0.85       200\n",
      "      120710       0.76      0.69      0.72       200\n",
      "      120730       0.00      0.00      0.00        32\n",
      "      120740       0.88      0.75      0.81       112\n",
      "      120750       0.96      0.75      0.84        36\n",
      "      120760       0.00      0.00      0.00         3\n",
      "      120791       0.90      0.40      0.56        47\n",
      "      120799       0.85      0.76      0.80       123\n",
      "      120810       0.89      0.75      0.81       177\n",
      "      120890       0.00      0.00      0.00        15\n",
      "      120910       1.00      0.09      0.17        11\n",
      "      120921       1.00      0.30      0.47        33\n",
      "      120922       0.00      0.00      0.00        24\n",
      "      120923       0.00      0.00      0.00        10\n",
      "      120924       0.05      0.88      0.09       200\n",
      "      120925       0.92      0.27      0.42        44\n",
      "      120929       0.67      0.07      0.12        30\n",
      "      120930       1.00      0.60      0.75        25\n",
      "      120991       0.76      0.67      0.71       200\n",
      "      120999       1.00      0.59      0.74        71\n",
      "      121010       0.00      0.00      0.00         7\n",
      "      121020       0.94      0.19      0.32        89\n",
      "      121120       1.00      0.20      0.33        35\n",
      "      121130       1.00      0.23      0.37        40\n",
      "      121140       1.00      0.10      0.18        10\n",
      "      121190       0.41      0.49      0.45       200\n",
      "      121221       1.00      0.07      0.13        14\n",
      "      121229       1.00      0.57      0.72        30\n",
      "      121291       1.00      0.81      0.90        43\n",
      "      121293       0.00      0.00      0.00         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.00      0.00      0.00        60\n",
      "      121300       0.00      0.00      0.00         6\n",
      "      121410       0.95      0.58      0.72       103\n",
      "      121490       0.99      0.79      0.88       173\n",
      "      130120       0.96      0.78      0.86       200\n",
      "      130190       0.89      0.61      0.72        56\n",
      "      130211       0.00      0.00      0.00        31\n",
      "      130212       0.00      0.00      0.00        14\n",
      "      130213       0.00      0.00      0.00         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.94      0.24      0.38       139\n",
      "      130220       1.00      0.14      0.25        21\n",
      "      130231       1.00      0.71      0.83        38\n",
      "      130232       0.95      0.79      0.86       200\n",
      "      130239       1.00      0.21      0.35        70\n",
      "      140110       0.93      0.86      0.89       200\n",
      "      140120       0.80      0.09      0.16        44\n",
      "      140190       1.00      0.22      0.36        18\n",
      "      140420       0.88      0.37      0.52        97\n",
      "      140490       1.00      0.11      0.20        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       0.00      0.00      0.00         5\n",
      "      150290       0.00      0.00      0.00         2\n",
      "      150300       0.00      0.00      0.00         9\n",
      "      150410       0.87      0.39      0.54        33\n",
      "      150420       1.00      0.21      0.34        24\n",
      "      150430       0.00      0.00      0.00         3\n",
      "      150500       1.00      0.25      0.40        93\n",
      "      150600       0.00      0.00      0.00         2\n",
      "      150710       0.77      0.45      0.57        66\n",
      "      150790       0.76      0.64      0.70       135\n",
      "      150810       0.94      0.94      0.94       200\n",
      "      150890       1.00      0.62      0.77         8\n",
      "      150910       0.80      0.68      0.74       200\n",
      "      150990       0.49      0.66      0.56       200\n",
      "      151000       0.64      0.65      0.65       200\n",
      "      151110       0.91      0.98      0.94       200\n",
      "      151190       0.76      0.90      0.82       200\n",
      "      151211       1.00      0.62      0.77        37\n",
      "      151219       0.83      0.55      0.66       107\n",
      "      151221       1.00      0.12      0.22        33\n",
      "      151229       0.94      0.36      0.52        47\n",
      "      151311       1.00      0.17      0.29        24\n",
      "      151319       0.81      0.81      0.81        90\n",
      "      151321       0.00      0.00      0.00         1\n",
      "      151329       0.96      0.30      0.45        74\n",
      "      151411       0.00      0.00      0.00         7\n",
      "      151419       0.84      0.55      0.67        67\n",
      "      151491       0.00      0.00      0.00         4\n",
      "      151499       0.00      0.00      0.00         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       0.00      0.00      0.00        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       0.00      0.00      0.00         4\n",
      "      151530       0.97      0.68      0.80        99\n",
      "      151550       0.92      0.46      0.61        50\n",
      "      151590       0.57      0.79      0.66       191\n",
      "      151610       0.99      0.88      0.93       108\n",
      "      151620       0.85      0.62      0.71        91\n",
      "      151710       1.00      0.07      0.14        27\n",
      "      151790       0.62      0.55      0.58       173\n",
      "      151800       0.85      0.42      0.57        40\n",
      "      152000       1.00      0.83      0.91       131\n",
      "      152110       0.97      0.81      0.88        36\n",
      "      152190       0.00      0.00      0.00        35\n",
      "      152200       1.00      0.60      0.75        10\n",
      "      160100       0.72      0.53      0.61       144\n",
      "      160210       0.00      0.00      0.00        11\n",
      "      160220       0.95      0.71      0.81        89\n",
      "      160231       1.00      0.19      0.32        42\n",
      "      160232       1.00      0.08      0.15        12\n",
      "      160239       0.00      0.00      0.00         6\n",
      "      160241       0.99      0.90      0.94        86\n",
      "      160242       0.66      0.97      0.79       200\n",
      "      160249       0.89      0.22      0.35        73\n",
      "      160250       0.93      0.94      0.94       200\n",
      "      160290       1.00      0.05      0.09        41\n",
      "      160300       1.00      0.16      0.27        45\n",
      "      160411       0.85      0.72      0.78       200\n",
      "      160412       0.82      0.43      0.56        93\n",
      "      160413       0.93      0.65      0.76       200\n",
      "      160414       0.89      0.68      0.77       200\n",
      "      160415       0.91      0.50      0.65       100\n",
      "      160416       0.86      0.19      0.31        32\n",
      "      160417       0.00      0.00      0.00         3\n",
      "      160419       0.75      0.50      0.60       129\n",
      "      160420       0.73      0.72      0.73       200\n",
      "      160432       0.00      0.00      0.00         4\n",
      "      160510       0.71      0.69      0.70       200\n",
      "      160521       0.85      0.92      0.88       173\n",
      "      160529       0.88      0.84      0.86       119\n",
      "      160530       1.00      0.26      0.41        39\n",
      "      160540       0.93      0.53      0.67        51\n",
      "      160551       0.00      0.00      0.00         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       1.00      0.11      0.19        19\n",
      "      160554       0.75      0.10      0.18        29\n",
      "      160555       0.00      0.00      0.00        11\n",
      "      160556       1.00      0.23      0.38        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       0.00      0.00      0.00         5\n",
      "      160559       0.67      0.17      0.27        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.73      0.12      0.21        64\n",
      "      170113       0.00      0.00      0.00        17\n",
      "      170114       0.86      0.32      0.46        38\n",
      "      170191       0.34      0.71      0.46       200\n",
      "      170199       0.95      0.79      0.86       182\n",
      "      170211       0.97      0.51      0.67        55\n",
      "      170219       0.77      0.74      0.76       200\n",
      "      170220       0.72      0.41      0.52       191\n",
      "      170230       0.85      0.69      0.76       200\n",
      "      170240       0.51      0.47      0.49       200\n",
      "      170250       0.85      0.39      0.54        89\n",
      "      170260       0.53      0.54      0.53       200\n",
      "      170290       0.70      0.71      0.71       200\n",
      "      170310       0.81      0.61      0.69       130\n",
      "      170390       0.00      0.00      0.00         6\n",
      "      170410       1.00      0.36      0.53        75\n",
      "      170490       0.59      0.28      0.37       200\n",
      "      180100       0.92      0.74      0.82       200\n",
      "      180200       0.90      0.38      0.54        94\n",
      "      180310       0.92      0.72      0.81       188\n",
      "      180320       0.93      0.76      0.83        49\n",
      "      180400       0.88      0.67      0.76       200\n",
      "      180500       0.78      0.83      0.81       200\n",
      "      180610       0.74      0.49      0.59       200\n",
      "      180620       0.62      0.48      0.54       200\n",
      "      180631       0.80      0.56      0.65       189\n",
      "      180632       0.56      0.53      0.54       200\n",
      "      180690       0.13      0.51      0.20       200\n",
      "      190110       0.98      0.56      0.71       200\n",
      "      190120       0.56      0.40      0.46       200\n",
      "      190190       0.28      0.30      0.29       200\n",
      "      190211       0.91      0.52      0.66       113\n",
      "      190219       0.71      0.42      0.53       200\n",
      "      190220       0.98      0.45      0.62       120\n",
      "      190230       0.46      0.69      0.55       200\n",
      "      190240       1.00      0.17      0.29        41\n",
      "      190300       0.78      0.77      0.77       200\n",
      "      190410       0.35      0.58      0.44       200\n",
      "      190420       1.00      0.10      0.19        39\n",
      "      190430       0.93      0.73      0.82       200\n",
      "      190490       0.87      0.69      0.77       177\n",
      "      190510       0.00      0.00      0.00        32\n",
      "      190520       1.00      0.08      0.15        51\n",
      "      190531       0.64      0.52      0.57       200\n",
      "      190532       0.82      0.41      0.55       188\n",
      "      190540       0.88      0.42      0.57       200\n",
      "      190590       0.28      0.29      0.29       200\n",
      "\n",
      "    accuracy                           0.60     21023\n",
      "   macro avg       0.64      0.39      0.44     21023\n",
      "weighted avg       0.77      0.60      0.63     21023\n",
      "\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  3 ...  0  0  0]\n",
      " [ 0  0 90 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 77  0  3]\n",
      " [ 0  0  2 ...  0 85  2]\n",
      " [ 0  0  1 ...  2  4 59]]\n",
      "Created predictions in 911.6 seconds\n"
     ]
    }
   ],
   "source": [
    "trial7 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', SVC()),\n",
    "])\n",
    " \n",
    "acc = train(trial7, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/SVM_BOW_model_1x.sav')\n",
    "\n",
    "results.at['SVM-BOW Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "honey-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 1.847e+03 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       0.00      0.00      0.00         4\n",
      "      100199       0.92      0.98      0.95        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       1.00      0.50      0.67         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       0.00      0.00      0.00         2\n",
      "      100410       1.00      0.91      0.95        11\n",
      "      100490       0.97      0.92      0.94        37\n",
      "      100510       0.98      0.82      0.89        56\n",
      "      100590       0.97      0.84      0.90        73\n",
      "      100610       0.89      0.91      0.90        75\n",
      "      100620       0.92      0.91      0.91       200\n",
      "      100630       0.90      0.92      0.91       200\n",
      "      100640       0.82      0.94      0.88       200\n",
      "      100810       0.78      0.90      0.84        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       1.00      0.75      0.86         4\n",
      "      100830       0.92      0.93      0.92       180\n",
      "      100840       0.00      0.00      0.00         1\n",
      "      100850       0.95      0.98      0.96       179\n",
      "      100890       0.95      0.97      0.96        96\n",
      "      110100       0.84      0.90      0.87       200\n",
      "      110220       0.77      0.96      0.86       157\n",
      "      110290       0.85      0.82      0.84       115\n",
      "      110311       1.00      0.60      0.75        20\n",
      "      110313       0.88      0.71      0.79        31\n",
      "      110319       0.89      0.95      0.92       200\n",
      "      110320       0.92      0.92      0.92        66\n",
      "      110412       0.94      0.93      0.94        91\n",
      "      110419       0.94      0.77      0.85        61\n",
      "      110422       1.00      0.67      0.80         6\n",
      "      110423       0.92      0.87      0.89        38\n",
      "      110429       0.83      0.43      0.57        23\n",
      "      110430       0.80      0.57      0.67         7\n",
      "      110510       0.97      0.91      0.94        68\n",
      "      110520       0.96      0.91      0.93        55\n",
      "      110610       0.81      0.64      0.71        86\n",
      "      110620       0.80      0.71      0.76        63\n",
      "      110630       0.90      0.84      0.87       146\n",
      "      110710       0.89      0.81      0.85       200\n",
      "      110720       0.74      0.91      0.82       127\n",
      "      110811       0.95      0.75      0.84        53\n",
      "      110812       0.93      0.81      0.86        63\n",
      "      110813       0.91      0.95      0.93       200\n",
      "      110814       0.92      0.94      0.93       200\n",
      "      110819       0.90      0.83      0.86        63\n",
      "      110820       1.00      0.96      0.98        46\n",
      "      110900       0.98      0.96      0.97       200\n",
      "      120110       1.00      0.85      0.92        20\n",
      "      120190       0.93      0.88      0.90       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.96      1.00      0.98        23\n",
      "      120300       1.00      0.62      0.77         8\n",
      "      120400       0.97      0.92      0.94       100\n",
      "      120510       0.94      0.77      0.85        22\n",
      "      120590       0.86      0.75      0.80         8\n",
      "      120600       0.94      0.95      0.95       200\n",
      "      120710       0.94      0.95      0.94       200\n",
      "      120730       0.97      0.91      0.94        32\n",
      "      120740       0.96      0.93      0.95       112\n",
      "      120750       0.89      0.94      0.92        36\n",
      "      120760       0.00      0.00      0.00         3\n",
      "      120791       1.00      0.87      0.93        47\n",
      "      120799       0.90      0.92      0.91       123\n",
      "      120810       0.92      0.90      0.91       177\n",
      "      120890       1.00      0.60      0.75        15\n",
      "      120910       0.89      0.73      0.80        11\n",
      "      120921       0.94      0.94      0.94        33\n",
      "      120922       1.00      0.42      0.59        24\n",
      "      120923       0.80      0.40      0.53        10\n",
      "      120924       0.48      0.90      0.62       200\n",
      "      120925       1.00      0.89      0.94        44\n",
      "      120929       0.92      0.73      0.81        30\n",
      "      120930       0.96      0.88      0.92        25\n",
      "      120991       0.91      0.93      0.92       200\n",
      "      120999       1.00      0.92      0.96        71\n",
      "      121010       1.00      0.14      0.25         7\n",
      "      121020       0.94      0.91      0.93        89\n",
      "      121120       1.00      0.91      0.96        35\n",
      "      121130       0.84      0.95      0.89        40\n",
      "      121140       0.89      0.80      0.84        10\n",
      "      121190       0.78      0.85      0.81       200\n",
      "      121221       1.00      0.71      0.83        14\n",
      "      121229       1.00      0.80      0.89        30\n",
      "      121291       0.98      0.95      0.96        43\n",
      "      121293       0.00      0.00      0.00         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.88      0.60      0.71        60\n",
      "      121300       1.00      0.50      0.67         6\n",
      "      121410       0.96      0.94      0.95       103\n",
      "      121490       0.99      0.97      0.98       173\n",
      "      130120       0.98      0.93      0.95       200\n",
      "      130190       0.96      0.91      0.94        56\n",
      "      130211       0.89      0.81      0.85        31\n",
      "      130212       0.77      0.71      0.74        14\n",
      "      130213       0.83      0.62      0.71         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.87      0.85      0.86       139\n",
      "      130220       1.00      0.81      0.89        21\n",
      "      130231       0.97      0.95      0.96        38\n",
      "      130232       0.94      0.96      0.95       200\n",
      "      130239       0.91      0.89      0.90        70\n",
      "      140110       0.94      0.98      0.96       200\n",
      "      140120       0.97      0.70      0.82        44\n",
      "      140190       1.00      0.78      0.88        18\n",
      "      140420       0.90      0.98      0.94        97\n",
      "      140490       1.00      0.75      0.86        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       1.00      0.40      0.57         5\n",
      "      150290       1.00      1.00      1.00         2\n",
      "      150300       1.00      0.44      0.62         9\n",
      "      150410       0.92      0.70      0.79        33\n",
      "      150420       0.83      0.83      0.83        24\n",
      "      150430       1.00      0.33      0.50         3\n",
      "      150500       1.00      0.99      0.99        93\n",
      "      150600       1.00      0.50      0.67         2\n",
      "      150710       0.84      0.89      0.87        66\n",
      "      150790       0.82      0.84      0.83       135\n",
      "      150810       0.98      0.97      0.98       200\n",
      "      150890       0.86      0.75      0.80         8\n",
      "      150910       0.90      0.81      0.85       200\n",
      "      150990       0.74      0.84      0.79       200\n",
      "      151000       0.86      0.90      0.88       200\n",
      "      151110       0.99      1.00      0.99       200\n",
      "      151190       0.95      0.96      0.96       200\n",
      "      151211       0.91      0.84      0.87        37\n",
      "      151219       0.89      0.80      0.84       107\n",
      "      151221       0.93      0.79      0.85        33\n",
      "      151229       0.88      0.91      0.90        47\n",
      "      151311       0.92      0.50      0.65        24\n",
      "      151319       0.84      0.90      0.87        90\n",
      "      151321       0.00      0.00      0.00         1\n",
      "      151329       0.99      1.00      0.99        74\n",
      "      151411       1.00      0.86      0.92         7\n",
      "      151419       0.89      0.87      0.88        67\n",
      "      151491       1.00      0.25      0.40         4\n",
      "      151499       0.00      0.00      0.00         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       1.00      0.60      0.75        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       1.00      0.25      0.40         4\n",
      "      151530       0.98      0.97      0.97        99\n",
      "      151550       0.94      0.92      0.93        50\n",
      "      151590       0.83      0.91      0.86       191\n",
      "      151610       1.00      0.94      0.97       108\n",
      "      151620       0.98      0.87      0.92        91\n",
      "      151710       0.91      0.78      0.84        27\n",
      "      151790       0.86      0.77      0.81       173\n",
      "      151800       0.88      0.70      0.78        40\n",
      "      152000       0.98      0.98      0.98       131\n",
      "      152110       0.94      0.92      0.93        36\n",
      "      152190       1.00      0.97      0.99        35\n",
      "      152200       1.00      0.70      0.82        10\n",
      "      160100       0.84      0.89      0.86       144\n",
      "      160210       1.00      0.55      0.71        11\n",
      "      160220       1.00      0.97      0.98        89\n",
      "      160231       0.95      0.88      0.91        42\n",
      "      160232       1.00      0.50      0.67        12\n",
      "      160239       0.71      0.83      0.77         6\n",
      "      160241       1.00      0.92      0.96        86\n",
      "      160242       0.88      0.97      0.93       200\n",
      "      160249       0.81      0.85      0.83        73\n",
      "      160250       0.97      0.96      0.97       200\n",
      "      160290       0.97      0.76      0.85        41\n",
      "      160300       0.85      0.76      0.80        45\n",
      "      160411       0.86      0.96      0.91       200\n",
      "      160412       0.85      0.87      0.86        93\n",
      "      160413       0.93      0.90      0.91       200\n",
      "      160414       0.91      0.81      0.86       200\n",
      "      160415       0.86      0.98      0.92       100\n",
      "      160416       0.86      0.59      0.70        32\n",
      "      160417       1.00      0.33      0.50         3\n",
      "      160419       0.84      0.85      0.85       129\n",
      "      160420       0.90      0.89      0.90       200\n",
      "      160432       1.00      0.25      0.40         4\n",
      "      160510       0.94      0.92      0.93       200\n",
      "      160521       0.94      0.96      0.95       173\n",
      "      160529       0.90      0.96      0.93       119\n",
      "      160530       0.97      0.92      0.95        39\n",
      "      160540       0.95      0.80      0.87        51\n",
      "      160551       1.00      0.50      0.67         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       1.00      0.68      0.81        19\n",
      "      160554       0.80      0.83      0.81        29\n",
      "      160555       1.00      0.64      0.78        11\n",
      "      160556       1.00      0.46      0.63        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       1.00      0.80      0.89         5\n",
      "      160559       0.83      0.42      0.56        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.73      0.59      0.66        64\n",
      "      170113       0.88      0.41      0.56        17\n",
      "      170114       0.91      0.79      0.85        38\n",
      "      170191       0.63      0.78      0.69       200\n",
      "      170199       0.95      0.90      0.93       182\n",
      "      170211       0.91      0.87      0.89        55\n",
      "      170219       0.94      0.95      0.95       200\n",
      "      170220       0.79      0.81      0.80       191\n",
      "      170230       0.97      0.94      0.95       200\n",
      "      170240       0.58      0.67      0.62       200\n",
      "      170250       0.91      0.90      0.90        89\n",
      "      170260       0.67      0.73      0.70       200\n",
      "      170290       0.81      0.88      0.84       200\n",
      "      170310       0.89      0.96      0.92       130\n",
      "      170390       1.00      0.67      0.80         6\n",
      "      170410       0.96      0.87      0.91        75\n",
      "      170490       0.56      0.59      0.57       200\n",
      "      180100       0.95      0.93      0.94       200\n",
      "      180200       0.95      0.89      0.92        94\n",
      "      180310       0.93      0.95      0.94       188\n",
      "      180320       0.98      0.96      0.97        49\n",
      "      180400       0.88      0.92      0.90       200\n",
      "      180500       0.90      0.91      0.90       200\n",
      "      180610       0.72      0.72      0.72       200\n",
      "      180620       0.63      0.69      0.65       200\n",
      "      180631       0.85      0.78      0.81       189\n",
      "      180632       0.64      0.73      0.68       200\n",
      "      180690       0.61      0.68      0.64       200\n",
      "      190110       0.98      0.91      0.94       200\n",
      "      190120       0.71      0.79      0.75       200\n",
      "      190190       0.56      0.66      0.60       200\n",
      "      190211       0.91      0.86      0.88       113\n",
      "      190219       0.87      0.84      0.85       200\n",
      "      190220       0.92      0.87      0.89       120\n",
      "      190230       0.88      0.86      0.87       200\n",
      "      190240       0.87      0.63      0.73        41\n",
      "      190300       0.91      0.90      0.90       200\n",
      "      190410       0.74      0.82      0.78       200\n",
      "      190420       0.78      0.46      0.58        39\n",
      "      190430       0.93      0.95      0.94       200\n",
      "      190490       0.94      0.85      0.90       177\n",
      "      190510       0.96      0.72      0.82        32\n",
      "      190520       0.92      0.71      0.80        51\n",
      "      190531       0.82      0.80      0.81       200\n",
      "      190532       0.87      0.78      0.82       188\n",
      "      190540       0.91      0.88      0.90       200\n",
      "      190590       0.69      0.69      0.69       200\n",
      "\n",
      "    accuracy                           0.87     21023\n",
      "   macro avg       0.82      0.73      0.76     21023\n",
      "weighted avg       0.87      0.87      0.87     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   0   4 ...   0   0   0]\n",
      " [  0   0  91 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 146   0   1]\n",
      " [  0   0   0 ...   0 176   3]\n",
      " [  0   0   0 ...   4   4 137]]\n",
      "Created predictions in 797.7 seconds\n"
     ]
    }
   ],
   "source": [
    "trial8 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', SVC()),\n",
    "])\n",
    " \n",
    "acc = train(trial8, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/SVM_tfidf_model_1x.sav')\n",
    "\n",
    "results.at['SVM-tfidf Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "inclusive-tunisia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:30:46] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Trained model in 1.064e+04 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       0.00      0.00      0.00         4\n",
      "      100199       0.94      0.98      0.96        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       0.33      0.50      0.40         2\n",
      "      100410       1.00      0.91      0.95        11\n",
      "      100490       1.00      0.89      0.94        37\n",
      "      100510       0.94      0.91      0.93        56\n",
      "      100590       0.90      0.82      0.86        73\n",
      "      100610       0.86      0.91      0.88        75\n",
      "      100620       0.88      0.92      0.90       200\n",
      "      100630       0.88      0.91      0.89       200\n",
      "      100640       0.89      0.93      0.91       200\n",
      "      100810       0.83      1.00      0.91        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       0.67      0.50      0.57         4\n",
      "      100830       0.92      0.94      0.93       180\n",
      "      100840       1.00      1.00      1.00         1\n",
      "      100850       0.96      0.97      0.96       179\n",
      "      100890       0.90      0.91      0.90        96\n",
      "      110100       0.86      0.86      0.86       200\n",
      "      110220       0.89      0.94      0.91       157\n",
      "      110290       0.86      0.83      0.84       115\n",
      "      110311       0.75      0.60      0.67        20\n",
      "      110313       0.78      0.81      0.79        31\n",
      "      110319       0.93      0.96      0.95       200\n",
      "      110320       0.94      0.91      0.92        66\n",
      "      110412       0.97      0.95      0.96        91\n",
      "      110419       0.87      0.77      0.82        61\n",
      "      110422       0.86      1.00      0.92         6\n",
      "      110423       0.94      0.89      0.92        38\n",
      "      110429       0.62      0.43      0.51        23\n",
      "      110430       0.86      0.86      0.86         7\n",
      "      110510       0.93      0.91      0.92        68\n",
      "      110520       0.92      0.89      0.91        55\n",
      "      110610       0.74      0.66      0.70        86\n",
      "      110620       0.81      0.79      0.80        63\n",
      "      110630       0.88      0.88      0.88       146\n",
      "      110710       0.81      0.79      0.80       200\n",
      "      110720       0.72      0.86      0.78       127\n",
      "      110811       0.92      0.85      0.88        53\n",
      "      110812       0.90      0.87      0.89        63\n",
      "      110813       0.92      0.95      0.94       200\n",
      "      110814       0.92      0.95      0.93       200\n",
      "      110819       0.87      0.83      0.85        63\n",
      "      110820       0.98      1.00      0.99        46\n",
      "      110900       0.96      0.98      0.97       200\n",
      "      120110       0.81      0.85      0.83        20\n",
      "      120190       0.93      0.90      0.91       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.92      0.96      0.94        23\n",
      "      120300       1.00      0.62      0.77         8\n",
      "      120400       0.94      0.94      0.94       100\n",
      "      120510       0.80      0.73      0.76        22\n",
      "      120590       0.67      0.75      0.71         8\n",
      "      120600       0.95      0.95      0.95       200\n",
      "      120710       0.95      0.97      0.96       200\n",
      "      120730       0.94      0.91      0.92        32\n",
      "      120740       0.94      0.91      0.92       112\n",
      "      120750       0.91      0.89      0.90        36\n",
      "      120760       0.50      0.33      0.40         3\n",
      "      120791       1.00      0.94      0.97        47\n",
      "      120799       0.93      0.89      0.91       123\n",
      "      120810       0.94      0.93      0.94       177\n",
      "      120890       0.73      0.73      0.73        15\n",
      "      120910       0.82      0.82      0.82        11\n",
      "      120921       0.91      0.91      0.91        33\n",
      "      120922       0.84      0.67      0.74        24\n",
      "      120923       0.80      0.80      0.80        10\n",
      "      120924       0.67      0.84      0.75       200\n",
      "      120925       0.95      0.93      0.94        44\n",
      "      120929       0.93      0.87      0.90        30\n",
      "      120930       0.88      0.92      0.90        25\n",
      "      120991       0.85      0.94      0.89       200\n",
      "      120999       0.96      0.90      0.93        71\n",
      "      121010       0.33      0.14      0.20         7\n",
      "      121020       0.92      0.96      0.94        89\n",
      "      121120       1.00      0.97      0.99        35\n",
      "      121130       0.89      0.97      0.93        40\n",
      "      121140       1.00      0.80      0.89        10\n",
      "      121190       0.83      0.79      0.81       200\n",
      "      121221       0.90      0.64      0.75        14\n",
      "      121229       0.93      0.83      0.88        30\n",
      "      121291       0.98      0.98      0.98        43\n",
      "      121293       1.00      0.50      0.67         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.83      0.73      0.78        60\n",
      "      121300       0.50      0.17      0.25         6\n",
      "      121410       0.94      0.98      0.96       103\n",
      "      121490       1.00      0.97      0.98       173\n",
      "      130120       0.94      0.94      0.94       200\n",
      "      130190       0.92      0.84      0.88        56\n",
      "      130211       0.82      0.90      0.86        31\n",
      "      130212       0.76      0.93      0.84        14\n",
      "      130213       0.75      0.75      0.75         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.83      0.83      0.83       139\n",
      "      130220       0.95      0.86      0.90        21\n",
      "      130231       0.88      1.00      0.94        38\n",
      "      130232       0.94      0.94      0.94       200\n",
      "      130239       0.87      0.86      0.86        70\n",
      "      140110       0.94      0.97      0.96       200\n",
      "      140120       0.94      0.75      0.84        44\n",
      "      140190       1.00      0.78      0.88        18\n",
      "      140420       0.91      0.97      0.94        97\n",
      "      140490       0.86      0.78      0.82        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       1.00      0.80      0.89         5\n",
      "      150290       1.00      1.00      1.00         2\n",
      "      150300       1.00      0.44      0.62         9\n",
      "      150410       0.91      0.91      0.91        33\n",
      "      150420       0.72      0.75      0.73        24\n",
      "      150430       1.00      0.67      0.80         3\n",
      "      150500       0.98      0.97      0.97        93\n",
      "      150600       0.00      0.00      0.00         2\n",
      "      150710       0.81      0.92      0.87        66\n",
      "      150790       0.81      0.81      0.81       135\n",
      "      150810       0.96      0.98      0.97       200\n",
      "      150890       1.00      0.75      0.86         8\n",
      "      150910       0.89      0.85      0.87       200\n",
      "      150990       0.81      0.78      0.79       200\n",
      "      151000       0.83      0.90      0.86       200\n",
      "      151110       0.98      0.98      0.98       200\n",
      "      151190       0.95      0.96      0.96       200\n",
      "      151211       0.87      0.92      0.89        37\n",
      "      151219       0.90      0.82      0.86       107\n",
      "      151221       0.93      0.82      0.87        33\n",
      "      151229       0.94      0.94      0.94        47\n",
      "      151311       0.75      0.62      0.68        24\n",
      "      151319       0.87      0.89      0.88        90\n",
      "      151321       0.00      0.00      0.00         1\n",
      "      151329       0.96      0.97      0.97        74\n",
      "      151411       0.67      0.57      0.62         7\n",
      "      151419       0.91      0.93      0.92        67\n",
      "      151491       1.00      0.75      0.86         4\n",
      "      151499       0.57      1.00      0.73         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       0.80      0.80      0.80        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       0.50      0.25      0.33         4\n",
      "      151530       0.97      0.94      0.95        99\n",
      "      151550       1.00      0.92      0.96        50\n",
      "      151590       0.81      0.91      0.85       191\n",
      "      151610       0.99      0.94      0.96       108\n",
      "      151620       0.83      0.80      0.82        91\n",
      "      151710       0.92      0.85      0.88        27\n",
      "      151790       0.81      0.75      0.77       173\n",
      "      151800       0.70      0.78      0.74        40\n",
      "      152000       0.96      0.96      0.96       131\n",
      "      152110       0.92      0.92      0.92        36\n",
      "      152190       1.00      0.94      0.97        35\n",
      "      152200       1.00      0.60      0.75        10\n",
      "      160100       0.84      0.89      0.86       144\n",
      "      160210       1.00      0.55      0.71        11\n",
      "      160220       1.00      0.97      0.98        89\n",
      "      160231       0.88      0.90      0.89        42\n",
      "      160232       1.00      0.75      0.86        12\n",
      "      160239       0.71      0.83      0.77         6\n",
      "      160241       0.98      0.95      0.96        86\n",
      "      160242       0.92      0.93      0.92       200\n",
      "      160249       0.80      0.86      0.83        73\n",
      "      160250       0.96      0.96      0.96       200\n",
      "      160290       0.92      0.85      0.89        41\n",
      "      160300       0.95      0.84      0.89        45\n",
      "      160411       0.91      0.93      0.92       200\n",
      "      160412       0.92      0.83      0.87        93\n",
      "      160413       0.90      0.89      0.89       200\n",
      "      160414       0.85      0.80      0.82       200\n",
      "      160415       0.87      0.96      0.91       100\n",
      "      160416       0.95      0.56      0.71        32\n",
      "      160417       1.00      0.33      0.50         3\n",
      "      160419       0.86      0.88      0.87       129\n",
      "      160420       0.86      0.86      0.86       200\n",
      "      160432       1.00      0.50      0.67         4\n",
      "      160510       0.92      0.92      0.92       200\n",
      "      160521       0.96      0.95      0.96       173\n",
      "      160529       0.96      0.92      0.94       119\n",
      "      160530       0.95      0.92      0.94        39\n",
      "      160540       0.83      0.86      0.85        51\n",
      "      160551       0.75      0.75      0.75         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       0.88      0.79      0.83        19\n",
      "      160554       0.69      0.83      0.75        29\n",
      "      160555       0.75      0.82      0.78        11\n",
      "      160556       0.90      0.69      0.78        13\n",
      "      160557       1.00      0.50      0.67         2\n",
      "      160558       1.00      0.80      0.89         5\n",
      "      160559       0.75      0.50      0.60        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.77      0.62      0.69        64\n",
      "      170113       0.65      0.65      0.65        17\n",
      "      170114       0.89      0.82      0.85        38\n",
      "      170191       0.71      0.71      0.71       200\n",
      "      170199       0.95      0.91      0.93       182\n",
      "      170211       0.95      0.96      0.95        55\n",
      "      170219       0.92      0.95      0.94       200\n",
      "      170220       0.76      0.78      0.77       191\n",
      "      170230       0.94      0.92      0.93       200\n",
      "      170240       0.59      0.61      0.60       200\n",
      "      170250       0.82      0.89      0.85        89\n",
      "      170260       0.66      0.71      0.68       200\n",
      "      170290       0.79      0.81      0.80       200\n",
      "      170310       0.93      0.95      0.94       130\n",
      "      170390       0.83      0.83      0.83         6\n",
      "      170410       0.96      0.87      0.91        75\n",
      "      170490       0.56      0.51      0.53       200\n",
      "      180100       0.94      0.92      0.93       200\n",
      "      180200       0.88      0.84      0.86        94\n",
      "      180310       0.92      0.93      0.93       188\n",
      "      180320       0.96      0.98      0.97        49\n",
      "      180400       0.92      0.97      0.94       200\n",
      "      180500       0.91      0.93      0.92       200\n",
      "      180610       0.71      0.67      0.69       200\n",
      "      180620       0.63      0.65      0.64       200\n",
      "      180631       0.79      0.80      0.80       189\n",
      "      180632       0.69      0.69      0.69       200\n",
      "      180690       0.64      0.69      0.66       200\n",
      "      190110       0.93      0.95      0.94       200\n",
      "      190120       0.68      0.71      0.70       200\n",
      "      190190       0.61      0.58      0.60       200\n",
      "      190211       0.90      0.83      0.87       113\n",
      "      190219       0.80      0.81      0.80       200\n",
      "      190220       0.86      0.92      0.89       120\n",
      "      190230       0.84      0.88      0.86       200\n",
      "      190240       0.86      0.73      0.79        41\n",
      "      190300       0.91      0.89      0.90       200\n",
      "      190410       0.71      0.81      0.76       200\n",
      "      190420       0.71      0.62      0.66        39\n",
      "      190430       0.96      0.94      0.95       200\n",
      "      190490       0.86      0.83      0.84       177\n",
      "      190510       0.92      0.75      0.83        32\n",
      "      190520       0.97      0.76      0.86        51\n",
      "      190531       0.81      0.81      0.81       200\n",
      "      190532       0.84      0.79      0.81       188\n",
      "      190540       0.85      0.91      0.88       200\n",
      "      190590       0.63      0.63      0.63       200\n",
      "\n",
      "    accuracy                           0.86     21023\n",
      "   macro avg       0.80      0.76      0.77     21023\n",
      "weighted avg       0.86      0.86      0.86     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   0   3 ...   0   0   0]\n",
      " [  0   0  91 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 148   0   1]\n",
      " [  0   0   0 ...   1 181   5]\n",
      " [  0   0   0 ...   4   3 126]]\n",
      "Created predictions in 6.27 seconds\n"
     ]
    }
   ],
   "source": [
    "trial9 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(trial9, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/XGB_BOW_model_1x.sav')\n",
    "\n",
    "results.at['XGBoost-BOW Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aging-bibliography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:28:27] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Trained model in 1.097e+04 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       0.25      0.25      0.25         4\n",
      "      100199       0.96      0.96      0.96        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       1.00      0.50      0.67         2\n",
      "      100410       1.00      0.91      0.95        11\n",
      "      100490       0.97      0.89      0.93        37\n",
      "      100510       0.93      0.93      0.93        56\n",
      "      100590       0.90      0.85      0.87        73\n",
      "      100610       0.89      0.91      0.90        75\n",
      "      100620       0.86      0.89      0.88       200\n",
      "      100630       0.81      0.85      0.83       200\n",
      "      100640       0.84      0.93      0.88       200\n",
      "      100810       0.90      0.90      0.90        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       0.33      0.25      0.29         4\n",
      "      100830       0.89      0.96      0.92       180\n",
      "      100840       0.50      1.00      0.67         1\n",
      "      100850       0.95      0.97      0.96       179\n",
      "      100890       0.92      0.91      0.91        96\n",
      "      110100       0.83      0.83      0.83       200\n",
      "      110220       0.82      0.91      0.86       157\n",
      "      110290       0.81      0.82      0.81       115\n",
      "      110311       0.73      0.40      0.52        20\n",
      "      110313       0.83      0.77      0.80        31\n",
      "      110319       0.89      0.94      0.91       200\n",
      "      110320       0.95      0.92      0.94        66\n",
      "      110412       0.90      0.95      0.92        91\n",
      "      110419       0.85      0.72      0.78        61\n",
      "      110422       1.00      0.83      0.91         6\n",
      "      110423       0.89      0.84      0.86        38\n",
      "      110429       0.82      0.39      0.53        23\n",
      "      110430       1.00      0.71      0.83         7\n",
      "      110510       0.88      0.88      0.88        68\n",
      "      110520       0.96      0.91      0.93        55\n",
      "      110610       0.83      0.67      0.74        86\n",
      "      110620       0.85      0.71      0.78        63\n",
      "      110630       0.85      0.84      0.84       146\n",
      "      110710       0.79      0.86      0.83       200\n",
      "      110720       0.76      0.79      0.78       127\n",
      "      110811       0.96      0.81      0.88        53\n",
      "      110812       0.89      0.86      0.87        63\n",
      "      110813       0.90      0.94      0.92       200\n",
      "      110814       0.95      0.95      0.95       200\n",
      "      110819       0.87      0.87      0.87        63\n",
      "      110820       1.00      1.00      1.00        46\n",
      "      110900       0.96      0.94      0.95       200\n",
      "      120110       1.00      0.85      0.92        20\n",
      "      120190       0.93      0.91      0.92       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.92      0.96      0.94        23\n",
      "      120300       1.00      0.62      0.77         8\n",
      "      120400       0.96      0.93      0.94       100\n",
      "      120510       0.81      0.77      0.79        22\n",
      "      120590       0.75      0.75      0.75         8\n",
      "      120600       0.92      0.93      0.92       200\n",
      "      120710       0.95      0.95      0.95       200\n",
      "      120730       0.91      0.94      0.92        32\n",
      "      120740       0.96      0.85      0.90       112\n",
      "      120750       0.97      0.92      0.94        36\n",
      "      120760       1.00      0.33      0.50         3\n",
      "      120791       0.90      0.91      0.91        47\n",
      "      120799       0.89      0.89      0.89       123\n",
      "      120810       0.95      0.91      0.93       177\n",
      "      120890       0.71      0.67      0.69        15\n",
      "      120910       0.90      0.82      0.86        11\n",
      "      120921       0.97      0.88      0.92        33\n",
      "      120922       0.83      0.62      0.71        24\n",
      "      120923       1.00      0.70      0.82        10\n",
      "      120924       0.65      0.82      0.72       200\n",
      "      120925       0.91      0.91      0.91        44\n",
      "      120929       0.87      0.87      0.87        30\n",
      "      120930       0.85      0.88      0.86        25\n",
      "      120991       0.87      0.94      0.90       200\n",
      "      120999       0.97      0.89      0.93        71\n",
      "      121010       0.50      0.14      0.22         7\n",
      "      121020       0.89      0.94      0.92        89\n",
      "      121120       1.00      0.97      0.99        35\n",
      "      121130       0.88      0.95      0.92        40\n",
      "      121140       1.00      0.70      0.82        10\n",
      "      121190       0.74      0.81      0.77       200\n",
      "      121221       0.69      0.64      0.67        14\n",
      "      121229       1.00      0.77      0.87        30\n",
      "      121291       0.98      0.98      0.98        43\n",
      "      121293       1.00      0.50      0.67         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.74      0.58      0.65        60\n",
      "      121300       0.50      0.33      0.40         6\n",
      "      121410       0.98      0.96      0.97       103\n",
      "      121490       0.99      0.98      0.99       173\n",
      "      130120       0.94      0.96      0.95       200\n",
      "      130190       0.89      0.86      0.87        56\n",
      "      130211       0.93      0.87      0.90        31\n",
      "      130212       0.81      0.93      0.87        14\n",
      "      130213       0.67      0.50      0.57         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.78      0.83      0.80       139\n",
      "      130220       1.00      0.86      0.92        21\n",
      "      130231       0.93      1.00      0.96        38\n",
      "      130232       0.96      0.91      0.93       200\n",
      "      130239       0.94      0.86      0.90        70\n",
      "      140110       0.93      0.97      0.95       200\n",
      "      140120       0.82      0.64      0.72        44\n",
      "      140190       1.00      0.78      0.88        18\n",
      "      140420       0.95      0.98      0.96        97\n",
      "      140490       0.78      0.75      0.77        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       1.00      0.60      0.75         5\n",
      "      150290       1.00      0.50      0.67         2\n",
      "      150300       1.00      0.67      0.80         9\n",
      "      150410       0.83      0.73      0.77        33\n",
      "      150420       0.82      0.58      0.68        24\n",
      "      150430       1.00      0.67      0.80         3\n",
      "      150500       0.99      0.99      0.99        93\n",
      "      150600       1.00      0.50      0.67         2\n",
      "      150710       0.84      0.89      0.87        66\n",
      "      150790       0.81      0.81      0.81       135\n",
      "      150810       0.98      0.97      0.98       200\n",
      "      150890       0.86      0.75      0.80         8\n",
      "      150910       0.88      0.82      0.85       200\n",
      "      150990       0.78      0.84      0.81       200\n",
      "      151000       0.87      0.90      0.88       200\n",
      "      151110       0.98      0.99      0.98       200\n",
      "      151190       0.93      0.94      0.94       200\n",
      "      151211       0.85      0.89      0.87        37\n",
      "      151219       0.86      0.83      0.85       107\n",
      "      151221       0.80      0.85      0.82        33\n",
      "      151229       0.82      0.85      0.83        47\n",
      "      151311       0.94      0.71      0.81        24\n",
      "      151319       0.90      0.93      0.92        90\n",
      "      151321       0.00      0.00      0.00         1\n",
      "      151329       0.97      0.99      0.98        74\n",
      "      151411       0.75      0.86      0.80         7\n",
      "      151419       0.94      0.91      0.92        67\n",
      "      151491       1.00      0.25      0.40         4\n",
      "      151499       0.00      0.00      0.00         4\n",
      "      151511       0.00      0.00      0.00         1\n",
      "      151519       0.89      0.80      0.84        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       1.00      0.50      0.67         4\n",
      "      151530       0.97      0.95      0.96        99\n",
      "      151550       0.90      0.90      0.90        50\n",
      "      151590       0.83      0.87      0.85       191\n",
      "      151610       0.99      0.94      0.96       108\n",
      "      151620       0.82      0.81      0.82        91\n",
      "      151710       0.88      0.78      0.82        27\n",
      "      151790       0.75      0.76      0.76       173\n",
      "      151800       0.82      0.70      0.76        40\n",
      "      152000       0.94      0.98      0.96       131\n",
      "      152110       0.94      0.89      0.91        36\n",
      "      152190       1.00      0.94      0.97        35\n",
      "      152200       1.00      0.70      0.82        10\n",
      "      160100       0.79      0.88      0.83       144\n",
      "      160210       1.00      0.45      0.62        11\n",
      "      160220       0.99      0.96      0.97        89\n",
      "      160231       0.93      0.88      0.90        42\n",
      "      160232       0.82      0.75      0.78        12\n",
      "      160239       0.83      0.83      0.83         6\n",
      "      160241       0.91      0.92      0.91        86\n",
      "      160242       0.92      0.92      0.92       200\n",
      "      160249       0.89      0.88      0.88        73\n",
      "      160250       0.94      0.96      0.95       200\n",
      "      160290       0.92      0.80      0.86        41\n",
      "      160300       0.92      0.76      0.83        45\n",
      "      160411       0.87      0.95      0.91       200\n",
      "      160412       0.89      0.85      0.87        93\n",
      "      160413       0.86      0.87      0.87       200\n",
      "      160414       0.83      0.81      0.82       200\n",
      "      160415       0.88      0.91      0.90       100\n",
      "      160416       0.86      0.59      0.70        32\n",
      "      160417       1.00      0.33      0.50         3\n",
      "      160419       0.85      0.84      0.84       129\n",
      "      160420       0.83      0.85      0.84       200\n",
      "      160431       0.00      0.00      0.00         0\n",
      "      160432       1.00      0.25      0.40         4\n",
      "      160510       0.92      0.92      0.92       200\n",
      "      160521       0.95      0.94      0.94       173\n",
      "      160529       0.93      0.91      0.92       119\n",
      "      160530       0.92      0.92      0.92        39\n",
      "      160540       0.91      0.94      0.92        51\n",
      "      160551       1.00      0.50      0.67         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       0.93      0.68      0.79        19\n",
      "      160554       0.69      0.76      0.72        29\n",
      "      160555       0.75      0.55      0.63        11\n",
      "      160556       0.91      0.77      0.83        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       0.80      0.80      0.80         5\n",
      "      160559       0.45      0.42      0.43        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.73      0.64      0.68        64\n",
      "      170113       0.50      0.29      0.37        17\n",
      "      170114       0.74      0.68      0.71        38\n",
      "      170191       0.72      0.78      0.75       200\n",
      "      170199       0.95      0.88      0.91       182\n",
      "      170211       0.96      0.87      0.91        55\n",
      "      170219       0.94      0.95      0.95       200\n",
      "      170220       0.77      0.72      0.75       191\n",
      "      170230       0.92      0.92      0.92       200\n",
      "      170240       0.59      0.61      0.60       200\n",
      "      170250       0.86      0.91      0.89        89\n",
      "      170260       0.57      0.65      0.61       200\n",
      "      170290       0.81      0.81      0.81       200\n",
      "      170310       0.91      0.96      0.94       130\n",
      "      170390       1.00      0.83      0.91         6\n",
      "      170410       0.91      0.84      0.87        75\n",
      "      170490       0.58      0.53      0.55       200\n",
      "      180100       0.94      0.91      0.93       200\n",
      "      180200       0.88      0.97      0.92        94\n",
      "      180310       0.93      0.95      0.94       188\n",
      "      180320       0.94      0.96      0.95        49\n",
      "      180400       0.89      0.93      0.91       200\n",
      "      180500       0.90      0.89      0.90       200\n",
      "      180610       0.71      0.68      0.69       200\n",
      "      180620       0.65      0.66      0.65       200\n",
      "      180631       0.80      0.81      0.81       189\n",
      "      180632       0.66      0.74      0.70       200\n",
      "      180690       0.60      0.66      0.63       200\n",
      "      190110       0.95      0.92      0.93       200\n",
      "      190120       0.65      0.67      0.66       200\n",
      "      190190       0.64      0.57      0.60       200\n",
      "      190211       0.88      0.81      0.84       113\n",
      "      190219       0.75      0.81      0.77       200\n",
      "      190220       0.83      0.88      0.85       120\n",
      "      190230       0.83      0.83      0.83       200\n",
      "      190240       0.77      0.73      0.75        41\n",
      "      190300       0.92      0.92      0.92       200\n",
      "      190410       0.74      0.82      0.78       200\n",
      "      190420       0.79      0.49      0.60        39\n",
      "      190430       0.94      0.94      0.94       200\n",
      "      190490       0.83      0.82      0.82       177\n",
      "      190510       0.88      0.69      0.77        32\n",
      "      190520       0.85      0.78      0.82        51\n",
      "      190531       0.75      0.79      0.77       200\n",
      "      190532       0.78      0.77      0.77       188\n",
      "      190540       0.91      0.89      0.90       200\n",
      "      190590       0.65      0.64      0.64       200\n",
      "\n",
      "    accuracy                           0.85     21023\n",
      "   macro avg       0.79      0.73      0.75     21023\n",
      "weighted avg       0.85      0.85      0.85     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   1   2 ...   0   0   0]\n",
      " [  0   1  89 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 145   0   2]\n",
      " [  0   0   0 ...   3 177   2]\n",
      " [  0   0   0 ...   4   3 128]]\n",
      "Created predictions in 5.473 seconds\n"
     ]
    }
   ],
   "source": [
    "trial10 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(trial10, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/XGB_tfidf_model_1x.sav')\n",
    "\n",
    "results.at['XGBoost-tfidf Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "magnetic-gospel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 914.6 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       1.00      0.25      0.40         4\n",
      "      100199       0.93      0.99      0.96        93\n",
      "      100210       0.00      0.00      0.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       1.00      0.50      0.67         2\n",
      "      100410       1.00      0.82      0.90        11\n",
      "      100490       0.92      0.92      0.92        37\n",
      "      100510       0.95      0.96      0.96        56\n",
      "      100590       0.90      0.86      0.88        73\n",
      "      100610       0.86      0.92      0.89        75\n",
      "      100620       0.87      0.91      0.89       200\n",
      "      100630       0.88      0.95      0.91       200\n",
      "      100640       0.86      0.95      0.90       200\n",
      "      100810       0.95      0.90      0.92        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       1.00      0.50      0.67         4\n",
      "      100830       0.87      0.97      0.91       180\n",
      "      100840       1.00      1.00      1.00         1\n",
      "      100850       0.93      0.98      0.95       179\n",
      "      100890       0.95      0.90      0.92        96\n",
      "      110100       0.76      0.91      0.83       200\n",
      "      110220       0.80      0.92      0.86       157\n",
      "      110290       0.83      0.78      0.80       115\n",
      "      110311       0.86      0.60      0.71        20\n",
      "      110313       0.92      0.77      0.84        31\n",
      "      110319       0.90      0.95      0.92       200\n",
      "      110320       0.94      0.95      0.95        66\n",
      "      110412       0.92      0.96      0.94        91\n",
      "      110419       0.93      0.82      0.87        61\n",
      "      110422       1.00      1.00      1.00         6\n",
      "      110423       0.97      0.92      0.95        38\n",
      "      110429       0.81      0.57      0.67        23\n",
      "      110430       1.00      0.57      0.73         7\n",
      "      110510       0.89      0.94      0.91        68\n",
      "      110520       0.92      0.89      0.91        55\n",
      "      110610       0.81      0.69      0.74        86\n",
      "      110620       0.89      0.79      0.84        63\n",
      "      110630       0.88      0.87      0.88       146\n",
      "      110710       0.67      0.94      0.78       200\n",
      "      110720       0.78      0.43      0.55       127\n",
      "      110811       0.96      0.83      0.89        53\n",
      "      110812       0.90      0.87      0.89        63\n",
      "      110813       0.91      0.96      0.94       200\n",
      "      110814       0.95      0.96      0.96       200\n",
      "      110819       0.93      0.86      0.89        63\n",
      "      110820       0.96      0.98      0.97        46\n",
      "      110900       0.98      0.98      0.98       200\n",
      "      120110       0.90      0.95      0.93        20\n",
      "      120190       0.93      0.95      0.94       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.96      0.96      0.96        23\n",
      "      120300       0.83      0.62      0.71         8\n",
      "      120400       0.95      0.94      0.94       100\n",
      "      120510       0.82      0.82      0.82        22\n",
      "      120590       0.86      0.75      0.80         8\n",
      "      120600       0.91      0.96      0.93       200\n",
      "      120710       0.92      0.97      0.94       200\n",
      "      120730       0.97      0.91      0.94        32\n",
      "      120740       0.92      0.95      0.93       112\n",
      "      120750       0.94      0.94      0.94        36\n",
      "      120760       1.00      0.33      0.50         3\n",
      "      120791       1.00      0.96      0.98        47\n",
      "      120799       0.93      0.93      0.93       123\n",
      "      120810       0.94      0.90      0.92       177\n",
      "      120890       1.00      0.80      0.89        15\n",
      "      120910       0.82      0.82      0.82        11\n",
      "      120921       0.94      0.91      0.92        33\n",
      "      120922       0.81      0.54      0.65        24\n",
      "      120923       1.00      0.40      0.57        10\n",
      "      120924       0.66      0.85      0.74       200\n",
      "      120925       0.91      0.93      0.92        44\n",
      "      120929       0.96      0.87      0.91        30\n",
      "      120930       0.88      0.92      0.90        25\n",
      "      120991       0.86      0.94      0.90       200\n",
      "      120999       0.98      0.90      0.94        71\n",
      "      121010       0.40      0.29      0.33         7\n",
      "      121020       0.93      0.94      0.94        89\n",
      "      121120       0.97      0.97      0.97        35\n",
      "      121130       0.88      0.95      0.92        40\n",
      "      121140       1.00      0.80      0.89        10\n",
      "      121190       0.84      0.81      0.82       200\n",
      "      121221       1.00      0.79      0.88        14\n",
      "      121229       1.00      0.90      0.95        30\n",
      "      121291       0.93      1.00      0.97        43\n",
      "      121293       1.00      0.50      0.67         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.83      0.73      0.78        60\n",
      "      121300       1.00      0.50      0.67         6\n",
      "      121410       0.95      0.97      0.96       103\n",
      "      121490       0.99      0.99      0.99       173\n",
      "      130120       0.97      0.95      0.96       200\n",
      "      130190       0.96      0.86      0.91        56\n",
      "      130211       0.90      0.87      0.89        31\n",
      "      130212       0.81      0.93      0.87        14\n",
      "      130213       0.86      0.75      0.80         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.88      0.88      0.88       139\n",
      "      130220       0.95      0.95      0.95        21\n",
      "      130231       0.77      0.97      0.86        38\n",
      "      130232       0.94      0.96      0.95       200\n",
      "      130239       0.96      0.93      0.94        70\n",
      "      140110       0.90      0.98      0.94       200\n",
      "      140120       0.91      0.68      0.78        44\n",
      "      140190       1.00      0.83      0.91        18\n",
      "      140420       0.88      0.99      0.93        97\n",
      "      140490       0.88      0.78      0.82        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       1.00      1.00      1.00         5\n",
      "      150290       1.00      1.00      1.00         2\n",
      "      150300       0.67      0.67      0.67         9\n",
      "      150410       0.89      0.73      0.80        33\n",
      "      150420       0.91      0.88      0.89        24\n",
      "      150430       1.00      0.67      0.80         3\n",
      "      150500       0.98      0.99      0.98        93\n",
      "      150600       1.00      0.50      0.67         2\n",
      "      150710       0.83      0.89      0.86        66\n",
      "      150790       0.75      0.82      0.78       135\n",
      "      150810       0.97      0.98      0.98       200\n",
      "      150890       0.86      0.75      0.80         8\n",
      "      150910       0.83      0.89      0.86       200\n",
      "      150990       0.68      0.81      0.74       200\n",
      "      151000       0.81      0.90      0.85       200\n",
      "      151110       0.99      1.00      0.99       200\n",
      "      151190       0.93      0.96      0.95       200\n",
      "      151211       0.91      0.86      0.89        37\n",
      "      151219       0.88      0.85      0.86       107\n",
      "      151221       0.78      0.85      0.81        33\n",
      "      151229       0.87      0.85      0.86        47\n",
      "      151311       0.94      0.62      0.75        24\n",
      "      151319       0.91      0.94      0.93        90\n",
      "      151321       0.50      1.00      0.67         1\n",
      "      151329       0.97      0.96      0.97        74\n",
      "      151411       0.88      1.00      0.93         7\n",
      "      151419       0.88      0.94      0.91        67\n",
      "      151491       1.00      0.75      0.86         4\n",
      "      151499       1.00      0.25      0.40         4\n",
      "      151511       0.50      1.00      0.67         1\n",
      "      151519       1.00      0.70      0.82        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       1.00      0.25      0.40         4\n",
      "      151530       0.94      0.96      0.95        99\n",
      "      151550       0.96      0.94      0.95        50\n",
      "      151590       0.87      0.94      0.90       191\n",
      "      151610       0.98      0.95      0.97       108\n",
      "      151620       0.94      0.81      0.87        91\n",
      "      151710       0.89      0.89      0.89        27\n",
      "      151790       0.87      0.79      0.83       173\n",
      "      151800       0.82      0.78      0.79        40\n",
      "      152000       0.94      0.97      0.95       131\n",
      "      152110       0.89      0.92      0.90        36\n",
      "      152190       0.97      0.94      0.96        35\n",
      "      152200       1.00      0.70      0.82        10\n",
      "      160100       0.82      0.90      0.86       144\n",
      "      160210       0.86      0.55      0.67        11\n",
      "      160220       1.00      0.99      0.99        89\n",
      "      160231       0.95      0.93      0.94        42\n",
      "      160232       0.89      0.67      0.76        12\n",
      "      160239       0.71      0.83      0.77         6\n",
      "      160241       0.98      0.95      0.96        86\n",
      "      160242       0.88      0.96      0.92       200\n",
      "      160249       0.81      0.89      0.85        73\n",
      "      160250       0.97      0.96      0.97       200\n",
      "      160290       0.94      0.78      0.85        41\n",
      "      160300       0.97      0.76      0.85        45\n",
      "      160411       0.88      0.94      0.91       200\n",
      "      160412       0.92      0.82      0.86        93\n",
      "      160413       0.87      0.91      0.89       200\n",
      "      160414       0.88      0.84      0.86       200\n",
      "      160415       0.91      0.98      0.94       100\n",
      "      160416       0.86      0.75      0.80        32\n",
      "      160417       1.00      0.33      0.50         3\n",
      "      160419       0.88      0.87      0.88       129\n",
      "      160420       0.94      0.88      0.91       200\n",
      "      160432       1.00      0.25      0.40         4\n",
      "      160510       0.94      0.94      0.94       200\n",
      "      160521       0.93      0.97      0.95       173\n",
      "      160529       0.90      0.96      0.93       119\n",
      "      160530       0.97      0.92      0.95        39\n",
      "      160540       0.88      0.86      0.87        51\n",
      "      160551       1.00      0.50      0.67         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       0.93      0.74      0.82        19\n",
      "      160554       0.76      0.90      0.83        29\n",
      "      160555       0.82      0.82      0.82        11\n",
      "      160556       0.78      0.54      0.64        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       0.67      0.80      0.73         5\n",
      "      160559       0.75      0.50      0.60        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.69      0.64      0.67        64\n",
      "      170113       0.57      0.24      0.33        17\n",
      "      170114       0.80      0.87      0.84        38\n",
      "      170191       0.73      0.68      0.70       200\n",
      "      170199       0.94      0.89      0.92       182\n",
      "      170211       0.94      0.89      0.92        55\n",
      "      170219       0.90      0.96      0.93       200\n",
      "      170220       0.76      0.71      0.73       191\n",
      "      170230       0.94      0.94      0.94       200\n",
      "      170240       0.67      0.56      0.61       200\n",
      "      170250       0.86      0.96      0.90        89\n",
      "      170260       0.71      0.72      0.71       200\n",
      "      170290       0.84      0.81      0.83       200\n",
      "      170310       0.85      0.98      0.91       130\n",
      "      170390       1.00      0.67      0.80         6\n",
      "      170410       0.94      0.91      0.93        75\n",
      "      170490       0.78      0.52      0.62       200\n",
      "      180100       0.93      0.92      0.92       200\n",
      "      180200       0.91      0.93      0.92        94\n",
      "      180310       0.94      0.94      0.94       188\n",
      "      180320       0.96      0.98      0.97        49\n",
      "      180400       0.90      0.93      0.92       200\n",
      "      180500       0.87      0.93      0.90       200\n",
      "      180610       0.75      0.66      0.70       200\n",
      "      180620       0.66      0.62      0.64       200\n",
      "      180631       0.77      0.85      0.81       189\n",
      "      180632       0.76      0.64      0.69       200\n",
      "      180690       0.67      0.67      0.67       200\n",
      "      190110       0.94      0.95      0.95       200\n",
      "      190120       0.82      0.68      0.74       200\n",
      "      190190       0.81      0.58      0.67       200\n",
      "      190211       0.84      0.91      0.88       113\n",
      "      190219       0.85      0.86      0.85       200\n",
      "      190220       0.87      0.89      0.88       120\n",
      "      190230       0.90      0.85      0.87       200\n",
      "      190240       0.76      0.71      0.73        41\n",
      "      190300       0.91      0.94      0.93       200\n",
      "      190410       0.83      0.84      0.84       200\n",
      "      190420       0.84      0.54      0.66        39\n",
      "      190430       0.92      0.95      0.94       200\n",
      "      190490       0.88      0.88      0.88       177\n",
      "      190510       0.75      0.75      0.75        32\n",
      "      190520       0.88      0.84      0.86        51\n",
      "      190531       0.79      0.80      0.79       200\n",
      "      190532       0.82      0.78      0.80       188\n",
      "      190540       0.79      0.88      0.83       200\n",
      "      190590       0.78      0.62      0.69       200\n",
      "\n",
      "    accuracy                           0.87     21023\n",
      "   macro avg       0.83      0.77      0.79     21023\n",
      "weighted avg       0.87      0.87      0.87     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   1   3 ...   0   0   0]\n",
      " [  0   0  92 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 147   1   2]\n",
      " [  0   0   0 ...   1 176   2]\n",
      " [  0   0   0 ...   2   5 124]]\n",
      "Created predictions in 6.76 seconds\n"
     ]
    }
   ],
   "source": [
    "trial11 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "    ('classifier', RandomForestClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(trial11, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/RF_BOW_model_1x.sav')\n",
    "\n",
    "results.at['RF-BOW Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "seeing-algeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 1.035e+03 seconds\n",
      "Dev set results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      100119       0.00      0.00      0.00         1\n",
      "      100191       1.00      0.25      0.40         4\n",
      "      100199       0.95      0.99      0.97        93\n",
      "      100210       1.00      1.00      1.00         1\n",
      "      100290       0.00      0.00      0.00         2\n",
      "      100310       0.00      0.00      0.00         1\n",
      "      100390       1.00      0.50      0.67         2\n",
      "      100410       1.00      0.91      0.95        11\n",
      "      100490       0.97      0.92      0.94        37\n",
      "      100510       0.96      0.91      0.94        56\n",
      "      100590       0.97      0.88      0.92        73\n",
      "      100610       0.88      0.95      0.91        75\n",
      "      100620       0.89      0.91      0.90       200\n",
      "      100630       0.87      0.94      0.90       200\n",
      "      100640       0.85      0.93      0.89       200\n",
      "      100810       0.90      0.90      0.90        20\n",
      "      100821       0.00      0.00      0.00         2\n",
      "      100829       1.00      0.50      0.67         4\n",
      "      100830       0.84      0.96      0.90       180\n",
      "      100840       1.00      1.00      1.00         1\n",
      "      100850       0.94      0.98      0.96       179\n",
      "      100890       0.93      0.94      0.93        96\n",
      "      110100       0.86      0.94      0.90       200\n",
      "      110220       0.78      0.96      0.86       157\n",
      "      110290       0.84      0.89      0.86       115\n",
      "      110311       0.92      0.60      0.73        20\n",
      "      110313       0.87      0.84      0.85        31\n",
      "      110319       0.91      0.95      0.93       200\n",
      "      110320       0.95      0.92      0.94        66\n",
      "      110412       0.93      0.93      0.93        91\n",
      "      110419       0.93      0.82      0.87        61\n",
      "      110422       1.00      1.00      1.00         6\n",
      "      110423       1.00      0.95      0.97        38\n",
      "      110429       0.80      0.52      0.63        23\n",
      "      110430       1.00      0.57      0.73         7\n",
      "      110510       0.95      0.93      0.94        68\n",
      "      110520       0.91      0.89      0.90        55\n",
      "      110610       0.90      0.71      0.79        86\n",
      "      110620       0.94      0.79      0.86        63\n",
      "      110630       0.90      0.89      0.89       146\n",
      "      110710       0.85      0.94      0.89       200\n",
      "      110720       0.91      0.85      0.88       127\n",
      "      110811       0.96      0.83      0.89        53\n",
      "      110812       0.94      0.81      0.87        63\n",
      "      110813       0.92      0.96      0.94       200\n",
      "      110814       0.94      0.97      0.95       200\n",
      "      110819       0.92      0.87      0.89        63\n",
      "      110820       0.94      1.00      0.97        46\n",
      "      110900       0.96      0.98      0.97       200\n",
      "      120110       1.00      0.90      0.95        20\n",
      "      120190       0.93      0.98      0.96       200\n",
      "      120241       0.00      0.00      0.00         1\n",
      "      120242       0.96      1.00      0.98        23\n",
      "      120300       1.00      0.62      0.77         8\n",
      "      120400       0.92      0.96      0.94       100\n",
      "      120510       0.91      0.91      0.91        22\n",
      "      120590       0.80      0.50      0.62         8\n",
      "      120600       0.92      0.97      0.94       200\n",
      "      120710       0.93      0.96      0.95       200\n",
      "      120730       0.97      0.94      0.95        32\n",
      "      120740       0.95      0.97      0.96       112\n",
      "      120750       0.97      0.94      0.96        36\n",
      "      120760       1.00      0.33      0.50         3\n",
      "      120791       1.00      0.96      0.98        47\n",
      "      120799       0.93      0.92      0.92       123\n",
      "      120810       0.96      0.93      0.94       177\n",
      "      120890       0.92      0.73      0.81        15\n",
      "      120910       0.82      0.82      0.82        11\n",
      "      120921       0.91      0.88      0.89        33\n",
      "      120922       0.87      0.54      0.67        24\n",
      "      120923       1.00      0.40      0.57        10\n",
      "      120924       0.71      0.80      0.75       200\n",
      "      120925       0.91      0.95      0.93        44\n",
      "      120929       0.89      0.83      0.86        30\n",
      "      120930       0.96      0.92      0.94        25\n",
      "      120991       0.89      0.96      0.92       200\n",
      "      120999       0.98      0.90      0.94        71\n",
      "      121010       0.75      0.43      0.55         7\n",
      "      121020       0.92      0.98      0.95        89\n",
      "      121120       0.97      0.94      0.96        35\n",
      "      121130       0.87      0.97      0.92        40\n",
      "      121140       1.00      0.80      0.89        10\n",
      "      121190       0.88      0.89      0.89       200\n",
      "      121221       0.90      0.64      0.75        14\n",
      "      121229       1.00      0.83      0.91        30\n",
      "      121291       0.98      1.00      0.99        43\n",
      "      121293       1.00      0.50      0.67         2\n",
      "      121294       0.00      0.00      0.00         1\n",
      "      121299       0.96      0.77      0.85        60\n",
      "      121300       1.00      0.50      0.67         6\n",
      "      121410       0.96      0.97      0.97       103\n",
      "      121490       0.98      0.99      0.99       173\n",
      "      130120       0.97      0.96      0.97       200\n",
      "      130190       0.96      0.95      0.95        56\n",
      "      130211       0.96      0.84      0.90        31\n",
      "      130212       0.81      0.93      0.87        14\n",
      "      130213       0.88      0.88      0.88         8\n",
      "      130214       0.00      0.00      0.00         1\n",
      "      130219       0.91      0.95      0.93       139\n",
      "      130220       0.95      0.95      0.95        21\n",
      "      130231       0.93      1.00      0.96        38\n",
      "      130232       0.95      0.95      0.95       200\n",
      "      130239       0.90      0.94      0.92        70\n",
      "      140110       0.90      0.99      0.94       200\n",
      "      140120       0.89      0.73      0.80        44\n",
      "      140190       1.00      0.78      0.88        18\n",
      "      140420       0.92      0.98      0.95        97\n",
      "      140490       0.94      0.85      0.89        72\n",
      "      150110       0.00      0.00      0.00         1\n",
      "      150120       0.00      0.00      0.00         1\n",
      "      150190       0.00      0.00      0.00         1\n",
      "      150210       1.00      1.00      1.00         5\n",
      "      150290       1.00      1.00      1.00         2\n",
      "      150300       1.00      0.78      0.88         9\n",
      "      150410       0.81      0.76      0.78        33\n",
      "      150420       0.90      0.79      0.84        24\n",
      "      150430       1.00      0.33      0.50         3\n",
      "      150500       0.98      0.98      0.98        93\n",
      "      150600       1.00      0.50      0.67         2\n",
      "      150710       0.86      0.92      0.89        66\n",
      "      150790       0.80      0.82      0.81       135\n",
      "      150810       0.98      0.98      0.98       200\n",
      "      150890       0.75      0.75      0.75         8\n",
      "      150910       0.85      0.90      0.88       200\n",
      "      150990       0.72      0.83      0.77       200\n",
      "      151000       0.87      0.91      0.89       200\n",
      "      151110       0.99      1.00      0.99       200\n",
      "      151190       0.92      0.97      0.94       200\n",
      "      151211       0.92      0.92      0.92        37\n",
      "      151219       0.94      0.90      0.92       107\n",
      "      151221       0.85      0.85      0.85        33\n",
      "      151229       0.91      0.89      0.90        47\n",
      "      151311       0.94      0.62      0.75        24\n",
      "      151319       0.90      0.96      0.92        90\n",
      "      151321       0.50      1.00      0.67         1\n",
      "      151329       0.96      0.93      0.95        74\n",
      "      151411       1.00      1.00      1.00         7\n",
      "      151419       0.94      0.97      0.96        67\n",
      "      151491       1.00      0.75      0.86         4\n",
      "      151499       1.00      0.25      0.40         4\n",
      "      151511       1.00      1.00      1.00         1\n",
      "      151519       1.00      0.70      0.82        10\n",
      "      151521       0.00      0.00      0.00         1\n",
      "      151529       1.00      0.25      0.40         4\n",
      "      151530       0.98      0.98      0.98        99\n",
      "      151550       0.96      0.94      0.95        50\n",
      "      151590       0.92      0.96      0.94       191\n",
      "      151610       0.98      0.94      0.96       108\n",
      "      151620       0.98      0.93      0.96        91\n",
      "      151710       0.92      0.85      0.88        27\n",
      "      151790       0.87      0.82      0.84       173\n",
      "      151800       0.89      0.80      0.84        40\n",
      "      152000       0.95      0.98      0.96       131\n",
      "      152110       0.92      0.92      0.92        36\n",
      "      152190       1.00      0.91      0.96        35\n",
      "      152200       1.00      0.70      0.82        10\n",
      "      160100       0.86      0.90      0.88       144\n",
      "      160210       0.86      0.55      0.67        11\n",
      "      160220       0.99      0.97      0.98        89\n",
      "      160231       0.95      0.90      0.93        42\n",
      "      160232       0.88      0.58      0.70        12\n",
      "      160239       0.50      1.00      0.67         6\n",
      "      160241       1.00      0.97      0.98        86\n",
      "      160242       0.84      0.94      0.89       200\n",
      "      160249       0.81      0.86      0.83        73\n",
      "      160250       0.94      0.96      0.95       200\n",
      "      160290       0.97      0.76      0.85        41\n",
      "      160300       0.97      0.73      0.84        45\n",
      "      160411       0.88      0.96      0.92       200\n",
      "      160412       0.90      0.83      0.86        93\n",
      "      160413       0.89      0.94      0.91       200\n",
      "      160414       0.87      0.80      0.83       200\n",
      "      160415       0.82      0.97      0.89       100\n",
      "      160416       0.96      0.78      0.86        32\n",
      "      160417       1.00      0.33      0.50         3\n",
      "      160419       0.89      0.85      0.87       129\n",
      "      160420       0.92      0.92      0.92       200\n",
      "      160432       0.00      0.00      0.00         4\n",
      "      160510       0.93      0.95      0.94       200\n",
      "      160521       0.91      0.96      0.94       173\n",
      "      160529       0.88      0.94      0.91       119\n",
      "      160530       0.97      0.87      0.92        39\n",
      "      160540       0.94      0.86      0.90        51\n",
      "      160551       0.67      0.50      0.57         4\n",
      "      160552       0.00      0.00      0.00         1\n",
      "      160553       1.00      0.79      0.88        19\n",
      "      160554       0.79      0.93      0.86        29\n",
      "      160555       0.71      0.91      0.80        11\n",
      "      160556       0.75      0.69      0.72        13\n",
      "      160557       0.00      0.00      0.00         2\n",
      "      160558       0.57      0.80      0.67         5\n",
      "      160559       1.00      0.67      0.80        12\n",
      "      160561       0.00      0.00      0.00         1\n",
      "      160569       0.00      0.00      0.00         1\n",
      "      170112       0.71      0.66      0.68        64\n",
      "      170113       0.77      0.59      0.67        17\n",
      "      170114       0.88      0.95      0.91        38\n",
      "      170191       0.73      0.78      0.75       200\n",
      "      170199       0.96      0.93      0.94       182\n",
      "      170211       0.96      0.91      0.93        55\n",
      "      170219       0.88      0.97      0.93       200\n",
      "      170220       0.82      0.76      0.79       191\n",
      "      170230       0.95      0.95      0.95       200\n",
      "      170240       0.66      0.59      0.62       200\n",
      "      170250       0.85      0.94      0.89        89\n",
      "      170260       0.69      0.69      0.69       200\n",
      "      170290       0.83      0.79      0.81       200\n",
      "      170310       0.91      0.97      0.94       130\n",
      "      170390       1.00      0.83      0.91         6\n",
      "      170410       0.92      0.91      0.91        75\n",
      "      170490       0.74      0.55      0.63       200\n",
      "      180100       0.94      0.94      0.94       200\n",
      "      180200       0.86      0.94      0.90        94\n",
      "      180310       0.92      0.96      0.94       188\n",
      "      180320       0.98      1.00      0.99        49\n",
      "      180400       0.91      0.94      0.92       200\n",
      "      180500       0.88      0.94      0.91       200\n",
      "      180610       0.77      0.71      0.74       200\n",
      "      180620       0.72      0.64      0.67       200\n",
      "      180631       0.88      0.90      0.89       189\n",
      "      180632       0.77      0.67      0.71       200\n",
      "      180690       0.75      0.74      0.75       200\n",
      "      190110       0.95      0.95      0.95       200\n",
      "      190120       0.79      0.72      0.76       200\n",
      "      190190       0.89      0.72      0.80       200\n",
      "      190211       0.92      0.93      0.93       113\n",
      "      190219       0.86      0.90      0.88       200\n",
      "      190220       0.93      0.93      0.93       120\n",
      "      190230       0.91      0.87      0.89       200\n",
      "      190240       0.85      0.71      0.77        41\n",
      "      190300       0.93      0.96      0.95       200\n",
      "      190410       0.87      0.88      0.87       200\n",
      "      190420       0.88      0.56      0.69        39\n",
      "      190430       0.92      0.94      0.93       200\n",
      "      190490       0.91      0.90      0.91       177\n",
      "      190510       0.85      0.69      0.76        32\n",
      "      190520       0.95      0.76      0.85        51\n",
      "      190531       0.78      0.84      0.81       200\n",
      "      190532       0.90      0.86      0.88       188\n",
      "      190540       0.91      0.90      0.90       200\n",
      "      190590       0.81      0.74      0.77       200\n",
      "\n",
      "    accuracy                           0.89     21023\n",
      "   macro avg       0.84      0.79      0.80     21023\n",
      "weighted avg       0.89      0.89      0.89     21023\n",
      "\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   1   3 ...   0   0   0]\n",
      " [  0   0  92 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 162   0   1]\n",
      " [  0   0   0 ...   0 180   2]\n",
      " [  0   0   0 ...   2   5 148]]\n",
      "Created predictions in 7.138 seconds\n"
     ]
    }
   ],
   "source": [
    "trial12 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', RandomForestClassifier()),\n",
    "])\n",
    " \n",
    "acc = train(trial12, X_train['Desc'], y_train, X_dev['Desc'], y_dev, 'saved_models/RF_tfidf_model_1x.sav')\n",
    "\n",
    "results.at['RF-tfidf Desc','Accuracy'] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-brooks",
   "metadata": {},
   "source": [
    "### Reload models, generate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "recognized-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "\n",
    "def load_model(filename):\n",
    "    loaded = pickle.load(open(filename, 'rb'))\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "systematic-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_bow_mod = load_model('saved_models/NB_BOW_model_1x.sav')\n",
    "results.at['NB-BOW Desc','Accuracy'] = nb_bow_mod.score(X_dev['Desc'], y_dev)\n",
    "\n",
    "nb_tfidf_mod = load_model('saved_models/NB_tfidf_model_1x.sav')\n",
    "results.at['NB-tfidf Desc','Accuracy'] = nb_tfidf_mod.score(X_dev['Desc'], y_dev)\n",
    "\n",
    "knn_bow_mod = load_model('saved_models/KNN_BOW_model_1x.sav')\n",
    "results.at['KNN-BOW Desc','Accuracy'] = knn_bow_mod.score(X_dev['Desc'], y_dev)\n",
    "\n",
    "knn_tfidf_mod = load_model('saved_models/KNN_tfidf_model_1x.sav')\n",
    "results.at['KNN-tfidf Desc','Accuracy'] = knn_tfidf_mod.score(X_dev['Desc'], y_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tight-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow_mod = load_model('saved_models/LogReg_BOW_model_1x.sav')\n",
    "results.at['LogReg-BOW Desc','Accuracy'] = lr_bow_mod.score(X_dev['Desc'], y_dev)\n",
    "\n",
    "lr_tfidf_mod = load_model('saved_models/LogReg_tfidf_model_1x.sav')\n",
    "results.at['LogReg-tfidf Desc','Accuracy'] = lr_tfidf_mod.score(X_dev['Desc'], y_dev)\n",
    "\n",
    "rf_bow_mod = load_model('saved_models/RF_BOW_model_1x.sav')\n",
    "results.at['RF-BOW Desc','Accuracy'] = rf_bow_mod.score(X_dev['Desc'], y_dev)\n",
    "\n",
    "rf_tfidf_mod = load_model('saved_models/RF_tfidf_model_1x.sav')\n",
    "results.at['RF-tfidf Desc','Accuracy'] = rf_tfidf_mod.score(X_dev['Desc'], y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hispanic-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bow_mod = load_model('saved_models/XGB_BOW_model_1x.sav')\n",
    "results.at['XGBoost-BOW Desc','Accuracy'] = xgb_bow_mod.score(X_dev['Desc'], y_dev)\n",
    "\n",
    "xgb_tfidf_mod = load_model('saved_models/XGB_tfidf_model_1x.sav')\n",
    "results.at['XGBoost-tfidf Desc','Accuracy'] = xgb_tfidf_mod.score(X_dev['Desc'], y_dev)\n",
    "\n",
    "svm_bow_mod = load_model('saved_models/SVM_BOW_model_1x.sav')\n",
    "results.at['SVM-BOW Desc','Accuracy'] = svm_bow_mod.score(X_dev['Desc'], y_dev)\n",
    "\n",
    "svm_tfidf_mod = load_model('saved_models/SVM_tfidf_model_1x.sav')\n",
    "results.at['SVM-tfidf Desc','Accuracy'] = svm_tfidf_mod.score(X_dev['Desc'], y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "destroyed-standard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.009526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB-BOW Desc</th>\n",
       "      <td>0.679732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB-tfidf Desc</th>\n",
       "      <td>0.681016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN-BOW Desc</th>\n",
       "      <td>0.778814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN-tfidf Desc</th>\n",
       "      <td>0.784664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg-BOW Desc</th>\n",
       "      <td>0.870998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogReg-tfidf Desc</th>\n",
       "      <td>0.83989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-BOW Desc</th>\n",
       "      <td>0.592827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-tfidf Desc</th>\n",
       "      <td>0.864149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost-BOW Desc</th>\n",
       "      <td>0.857109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost-tfidf Desc</th>\n",
       "      <td>0.845645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-BOW Desc</th>\n",
       "      <td>0.868763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-tfidf Desc</th>\n",
       "      <td>0.887837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Accuracy\n",
       "Baseline            0.009526\n",
       "NB-BOW Desc         0.679732\n",
       "NB-tfidf Desc       0.681016\n",
       "KNN-BOW Desc        0.778814\n",
       "KNN-tfidf Desc      0.784664\n",
       "LogReg-BOW Desc     0.870998\n",
       "LogReg-tfidf Desc    0.83989\n",
       "SVM-BOW Desc        0.592827\n",
       "SVM-tfidf Desc      0.864149\n",
       "XGBoost-BOW Desc    0.857109\n",
       "XGBoost-tfidf Desc  0.845645\n",
       "RF-BOW Desc         0.868763\n",
       "RF-tfidf Desc       0.887837"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "tested-wholesale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Model Comparison, Chapters 10 - 19')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(x=results.index, y=results[\"Accuracy\"])\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=75)\n",
    "plt.title(\"Model Comparison, Chapters 10 - 19\", size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "helpful-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('ModelResults_Chps_10_19'+ '03032021' + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-divide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
