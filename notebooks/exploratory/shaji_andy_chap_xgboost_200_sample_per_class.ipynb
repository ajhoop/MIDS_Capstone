{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wrong-fiber",
   "metadata": {},
   "source": [
    "## References & starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-excess",
   "metadata": {},
   "source": [
    "https://github.com/dmlc/xgboost/blob/master/demo/guide-python/basic_walkthrough.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alone-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "backed-aerospace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.90'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interpreted-genome",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assisted-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "becoming-drilling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_common_metadata  part.24.parquet  part.41.parquet  part.59.parquet\n",
      "_metadata\t  part.25.parquet  part.42.parquet  part.6.parquet\n",
      "part.0.parquet\t  part.26.parquet  part.43.parquet  part.60.parquet\n",
      "part.1.parquet\t  part.27.parquet  part.44.parquet  part.61.parquet\n",
      "part.10.parquet   part.28.parquet  part.45.parquet  part.62.parquet\n",
      "part.11.parquet   part.29.parquet  part.46.parquet  part.63.parquet\n",
      "part.12.parquet   part.3.parquet   part.47.parquet  part.64.parquet\n",
      "part.13.parquet   part.30.parquet  part.48.parquet  part.65.parquet\n",
      "part.14.parquet   part.31.parquet  part.49.parquet  part.66.parquet\n",
      "part.15.parquet   part.32.parquet  part.5.parquet   part.67.parquet\n",
      "part.16.parquet   part.33.parquet  part.50.parquet  part.68.parquet\n",
      "part.17.parquet   part.34.parquet  part.51.parquet  part.69.parquet\n",
      "part.18.parquet   part.35.parquet  part.52.parquet  part.7.parquet\n",
      "part.19.parquet   part.36.parquet  part.53.parquet  part.70.parquet\n",
      "part.2.parquet\t  part.37.parquet  part.54.parquet  part.71.parquet\n",
      "part.20.parquet   part.38.parquet  part.55.parquet  part.8.parquet\n",
      "part.21.parquet   part.39.parquet  part.56.parquet  part.9.parquet\n",
      "part.22.parquet   part.4.parquet   part.57.parquet\n",
      "part.23.parquet   part.40.parquet  part.58.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls /data/work/shajikk/0308/data/full_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "premier-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dd = dd.read_parquet('/data/work/shajikk/0308/data/full_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "external-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>090111</td>\n",
       "      <td>1 x 20 container described as 275 bags green c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>090111</td>\n",
       "      <td>arabica green coffee beans</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "index                                                           \n",
       "0      090111  1 x 20 container described as 275 bags green c...\n",
       "1      090111                         arabica green coffee beans"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cubic-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files1 = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_1*.parq')\n",
    "# files2 = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_2*.parq')\n",
    "# files3 = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_3*.parq')\n",
    "# files4 = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_4*.parq')\n",
    "# files5 = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_5*.parq')\n",
    "# files6 = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_6*.parq')\n",
    "# files7 = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_7*.parq')\n",
    "# files8 = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_8*.parq')\n",
    "# files9 = glob.glob('/data/common/trade_data/2019_updated/data_samples_ignore_multiple_hscode/sample_by_chapter/sample_chap_9*.parq')\n",
    "\n",
    "# all_files = files1 + files2 + files3 + files4 + files5 + files6 + files7 + files8 + files9\n",
    "# result_dd = dd.concat([dd.read_parquet(fp) for fp in all_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "canadian-sleeve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 19.9s\n",
      "                                                Desc  HSCode\n",
      "0  SUPREME ALL PURPOSE LARD IN PAILS 37LBS CAED N...  150110\n",
      "1  SLAC: 840 PAILS NABORI ALL PURPOSE LARD HS COD...  150110\n",
      "[########################################] | 100% Completed | 48.1s\n",
      "1982138\n"
     ]
    }
   ],
   "source": [
    "# df1 = result_dd[['Product Desc', 'Cleaned_HS_Code']]\n",
    "# df1.columns = ['Desc', 'HSCode']\n",
    "\n",
    "# print(df1.head(2))\n",
    "# print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cross-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 46.0s\n",
      "   HSCode                                               Desc\n",
      "0  150110  Pig fat (including lard) and poultry fat, othe...\n",
      "3  150120  Pig fat (including lard) and poultry fat, othe...\n",
      "[########################################] | 100% Completed | 49.5s\n",
      "4137\n"
     ]
    }
   ],
   "source": [
    "# hs_code_desc = result_dd[['Cleaned_HS_Code', 'Merged_Description']]\n",
    "# hs_code_desc = hs_code_desc.drop_duplicates()\n",
    "# hs_code_desc.columns = ['HSCode', 'Desc']\n",
    "# print(hs_code_desc.head(2))\n",
    "# print(len(hs_code_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "clinical-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    object\n",
       "text     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "perfect-bibliography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1.4s\n",
      "[########################################] | 100% Completed |  0.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09</td>\n",
       "      <td>1 x 20 container described as 275 bags green c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78</td>\n",
       "      <td>refined lead hs code: xxxx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "index                                                         \n",
       "0        09  1 x 20 container described as 275 bags green c...\n",
       "0        78                         refined lead hs code: xxxx"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result_dd = result_dd.append(hs_code_desc[['Desc', 'HSCode']]).reset_index()\n",
    "result_df = result_dd.compute()\n",
    "result_df.label = result_df.label.astype(str).str[:2]\n",
    "result_dd = dd.from_pandas(result_df, npartitions=3)\n",
    "result_dd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-emission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "theoretical-discount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.2s\n",
      "Number of classes :  72\n"
     ]
    }
   ],
   "source": [
    "all_labels = result_dd.label.unique()\n",
    "print('Number of classes : ', len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "square-cradle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1982138"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "artificial-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n"
     ]
    }
   ],
   "source": [
    "result = result_dd.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-sphere",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "finished-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tok = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "def clean_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "    remove_dig_pun = tok.tokenize(sentence.lower())\n",
    "\n",
    "    nltk_tagged = nltk.pos_tag(remove_dig_pun)  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "            \n",
    "    lemmatized_sentence_clean = list(map((lambda x : x if x not in stop else \"\"), lemmatized_sentence))\n",
    "    input_clean = list(map((lambda x : x if x not in stop else \"\"), remove_dig_pun))\n",
    "\n",
    "    return \" \".join(input_clean + lemmatized_sentence_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-zimbabwe",
   "metadata": {},
   "source": [
    "## Read the HTS descriptions, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "narrative-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "hts = pd.read_csv(\"/data/work/shajikk/0308/hts_train.csv\", dtype={'hs': str, 'desc' : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "transsexual-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "hts['clean'] = hts.desc.apply(lambda x : clean_sentence(x))\n",
    "hts = hts.rename({'hs' : 'label', 'clean' : 'text'}, axis=1)[['label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hungry-exposure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>live horses asses mules  hinnies horses purebr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01</td>\n",
       "      <td>live horses asses mules  hinnies horses import...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0    01  live horses asses mules  hinnies horses purebr...\n",
       "1    01  live horses asses mules  hinnies horses import..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hts.label = hts.label.astype(str).str[:2]\n",
    "# hts = dd.from_pandas(hts_df, npartitions=3)\n",
    "# result_dd.head(2)\n",
    "hts.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-dakota",
   "metadata": {},
   "source": [
    "## Read the NACIS->HTS examples, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pursuant-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "nacis = pd.read_csv(\"/data/work/shajikk/0308/commodity_hts_extract.csv\", dtype={'hts6': str, 'description_long' : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "arctic-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "nacis['clean'] = nacis.description_long.apply(lambda x : clean_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "greatest-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "nacis = nacis.rename({'hts6' : 'label', 'clean' : 'text'}, axis=1)[['label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "orange-multimedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910211</td>\n",
       "      <td>batteries  wrist watches battery powered  mech...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text\n",
       "0  910211  batteries  wrist watches battery powered  mech..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nacis.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-sponsorship",
   "metadata": {},
   "source": [
    "## Sample the full data, 4137 classes. Create a subset of 100 examples each per class. Save it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dried-survivor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.2s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86605f05110042f38a74f4a8a15fd55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.2s\n"
     ]
    }
   ],
   "source": [
    "all_train_df = []\n",
    "all_valid_df = []\n",
    "count = 200\n",
    "recompute = True\n",
    "\n",
    "if not recompute :\n",
    "    with open('all_train_df_200_hschap.pkl', 'rb') as f: all_train_df = pickle.load(f)\n",
    "    with open('all_valid_df_200_hschap.pkl', 'rb') as f: all_valid_df = pickle.load(f)\n",
    "    pass\n",
    "\n",
    "\n",
    "for c in tqdm(all_labels) :\n",
    "    if (not recompute) : break\n",
    "    df = result[result.label == c]\n",
    "    df_sampled = df.sample(frac=min(count/len(df), 1))\n",
    "    df_hts = hts[hts.label == c]\n",
    "    df_nacis  = nacis[nacis.label == c]\n",
    "    \n",
    "    \n",
    "    train_df  = df_sampled.sample(frac=0.8)\n",
    "    valid_df = df_sampled.drop(train_df.index)\n",
    "\n",
    "    all_train_df.append(train_df)\n",
    "    all_train_df.append(df_hts)\n",
    "    all_train_df.append(df_nacis)\n",
    "    \n",
    "    all_valid_df.append(valid_df)\n",
    "\n",
    "train_df  = pd.concat(all_train_df)\n",
    "valid_df  = pd.concat(all_valid_df)\n",
    "\n",
    "if recompute :\n",
    "    with open('all_train_df_200_hschap.pkl', 'wb') as f: pickle.dump(all_train_df, f)\n",
    "    with open('all_valid_df_200_hschap.pkl', 'wb') as f: pickle.dump(all_valid_df, f)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fatal-order",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880, 15811, 0.395275)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_df['text']), len(train_df['text']), len(train_df['text'])/40000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-newspaper",
   "metadata": {},
   "source": [
    "## Construct count vectorizer with HTS, NACIS keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "apparent-coverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=30000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "count_vector = CountVectorizer(max_features=30000)\n",
    "count_vector.fit(list(hts['text']) + list(nacis['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "optical-profit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34707, 14588)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = count_vector.transform(list(hts['text']) + list(nacis['text']))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "actual-startup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15811, 14588)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts = count_vector.transform(list(train_df['text']))\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cheap-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder() \n",
    "label_enc.fit(result['label']) \n",
    "y_train = np.expand_dims(np.array(label_enc.transform(train_df['label'])), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "published-shark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15811, 14588), (15811, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pressed-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(count_vector, open(\"count_vector.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-paris",
   "metadata": {},
   "source": [
    "## Batch logic for GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "first-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(csr, y, rows, random_row_array, n=1):\n",
    "    l = len(rows)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield (csr[random_row_array[ndx:min(ndx + n, l)]].todense(), \n",
    "               y[random_row_array[ndx:min(ndx + n, l)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-compensation",
   "metadata": {},
   "source": [
    "## Enable batching class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "direct-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_model():\n",
    "    def __init__(self, param, lr, num_round = 5, batch_size=1000):\n",
    "        self.param     = param\n",
    "        self.num_round = num_round\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        \n",
    "    def fit(self, csr, y_val):\n",
    "        iteration = 0\n",
    "        print(\"Will run for {} rounds\".format(self.num_round))\n",
    "        for n_round in range(0, self.num_round):     \n",
    "            random_row_array = np.random.choice(np.arange(csr.shape[0]), csr.shape[0], replace=False)\n",
    "            rows = range(0, csr.shape[0])\n",
    "            with tqdm(total=int(len(rows)/self.batch_size)) as progress_bar:\n",
    "                for x,y in batch(csr, y_val, rows, random_row_array, self.batch_size):\n",
    "                    dtrain = xgb.DMatrix(x, y)\n",
    "                    watchlist = [(dtrain,'train')]\n",
    "\n",
    "                    if iteration == 0 : model = xgb.Booster(self.param, [dtrain])\n",
    "                    \n",
    "                    self.param['eta'] = self.lr[iteration]\n",
    "                    print('Round = {}, Iteration = {}, lr = {}'.format(n_round, iteration, self.lr[iteration]))\n",
    "                    \n",
    "                    model = xgb.train(self.param, dtrain, num_boost_round=1, xgb_model=model, evals=watchlist)\n",
    "                    iteration = iteration + 1\n",
    "                    progress_bar.update(1)\n",
    "            if n_round > 4 :\n",
    "                name = 'xgbchap_model_v2_{}'.format(n_round)\n",
    "                print(\"saving model: \", name)\n",
    "                model.save_model(name)\n",
    "                \n",
    "        self.model  = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-mainland",
   "metadata": {},
   "source": [
    "## Carefully adjust Learning rate for each iteration so that training converges (else it won't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "animated-paintball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "Will run for 8 rounds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f041d28b6947d9bbaea6ab30129eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round = 0, Iteration = 0, lr = 0.3\n",
      "[0]\ttrain-merror:0.4975\n",
      "Round = 0, Iteration = 1, lr = 0.3\n",
      "[0]\ttrain-merror:0.517\n",
      "Round = 0, Iteration = 2, lr = 0.3\n",
      "[0]\ttrain-merror:0.4925\n",
      "Round = 0, Iteration = 3, lr = 0.3\n",
      "[0]\ttrain-merror:0.5115\n",
      "Round = 0, Iteration = 4, lr = 0.3\n",
      "[0]\ttrain-merror:0.475\n",
      "Round = 0, Iteration = 5, lr = 0.3\n",
      "[0]\ttrain-merror:0.449\n",
      "Round = 0, Iteration = 6, lr = 0.3\n",
      "[0]\ttrain-merror:0.4405\n",
      "Round = 0, Iteration = 7, lr = 0.3\n",
      "[0]\ttrain-merror:0.436775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95927c2e31f41f1adba69ab9460c7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round = 1, Iteration = 8, lr = 0.3\n",
      "[0]\ttrain-merror:0.4025\n",
      "Round = 1, Iteration = 9, lr = 0.3\n",
      "[0]\ttrain-merror:0.4025\n",
      "Round = 1, Iteration = 10, lr = 0.3\n",
      "[0]\ttrain-merror:0.3725\n",
      "Round = 1, Iteration = 11, lr = 0.3\n",
      "[0]\ttrain-merror:0.3845\n",
      "Round = 1, Iteration = 12, lr = 0.3\n",
      "[0]\ttrain-merror:0.4015\n",
      "Round = 1, Iteration = 13, lr = 0.3\n",
      "[0]\ttrain-merror:0.388\n",
      "Round = 1, Iteration = 14, lr = 0.3\n",
      "[0]\ttrain-merror:0.375\n",
      "Round = 1, Iteration = 15, lr = 0.3\n",
      "[0]\ttrain-merror:0.38984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a3046a5da1450fbfc46e8e248c2ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round = 2, Iteration = 16, lr = 0.3\n",
      "[0]\ttrain-merror:0.3495\n",
      "Round = 2, Iteration = 17, lr = 0.3\n",
      "[0]\ttrain-merror:0.323\n",
      "Round = 2, Iteration = 18, lr = 0.3\n",
      "[0]\ttrain-merror:0.36\n",
      "Round = 2, Iteration = 19, lr = 0.3\n",
      "[0]\ttrain-merror:0.356\n",
      "Round = 2, Iteration = 20, lr = 0.3\n",
      "[0]\ttrain-merror:0.3545\n",
      "Round = 2, Iteration = 21, lr = 0.3\n",
      "[0]\ttrain-merror:0.3595\n",
      "Round = 2, Iteration = 22, lr = 0.3\n",
      "[0]\ttrain-merror:0.356\n",
      "Round = 2, Iteration = 23, lr = 0.3\n",
      "[0]\ttrain-merror:0.345665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b2923db9af428e8410958025e4b18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round = 3, Iteration = 24, lr = 0.3\n",
      "[0]\ttrain-merror:0.325\n",
      "Round = 3, Iteration = 25, lr = 0.3\n",
      "[0]\ttrain-merror:0.3125\n",
      "Round = 3, Iteration = 26, lr = 0.3\n",
      "[0]\ttrain-merror:0.324\n",
      "Round = 3, Iteration = 27, lr = 0.3\n",
      "[0]\ttrain-merror:0.324\n",
      "Round = 3, Iteration = 28, lr = 0.3\n",
      "[0]\ttrain-merror:0.3245\n",
      "Round = 3, Iteration = 29, lr = 0.3\n",
      "[0]\ttrain-merror:0.312\n",
      "Round = 3, Iteration = 30, lr = 0.3\n",
      "[0]\ttrain-merror:0.319\n",
      "Round = 3, Iteration = 31, lr = 0.3\n",
      "[0]\ttrain-merror:0.326891\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30a723b9fed414f839ddae27330e786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round = 4, Iteration = 32, lr = 0.3\n",
      "[0]\ttrain-merror:0.2935\n",
      "Round = 4, Iteration = 33, lr = 0.3\n",
      "[0]\ttrain-merror:0.3035\n",
      "Round = 4, Iteration = 34, lr = 0.3\n",
      "[0]\ttrain-merror:0.2855\n",
      "Round = 4, Iteration = 35, lr = 0.3\n",
      "[0]\ttrain-merror:0.293\n",
      "Round = 4, Iteration = 36, lr = 0.3\n",
      "[0]\ttrain-merror:0.311\n",
      "Round = 4, Iteration = 37, lr = 0.3\n",
      "[0]\ttrain-merror:0.293\n",
      "Round = 4, Iteration = 38, lr = 0.3\n",
      "[0]\ttrain-merror:0.2975\n",
      "Round = 4, Iteration = 39, lr = 0.3\n",
      "[0]\ttrain-merror:0.314743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c5d8c8421e4ce5a86e6c67b0440f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round = 5, Iteration = 40, lr = 0.3\n",
      "[0]\ttrain-merror:0.286\n",
      "Round = 5, Iteration = 41, lr = 0.3\n",
      "[0]\ttrain-merror:0.2845\n",
      "Round = 5, Iteration = 42, lr = 0.3\n",
      "[0]\ttrain-merror:0.2755\n",
      "Round = 5, Iteration = 43, lr = 0.3\n",
      "[0]\ttrain-merror:0.2845\n",
      "Round = 5, Iteration = 44, lr = 0.3\n",
      "[0]\ttrain-merror:0.282\n",
      "Round = 5, Iteration = 45, lr = 0.3\n",
      "[0]\ttrain-merror:0.272\n",
      "Round = 5, Iteration = 46, lr = 0.3\n",
      "[0]\ttrain-merror:0.291\n",
      "Round = 5, Iteration = 47, lr = 0.3\n",
      "[0]\ttrain-merror:0.280508\n",
      "saving model:  xgbchap_model_v2_5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d22f2b720b44dbb6b76cb64006f546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round = 6, Iteration = 48, lr = 0.3\n",
      "[0]\ttrain-merror:0.259\n",
      "Round = 6, Iteration = 49, lr = 0.3\n",
      "[0]\ttrain-merror:0.2635\n",
      "Round = 6, Iteration = 50, lr = 0.3\n",
      "[0]\ttrain-merror:0.269\n",
      "Round = 6, Iteration = 51, lr = 0.3\n",
      "[0]\ttrain-merror:0.2745\n",
      "Round = 6, Iteration = 52, lr = 0.3\n",
      "[0]\ttrain-merror:0.26\n",
      "Round = 6, Iteration = 53, lr = 0.3\n",
      "[0]\ttrain-merror:0.2755\n",
      "Round = 6, Iteration = 54, lr = 0.3\n",
      "[0]\ttrain-merror:0.269\n",
      "Round = 6, Iteration = 55, lr = 0.3\n",
      "[0]\ttrain-merror:0.269464\n",
      "saving model:  xgbchap_model_v2_6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0f440f692a4e6b8c3daeda91038f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round = 7, Iteration = 56, lr = 0.3\n",
      "[0]\ttrain-merror:0.251\n",
      "Round = 7, Iteration = 57, lr = 0.3\n",
      "[0]\ttrain-merror:0.255\n",
      "Round = 7, Iteration = 58, lr = 0.3\n",
      "[0]\ttrain-merror:0.248\n",
      "Round = 7, Iteration = 59, lr = 0.3\n",
      "[0]\ttrain-merror:0.2545\n",
      "Round = 7, Iteration = 60, lr = 0.3\n",
      "[0]\ttrain-merror:0.25\n",
      "Round = 7, Iteration = 61, lr = 0.3\n",
      "[0]\ttrain-merror:0.26\n",
      "Round = 7, Iteration = 62, lr = 0.3\n",
      "[0]\ttrain-merror:0.252\n",
      "Round = 7, Iteration = 63, lr = 0.3\n",
      "[0]\ttrain-merror:0.244616\n",
      "saving model:  xgbchap_model_v2_7\n"
     ]
    }
   ],
   "source": [
    "lr =  [0.3]*100\n",
    "# lr =  [0.45]*14*2 + [0.4]*14*2 + [0.3] * 14 * 2 + [0.2]* 14 * 1 +  [0.1]* 14 * 1 +  [0.05]* 14 * 100\n",
    "\n",
    "parameters = {'max_depth':5, 'objective':'multi:softprob', 'subsample':0.8, \n",
    "            'colsample_bytree':0.8, 'eta': 0.3, 'min_child_weight':0.1,\n",
    "            'tree_method':'auto', 'num_class' : len(all_labels)\n",
    "            }\n",
    "\n",
    "model = make_model(parameters, lr, num_round=8, batch_size=2000) \n",
    "model.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-liberal",
   "metadata": {},
   "source": [
    "## Load the saved, trained model for further experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "known-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model = xgb.Booster(model_file='xgbchap_model_v2_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-waterproof",
   "metadata": {},
   "source": [
    "## Do prediction in batches, else it will crash with out of memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "noble-sugar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88beb2c8fbae4f55b00225abfebd4060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batch(lst, n=1):\n",
    "    l = len(lst)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield lst[ndx:min(ndx + n, l)]\n",
    "\n",
    "def do_predict_batch(input):\n",
    "    first  = []\n",
    "    second = []\n",
    "    third  = []\n",
    "    fourth = []\n",
    "    fifth  = []\n",
    "    \n",
    "    with tqdm(total=len(valid_df['text'])) as progress_bar:\n",
    "        for x in batch(input, 2000):\n",
    "            tmp_valid_counts = count_vector.transform(x)\n",
    "            tmp_predict_da = model.model.predict(xgb.DMatrix(tmp_valid_counts.todense()))\n",
    "            sorted_idx = np.argsort(-tmp_predict_da)\n",
    "            first = first + list(label_enc.inverse_transform(list(sorted_idx[:,0])))\n",
    "            second = second + list(label_enc.inverse_transform(list(sorted_idx[:,1])))\n",
    "            third = third + list(label_enc.inverse_transform(list(sorted_idx[:,2])))\n",
    "            fourth = fourth + list(label_enc.inverse_transform(list(sorted_idx[:,3])))\n",
    "            fifth = fifth + list(label_enc.inverse_transform(list(sorted_idx[:,4])))\n",
    "            progress_bar.update(2000)\n",
    "        return first, second, third, fourth, fifth\n",
    "\n",
    "y1, y2, y3, y4, y5 = do_predict_batch(list(valid_df['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-joshua",
   "metadata": {},
   "source": [
    "## Calculate accuracy for top 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "satisfied-logan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49270833333333336 0.1076388888888889 0.059027777777777776 0.03923611111111111 0.025347222222222222\n",
      "Total: 0.7239583333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, plot_confusion_matrix, accuracy_score\n",
    "\n",
    "a1 = accuracy_score(list(valid_df['label']), y1)\n",
    "a2 = accuracy_score(list(valid_df['label']), y2)\n",
    "a3 = accuracy_score(list(valid_df['label']), y3)\n",
    "a4 = accuracy_score(list(valid_df['label']), y4)\n",
    "a5 = accuracy_score(list(valid_df['label']), y5)\n",
    "\n",
    "print('Accuracy:', a1, a2, a3, a4, a5)\n",
    "print('Total:', a1+a2+a3+a4+a5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(30)\n",
    "print('Terminate Instance')\n",
    "!aws ec2 terminate-instances --instance-ids i-0f41741a0c8b12972"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
