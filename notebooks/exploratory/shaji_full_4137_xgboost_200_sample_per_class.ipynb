{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automotive-amber",
   "metadata": {},
   "source": [
    "## References & starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-accuracy",
   "metadata": {},
   "source": [
    "https://github.com/dmlc/xgboost/blob/master/demo/guide-python/basic_walkthrough.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "swiss-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy import stats\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rational-petersburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "judicial-thermal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/work/shajikk/0308/\n"
     ]
    }
   ],
   "source": [
    "!ls -d /data/work/shajikk/0308/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "useful-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "republican-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "preliminary-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dd = dd.read_parquet('/data/work/shajikk/0308/data/full_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liable-reservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  3.7s\n",
      "Number of classes :  4137\n"
     ]
    }
   ],
   "source": [
    "all_labels = result_dd.label.unique()\n",
    "print('Number of classes : ', len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "quiet-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  3.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1982138"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "veterinary-turtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  3.8s\n"
     ]
    }
   ],
   "source": [
    "result = result_dd.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-lyric",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lesser-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tok = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "def clean_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "    remove_dig_pun = tok.tokenize(sentence.lower())\n",
    "\n",
    "    nltk_tagged = nltk.pos_tag(remove_dig_pun)  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "            \n",
    "    lemmatized_sentence_clean = list(map((lambda x : x if x not in stop else \"\"), lemmatized_sentence))\n",
    "    input_clean = list(map((lambda x : x if x not in stop else \"\"), remove_dig_pun))\n",
    "\n",
    "    return \" \".join(input_clean + lemmatized_sentence_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-measurement",
   "metadata": {},
   "source": [
    "## Read the HTS descriptions, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "selective-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "hts = pd.read_csv(\"/data/work/shajikk/0308/hts_train.csv\", dtype={'hs': str, 'desc' : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pressing-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "hts['clean'] = hts.desc.apply(lambda x : clean_sentence(x))\n",
    "hts = hts.rename({'hs' : 'label', 'clean' : 'text'}, axis=1)[['label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "figured-marathon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010121</td>\n",
       "      <td>live horses asses mules  hinnies horses purebr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text\n",
       "0  010121  live horses asses mules  hinnies horses purebr..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-robertson",
   "metadata": {},
   "source": [
    "## Read the NACIS->HTS examples, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "clean-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "nacis = pd.read_csv(\"/data/work/shajikk/0308/commodity_hts_extract.csv\", dtype={'hts6': str, 'description_long' : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "great-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "nacis['clean'] = nacis.description_long.apply(lambda x : clean_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "jewish-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "nacis = nacis.rename({'hts6' : 'label', 'clean' : 'text'}, axis=1)[['label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "complicated-flavor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910211</td>\n",
       "      <td>batteries  wrist watches battery powered  mech...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text\n",
       "0  910211  batteries  wrist watches battery powered  mech..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nacis.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-header",
   "metadata": {},
   "source": [
    "## Sample the full data, 4137 classes. Create a subset of 100 examples each per class. Save it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afraid-impact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  3.8s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0207870963094da1a5ccc94588a6f0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  3.7s\n"
     ]
    }
   ],
   "source": [
    "all_train_df = []\n",
    "all_valid_df = []\n",
    "count = 200\n",
    "recompute = False\n",
    "\n",
    "if not recompute :\n",
    "    with open('all_train_df_200.pkl', 'rb') as f: all_train_df = pickle.load(f)\n",
    "    with open('all_valid_df_200.pkl', 'rb') as f: all_valid_df = pickle.load(f)\n",
    "    pass\n",
    "\n",
    "\n",
    "for c in tqdm(all_labels) :\n",
    "    if (not recompute) : break\n",
    "    df = result[result.label == c]\n",
    "    df_sampled = df.sample(frac=min(count/len(df), 1))\n",
    "    df_hts = hts[hts.label == c]\n",
    "    df_nacis  = nacis[nacis.label == c]\n",
    "    \n",
    "    \n",
    "    train_df  = df_sampled.sample(frac=0.8)\n",
    "    valid_df = df_sampled.drop(train_df.index)\n",
    "\n",
    "    all_train_df.append(train_df)\n",
    "    all_train_df.append(df_hts)\n",
    "    all_train_df.append(df_nacis)\n",
    "    \n",
    "    all_valid_df.append(valid_df)\n",
    "\n",
    "train_df  = pd.concat(all_train_df)\n",
    "valid_df  = pd.concat(all_valid_df)\n",
    "\n",
    "if recompute :\n",
    "    with open('all_train_df_200.pkl', 'wb') as f: pickle.dump(all_train_df, f)\n",
    "    with open('all_valid_df_200.pkl', 'wb') as f: pickle.dump(all_valid_df, f)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "english-semester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125227, 525877, 13.146925)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_df['text']), len(train_df['text']), len(train_df['text'])/40000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-notebook",
   "metadata": {},
   "source": [
    "## Construct count vectorizer with HTS, NACIS keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "careful-trunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=30000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "count_vector = CountVectorizer(max_features=30000)\n",
    "count_vector.fit(list(hts['text']) + list(nacis['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "civilian-destination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34707, 14588)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = count_vector.transform(list(hts['text']) + list(nacis['text']))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ranging-montgomery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525877, 14588)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts = count_vector.transform(list(train_df['text']))\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "physical-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder() \n",
    "label_enc.fit(result['label']) \n",
    "y_train = np.expand_dims(np.array(label_enc.transform(train_df['label'])), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "detailed-appraisal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((525877, 14588), (525877, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "killing-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(count_vector, open(\"count_vector.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-oxygen",
   "metadata": {},
   "source": [
    "## Batch logic for GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "second-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(csr, y, rows, random_row_array, n=1):\n",
    "    l = len(rows)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield (csr[random_row_array[ndx:min(ndx + n, l)]].todense(), \n",
    "               y[random_row_array[ndx:min(ndx + n, l)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-uniform",
   "metadata": {},
   "source": [
    "## Enable batching class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "systematic-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_model():\n",
    "    def __init__(self, param, lr, num_round = 5, batch_size=1000):\n",
    "        self.param     = param\n",
    "        self.num_round = num_round\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        \n",
    "    def fit(self, csr, y_val):\n",
    "        iteration = 0\n",
    "        print(\"Will run for {} rounds\".format(self.num_round))\n",
    "        for n_round in range(0, self.num_round):     \n",
    "            random_row_array = np.random.choice(np.arange(csr.shape[0]), csr.shape[0], replace=False)\n",
    "            rows = range(0, csr.shape[0])\n",
    "            with tqdm(total=int(len(rows)/self.batch_size)) as progress_bar:\n",
    "                for x,y in batch(csr, y_val, rows, random_row_array, self.batch_size):\n",
    "                    dtrain = xgb.DMatrix(x, y)\n",
    "                    watchlist = [(dtrain,'train')]\n",
    "\n",
    "                    if iteration == 0 : model = xgb.Booster(self.param, [dtrain])\n",
    "                    \n",
    "                    self.param['eta'] = self.lr[iteration]\n",
    "                    print('Round = {}, Iteration = {}, lr = {}'.format(n_round, iteration, self.lr[iteration]))\n",
    "                    \n",
    "                    model = xgb.train(self.param, dtrain, num_boost_round=1, xgb_model=model, evals=watchlist)\n",
    "                    iteration = iteration + 1\n",
    "                    progress_bar.update(1)\n",
    "            if n_round > 4 :\n",
    "                name = 'xgb_model_v2_{}'.format(n_round)\n",
    "                print(\"saving model: \", name)\n",
    "                model.save_model(name)\n",
    "                \n",
    "        self.model  = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-marathon",
   "metadata": {},
   "source": [
    "## Carefully adjust Learning rate for each iteration so that training converges (else it won't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "copyrighted-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  3.8s\n"
     ]
    }
   ],
   "source": [
    "#lr =  [0.3]*100 + [0.1] * 20 + [0.05]*100 \n",
    "lr =  [0.45]*14*2 + [0.4]*14*2 + [0.3] * 14 * 2 + [0.2]* 14 * 1 +  [0.1]* 14 * 1 +  [0.05]* 14 * 100\n",
    "\n",
    "parameters = {'max_depth':5, 'objective':'multi:softprob', 'subsample':0.8, \n",
    "            'colsample_bytree':0.8, 'eta': 0.3, 'min_child_weight':0.1,\n",
    "            'tree_method':'gpu_hist', 'gpu_id': 0, 'num_class' : len(all_labels)\n",
    "            }\n",
    "\n",
    "model = make_model(parameters, lr, num_round=8, batch_size=40000) \n",
    "model.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-cherry",
   "metadata": {},
   "source": [
    "## Load the saved, trained model for further experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "greatest-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model = xgb.Booster(model_file='xgb_model_v2_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-worcester",
   "metadata": {},
   "source": [
    "## Do prediction in batches, else it will crash with out of memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "tracked-retention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce559423d132421c92f79fa38e651ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batch(lst, n=1):\n",
    "    l = len(lst)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield lst[ndx:min(ndx + n, l)]\n",
    "\n",
    "def do_predict_batch(input):\n",
    "    first  = []\n",
    "    second = []\n",
    "    third  = []\n",
    "    fourth = []\n",
    "    fifth  = []\n",
    "    \n",
    "    with tqdm(total=len(valid_df['text'])) as progress_bar:\n",
    "        for x in batch(input, 2000):\n",
    "            tmp_valid_counts = count_vector.transform(x)\n",
    "            tmp_predict_da = model.model.predict(xgb.DMatrix(tmp_valid_counts.todense()))\n",
    "            sorted_idx = np.argsort(-tmp_predict_da)\n",
    "            first = first + list(label_enc.inverse_transform(list(sorted_idx[:,0])))\n",
    "            second = second + list(label_enc.inverse_transform(list(sorted_idx[:,1])))\n",
    "            third = third + list(label_enc.inverse_transform(list(sorted_idx[:,2])))\n",
    "            fourth = fourth + list(label_enc.inverse_transform(list(sorted_idx[:,3])))\n",
    "            fifth = fifth + list(label_enc.inverse_transform(list(sorted_idx[:,4])))\n",
    "            progress_bar.update(2000)\n",
    "        return first, second, third, fourth, fifth\n",
    "\n",
    "y1, y2, y3, y4, y5 = do_predict_batch(list(valid_df['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-consideration",
   "metadata": {},
   "source": [
    "## Calculate accuracy for top 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "faced-howard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5625144737157323 0.09157769490605061 0.037811334616336734 0.022191699873030577 0.01474122992645356\n",
      "Total: 0.7288364330376037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, plot_confusion_matrix, accuracy_score\n",
    "\n",
    "a1 = accuracy_score(list(valid_df['label']), y1)\n",
    "a2 = accuracy_score(list(valid_df['label']), y2)\n",
    "a3 = accuracy_score(list(valid_df['label']), y3)\n",
    "a4 = accuracy_score(list(valid_df['label']), y4)\n",
    "a5 = accuracy_score(list(valid_df['label']), y5)\n",
    "\n",
    "print('Accuracy:', a1, a2, a3, a4, a5)\n",
    "print('Total:', a1+a2+a3+a4+a5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(30)\n",
    "print('Terminate Instance')\n",
    "!aws ec2 terminate-instances --instance-ids i-0f41741a0c8b12972"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
