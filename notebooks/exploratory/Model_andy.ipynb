{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "julian-wells",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (0.16)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.3.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.56.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: bert in /usr/local/lib/python3.8/dist-packages (2.2.0)\n",
      "Requirement already satisfied: erlastic in /usr/local/lib/python3.8/dist-packages (from bert) (2.0.0)\n",
      "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.8/dist-packages (1.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from bert-tensorflow) (1.15.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.8/dist-packages (from keras) (1.6.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from h5py->keras) (1.15.0)\n",
      "Collecting dask_ml\n",
      "  Downloading dask_ml-1.8.0-py3-none-any.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: distributed>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (2.30.1)\n",
      "Collecting dask-glm>=0.2.0\n",
      "  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.6.0)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.19.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from dask_ml) (20.9)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.2.2)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (2.17.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.24.1)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.52.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from dask-glm>=0.2.0->dask_ml) (1.6.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (5.4.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (0.11.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (0.8.5)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (1.1.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (1.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from distributed>=2.4.0->dask_ml) (45.2.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (1.0.2)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (6.1)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (5.8.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (7.1.2)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (2.3.0)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (2.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from multipledispatch>=0.4.9->dask_ml) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dask_ml) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dask_ml) (2021.1)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask[array,dataframe]>=2.4.0->dask_ml) (0.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->dask_ml) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->dask_ml) (1.0.1)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2.4.0->dask_ml) (1.0.1)\n",
      "Requirement already satisfied: llvmlite<0.36,>=0.35.0 in /usr/local/lib/python3.8/dist-packages (from numba->dask_ml) (0.35.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->dask_ml) (2.4.7)\n",
      "Installing collected packages: dask-glm, dask-ml\n",
      "Successfully installed dask-glm-0.2.0 dask-ml-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n",
    "!pip install transformers\n",
    "!pip install bert\n",
    "!pip install bert-tensorflow\n",
    "!pip install keras\n",
    "!pip install dask_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rental-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevent packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Import dask packages\n",
    "import dask.dataframe as ddf\n",
    "from math import nan\n",
    "import panel as pn\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dask_ml.model_selection import train_test_split\n",
    "import graphviz\n",
    "from datascroller import scroll\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "import transformers as ppb\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "import bert\n",
    "# from bert import run_classifier\n",
    "# from bert import optimization\n",
    "from bert import tokenization\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow import keras\n",
    "#### if use tensorflow=2.0.0, then import tensorflow.keras.model_selection \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-grant",
   "metadata": {},
   "source": [
    "## Install BERT and BERT Tokenizer from the HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-block",
   "metadata": {},
   "source": [
    "We can easily switch between variants of BERT by changing out which model we import from HuggingFace; the rest of the code just flows unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "higher-coupon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# For DistilBERT:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.TFDistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.TFBertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "## For BERT Large, use this:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.AutoModelWithLMHead, ppb.AutoTokenizer, 'bert-large-uncased')\n",
    "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# For ROBERTa base model, use this:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.TFRobertaModel, ppb.RobertaTokenizer, 'roberta-base')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-space",
   "metadata": {},
   "source": [
    "## Let's do an initial import on the sample dataset Padma created for HS4 codes 8712 and 8714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continuing-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2019\n",
      " HS\n",
      " enigma\n",
      " import_data_2017_enhanced.parq\n",
      " import_data_2018_enhanced.parq\n",
      " import_data_2019_enhanced.parq\n",
      " import_data_2020_enhanced.parq\n",
      " linux-bsd.gif\n",
      "'manifestDB - CMDT Codes directory.xlsx'\n",
      "'manifestDB - Data Fields Description.xlsx'\n",
      "'manifestDB - Main Coverage Ports Directory.xlsx'\n",
      "'manifestDB - Packing Unit Description.xlsx'\n",
      " raw_import_20152016.parq\n",
      " raw_import_2017.parq\n",
      " raw_import_2018.parq\n",
      " raw_import_2019.parq\n",
      " raw_import_2020.parq\n",
      " us_customs_aug2019.parq\n",
      " w210_raw_data.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls /data/common/trade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "clear-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_df = dd.read_parquet('/data/common/trade_data/2019/data_samples/sample_87128714.parq', engine='fastparquet', chunksize=\"100MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "indie-persian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['System Identity Id', 'Estimate Arrival Date', 'Actual Arrival Date',\n",
       "       'Bill of Lading', 'Master Bill of Lading', 'Bill Type Code',\n",
       "       'Carrier SASC Code', 'Vessel Country Code', 'Vessel Code',\n",
       "       'Vessel Name', 'Voyage', 'Inbond Type', 'Manifest No',\n",
       "       'Mode of Transportation', 'Loading Port', 'Last Vist Foreign Port',\n",
       "       'US Clearing District', 'Unloading Port', 'Place of Receipt', 'Country',\n",
       "       'Country Sure Level', 'Weight in KG', 'Weight', 'Weight Unit', 'TEU',\n",
       "       'Quantity', 'Quantity Unit', 'Measure in CM', 'Measure', 'Measure Unit',\n",
       "       'Container Id', 'Container Size', 'Container Type',\n",
       "       'Container Desc Code', 'Container Load Status',\n",
       "       'Container Type of Service', 'Shipper Name', 'Shipper Address ',\n",
       "       'Raw Shipper Name', 'Raw Shipper Addr1', 'Raw Shipper Addr2',\n",
       "       'Raw Shipper Addr3', 'Raw Shipper Addr4', 'Raw Shipper Addr Others',\n",
       "       'Consignee Name', 'Consignee Address ', 'Raw Consignee Name',\n",
       "       'Raw Consignee Addr1', 'Raw Consignee Addr2', 'Raw Consignee Addr3',\n",
       "       'Raw Consignee Addr4', 'Raw Consignee Addr Others', 'Notify Party Name',\n",
       "       'Notify Party Address ', 'Raw Notify Party Name',\n",
       "       'Raw Notify Party Addr1', 'Raw Notify Party Addr2',\n",
       "       'Raw Notify Party Addr3', 'Raw Notify Party Addr4',\n",
       "       'Raw Notify Party Addr Others', 'Product Desc', 'Marks & Numbers',\n",
       "       'HS Code Sure Level', 'CIF', 'Indicator of true supplier',\n",
       "       'Indicator of true buyer', 'END', 'HS Code', 'HS_Code',\n",
       "       'Merged_Description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "positive-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2.2s\n",
      "[########################################] | 100% Completed |  2.2s\n",
      "[########################################] | 100% Completed |  2.3s\n",
      "[########################################] | 100% Completed |  2.4s\n",
      "[########################################] | 100% Completed |  2.4s\n",
      "[########################################] | 100% Completed |  2.5s\n",
      "[########################################] | 100% Completed |  2.5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System Identity Id</th>\n",
       "      <th>Estimate Arrival Date</th>\n",
       "      <th>Actual Arrival Date</th>\n",
       "      <th>Bill of Lading</th>\n",
       "      <th>Master Bill of Lading</th>\n",
       "      <th>Bill Type Code</th>\n",
       "      <th>Carrier SASC Code</th>\n",
       "      <th>Vessel Country Code</th>\n",
       "      <th>Vessel Code</th>\n",
       "      <th>Vessel Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Product Desc</th>\n",
       "      <th>Marks &amp; Numbers</th>\n",
       "      <th>HS Code Sure Level</th>\n",
       "      <th>CIF</th>\n",
       "      <th>Indicator of true supplier</th>\n",
       "      <th>Indicator of true buyer</th>\n",
       "      <th>END</th>\n",
       "      <th>HS Code</th>\n",
       "      <th>HS_Code</th>\n",
       "      <th>Merged_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6003201907090000254809</td>\n",
       "      <td>20190705</td>\n",
       "      <td>20190705</td>\n",
       "      <td>CHSLTPE19060165</td>\n",
       "      <td>EGLV003901609611</td>\n",
       "      <td>H</td>\n",
       "      <td>CHSL, CHISLEY MOTOR COACHES</td>\n",
       "      <td>PA</td>\n",
       "      <td>9306990</td>\n",
       "      <td>OOCL VANCOUVER</td>\n",
       "      <td>...</td>\n",
       "      <td>ELLIPTICAL TRAINER,TREADMILL, ESCALATE&lt;br/&gt;</td>\n",
       "      <td>NO MARKS&lt;br/&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>END</td>\n",
       "      <td>871200</td>\n",
       "      <td>871200</td>\n",
       "      <td>Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6003201901040000381477</td>\n",
       "      <td>20181230</td>\n",
       "      <td>20190103</td>\n",
       "      <td>AMAWSHNGBA807064</td>\n",
       "      <td>APLUNPFB000624</td>\n",
       "      <td>H</td>\n",
       "      <td>AMAW</td>\n",
       "      <td>CN</td>\n",
       "      <td>APL BARCELONA</td>\n",
       "      <td>APL BARCELONA</td>\n",
       "      <td>...</td>\n",
       "      <td>SCH 830 TREADMILL&lt;br/&gt;</td>\n",
       "      <td>AS ADDRESSED&lt;br/&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>END</td>\n",
       "      <td>871200</td>\n",
       "      <td>871200</td>\n",
       "      <td>Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       System Identity Id  Estimate Arrival Date  Actual Arrival Date  \\\n",
       "0  6003201907090000254809  20190705               20190705              \n",
       "1  6003201901040000381477  20181230               20190103              \n",
       "\n",
       "     Bill of Lading Master Bill of Lading Bill Type Code  \\\n",
       "0  CHSLTPE19060165   EGLV003901609611      H               \n",
       "1  AMAWSHNGBA807064  APLUNPFB000624        H               \n",
       "\n",
       "             Carrier SASC Code Vessel Country Code    Vessel Code  \\\n",
       "0  CHSL, CHISLEY MOTOR COACHES  PA                  9306990         \n",
       "1  AMAW                         CN                  APL BARCELONA   \n",
       "\n",
       "      Vessel Name  ...                                 Product Desc  \\\n",
       "0  OOCL VANCOUVER  ...  ELLIPTICAL TRAINER,TREADMILL, ESCALATE<br/>   \n",
       "1  APL BARCELONA   ...  SCH 830 TREADMILL<br/>                        \n",
       "\n",
       "     Marks & Numbers  HS Code Sure Level  CIF Indicator of true supplier  \\\n",
       "0  NO MARKS<br/>      5                   0.0  Y                           \n",
       "1  AS ADDRESSED<br/>  5                   0.0  Y                           \n",
       "\n",
       "  Indicator of true buyer  END HS Code HS_Code  \\\n",
       "0  Y                       END  871200  871200   \n",
       "1  Y                       END  871200  871200   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Merged_Description  \n",
       "0  Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles  \n",
       "1  Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles  \n",
       "\n",
       "[2 rows x 70 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't truncate text fields in the display\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "\n",
    "import_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "educational-liquid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7269"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(import_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-combination",
   "metadata": {},
   "source": [
    "Let's tokenize the description fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adopted-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Tokenize text\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return list(\n",
    "        filter(lambda word: word.isalnum(), tokens)\n",
    "    )\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# ## Add some common words from text\n",
    "# stop_words.extend([\"from\",\"subject\",\"summary\",\"keywords\",\"article\"])\n",
    "def remove_stopwords(words):\n",
    "    '''\n",
    "    Remove stop words from the list of words\n",
    "    '''\n",
    "    \n",
    "    filtered = filter(lambda word: word not in stop_words, words)\n",
    "    \n",
    "    return list(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-personality",
   "metadata": {},
   "source": [
    "### Create train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "featured-cannon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "Size of train set is  4327\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "Size of dev set is  1509\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "Size of test set is  1433\n"
     ]
    }
   ],
   "source": [
    "# 20% to test, 20% to dev and 60% to train\n",
    "\n",
    "# Set aside a test set\n",
    "train_df, test_df = train_test_split(import_df, test_size=0.2)\n",
    "\n",
    "# Split into train and dev sets\n",
    "train_df, dev_df = train_test_split(train_df, test_size=0.25)\n",
    "\n",
    "print('Size of train set is', len(train_df.index))\n",
    "print('Size of dev set is', len(dev_df.index))\n",
    "print('Size of test set is', len(test_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "sufficient-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "[########################################] | 100% Completed |  1.0s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n"
     ]
    }
   ],
   "source": [
    "# convert the dataframes back to pandas\n",
    "train_df_pd = train_df.compute()\n",
    "dev_df_pd = dev_df.compute()\n",
    "test_df_pd = test_df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-flower",
   "metadata": {},
   "source": [
    "## BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "loved-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data, max_length):\n",
    "    \"\"\"\n",
    "    Function takes inputs:\n",
    "    - data in the form of a dataframe with a column named 'tweet'\n",
    "    - max length\n",
    "    and produces as output the input data BERT requires as an array consisting of:\n",
    "    - Sentence IDs padded to the max length\n",
    "    - BERT Masks that tell BERT which of the Sentence IDs are 0 and should be ignored\n",
    "    - SequenceIDs which are all 0 for our classification task\n",
    "    \"\"\"\n",
    "    # Tokenize each tweet and add the special beginning/end tokens\n",
    "    tokenized = data['Product Desc'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True)))\n",
    "      \n",
    "    # Create the padding based on the max length so all are same shape\n",
    "    bertSentenceIDs = np.array([i + [0]*(max_length-len(i)) for i in tokenized.values])\n",
    "    \n",
    "    # Create the attention mask so BERT knows which contain values and which are 0s that should be ignored\n",
    "    bertMasks = np.where(bertSentenceIDs != 0, 1, 0)\n",
    "\n",
    "    # Create the BERT sequence IDs. In this case they are all 0 since it's the same sentence input.\n",
    "    bertSequenceIDs = np.array([np.zeros(max_length) for i in tokenized.values], dtype=int)\n",
    "    \n",
    "    # Create and return the data array containing both the padded and the attention mask\n",
    "    X_data = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])\n",
    "\n",
    "    # Also look at the vocabulary size in the tokenizer\n",
    "\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-paste",
   "metadata": {},
   "source": [
    "In the past we have run into memory issues depending on the length of the input, so we set up a variable to truncate the tokens being input for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "precise-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 150\n",
    "\n",
    "X_train = pre_process(train_df_pd, max_length)\n",
    "X_dev = pre_process(dev_df_pd, max_length)\n",
    "\n",
    "train_labels = np.array(train_df_pd['HS_Code']).astype(int)\n",
    "dev_labels = np.array(dev_df_pd['HS_Code']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "northern-capture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3, 4327, 150)\n",
      "X_train labels shape: (4327,)\n",
      "\n",
      "X_dev shape: (3, 1509, 150)\n",
      "X_dev labels shape: (1509,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure we end up with the shapes we expect.\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_train labels shape:', train_labels.shape)\n",
    "print()\n",
    "\n",
    "print('X_dev shape:', X_dev.shape)\n",
    "print('X_dev labels shape:', dev_labels.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "included-burlington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "challenging-secondary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871200"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-output",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "grateful-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_model(max_input_length, train_layers, optimizer):\n",
    "    \"\"\"\n",
    "    UPDATE doc string\n",
    "    Implementation of multi classification model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_input_length,), dtype='int32', name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_input_length,), dtype='int32', name=\"attention_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_input_length,), dtype='int32', name=\"segment_ids\")\n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    \n",
    "    # Note: Bert layer from Hugging Face returns two values: sequence ouput, and pooled output. Here, we only want\n",
    "    # the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) \n",
    "    \n",
    "    bert_layer = model_class.from_pretrained(pretrained_weights)\n",
    "    \n",
    "    \n",
    "    # Freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    \n",
    "    retrain_layers = []\n",
    "    \n",
    "    for retrain_layer_number in range(train_layers):\n",
    "        \n",
    "        layer_code = '_' + str(11 - retrain_layer_number)\n",
    "        retrain_layers.append(layer_code)\n",
    "    \n",
    "    for w in bert_layer.weights:\n",
    "        if not any([x in w.name for x in retrain_layers]):\n",
    "            w._trainable = False\n",
    "            \n",
    "    # End of freezing section\n",
    "    \n",
    "    # bert_layer(bert_inputs) generates two outputs. \n",
    "    # Select this to use the average of the non-CLS tokens to make the prediction\n",
    "    # bert_sequence = tf.math.reduce_mean(bert_layer(bert_inputs)[0], axis=1) # take the average of all the non-CLS tokens\n",
    "    \n",
    "    # Select this to use the CLS token to make the prediction\n",
    "    bert_sequence = bert_layer(bert_inputs)[1]\n",
    "    \n",
    "    print('Let us check the shape of the BERT layer output:', bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dropped = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid', name='target')(dropped) \n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "   \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy'])  \n",
    "\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "lesbian-sugar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us check the shape of the BERT layer output: KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name='tf_bert_model_4/bert/pooler/dense/Tanh:0', description=\"created by layer 'tf_bert_model_4'\")\n",
      "pred:  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='target/Sigmoid:0', description=\"created by layer 'target'\")\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_4 (TFBertModel)   TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      tf_bert_model_4[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_188 (Dropout)           (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "target (Dense)                  (None, 1)            257         dropout_188[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 109,679,361\n",
      "Trainable params: 109,679,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Received a label value of 871499 which is outside the valid range of [0, 1).  Label values: 871410 871200 871493 871493 871420 871496 871491 871420 871496 871410 871200 871420 871492 871491 871494 871200 871499 871493 871493 871493 871410 871200 871200 871494 871492 871420 871493 871494 871420 871410 871420 871493 871493 871492 871499 871200 871492 871493 871493 871492 871493 871495 871420 871492 871494 871492 871499 871410 871200 871410 871492 871493 871200 871496 871200 871420 871499 871410 871496 871410 871200 871200 871200 871493 871410 871493 871493 871420 871493 871420 871200 871499 871493 871420 871420 871200 871420 871492 871493 871492 871200 871200 871410 871499 871492 871494 871420 871499 871492 871494 871496 871420 871499 871493 871491 871494 871200 871200 871492 871420 871494 871494 871492 871420 871200 871410 871491 871494 871496 871492 871493 871420 871491 871496 871420 871492 871494 871200 871200 871496 871493 871200 871492 871499 871499 871420 871496 871420\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-65-57a2079ae614>:33) ]] [Op:__inference_train_function_38597]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-57a2079ae614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                                         verbose=0, mode='auto')\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Capture the history of the model fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     34\u001b[0m     x = {\"input_ids\": training_data[0],\n\u001b[1;32m     35\u001b[0m          \u001b[0;34m\"attention_masks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Received a label value of 871499 which is outside the valid range of [0, 1).  Label values: 871410 871200 871493 871493 871420 871496 871491 871420 871496 871410 871200 871420 871492 871491 871494 871200 871499 871493 871493 871493 871410 871200 871200 871494 871492 871420 871493 871494 871420 871410 871420 871493 871493 871492 871499 871200 871492 871493 871493 871492 871493 871495 871420 871492 871494 871492 871499 871410 871200 871410 871492 871493 871200 871496 871200 871420 871499 871410 871496 871410 871200 871200 871200 871493 871410 871493 871493 871420 871493 871420 871200 871499 871493 871420 871420 871200 871420 871492 871493 871492 871200 871200 871410 871499 871492 871494 871420 871499 871492 871494 871496 871420 871499 871493 871491 871494 871200 871200 871492 871420 871494 871494 871492 871420 871200 871410 871491 871494 871496 871492 871493 871420 871491 871496 871420 871492 871494 871200 871200 871496 871493 871200 871492 871499 871499 871420 871496 871420\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-65-57a2079ae614>:33) ]] [Op:__inference_train_function_38597]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# Set hyperparameters\n",
    "adam_customized = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "train_layers = 2\n",
    "\n",
    "# ###Convert the labels into 1 and 0 for binary_crossentropy\n",
    "\n",
    "# new_train_labels = np.where(train_labels_b==\"TIN\", 1, 0)\n",
    "\n",
    "# new_dev_labels = np.where(dev_labels_b==\"TIN\", 1, 0)\n",
    "\n",
    "\n",
    "training_labels = train_labels\n",
    "\n",
    "\n",
    "training_data = X_train\n",
    "\n",
    "dev_data = X_dev\n",
    "\n",
    "\n",
    "model = sent_model(max_length, train_layers=train_layers, optimizer = adam_customized)\n",
    "\n",
    "\n",
    "\n",
    "# Set parameters for early stopping\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                        min_delta=0.001,\n",
    "                                                        patience=5,\n",
    "                                                        verbose=0, mode='auto')\n",
    "# Capture the history of the model fitting\n",
    "history = model.fit(\n",
    "    x = {\"input_ids\": training_data[0],\n",
    "         \"attention_masks\": training_data[1],\n",
    "         \"segment_ids\": training_data[2]}, y = {\"target\": training_labels},\n",
    "    validation_data=({\"input_ids\": dev_data[0],\n",
    "         \"attention_masks\": dev_data[1],\n",
    "         \"segment_ids\": dev_data[2]}, {\"target\": dev_labels}),\n",
    "    verbose=2,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-citizen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
