{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "athletic-retirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.8/dist-packages (0.16)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.3.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.56.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: bert in /usr/local/lib/python3.8/dist-packages (2.2.0)\n",
      "Requirement already satisfied: erlastic in /usr/local/lib/python3.8/dist-packages (from bert) (2.0.0)\n",
      "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.8/dist-packages (1.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from bert-tensorflow) (1.15.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.8/dist-packages (from keras) (1.6.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from h5py->keras) (1.15.0)\n",
      "Collecting dask_ml\n",
      "  Downloading dask_ml-1.8.0-py3-none-any.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: distributed>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (2.30.1)\n",
      "Collecting dask-glm>=0.2.0\n",
      "  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.6.0)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.19.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from dask_ml) (20.9)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (1.2.2)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (2.17.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.24.1)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from dask_ml) (0.52.0)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from dask-glm>=0.2.0->dask_ml) (1.6.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (5.4.1)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (0.11.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (0.8.5)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (1.1.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (1.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from distributed>=2.4.0->dask_ml) (45.2.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (1.0.2)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (6.1)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (5.8.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (7.1.2)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (2.3.0)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2.4.0->dask_ml) (2.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from multipledispatch>=0.4.9->dask_ml) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dask_ml) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->dask_ml) (2021.1)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask[array,dataframe]>=2.4.0->dask_ml) (0.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->dask_ml) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.23->dask_ml) (1.0.1)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2.4.0->dask_ml) (1.0.1)\n",
      "Requirement already satisfied: llvmlite<0.36,>=0.35.0 in /usr/local/lib/python3.8/dist-packages (from numba->dask_ml) (0.35.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->dask_ml) (2.4.7)\n",
      "Installing collected packages: dask-glm, dask-ml\n",
      "Successfully installed dask-glm-0.2.0 dask-ml-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n",
    "!pip install transformers\n",
    "!pip install bert\n",
    "!pip install bert-tensorflow\n",
    "!pip install keras\n",
    "!pip install dask_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ruled-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevent packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Import dask packages\n",
    "import dask.dataframe as ddf\n",
    "from math import nan\n",
    "import panel as pn\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from dask_ml.model_selection import train_test_split\n",
    "import graphviz\n",
    "from datascroller import scroll\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "# text processing libraries\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "import transformers as ppb\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "import bert\n",
    "# from bert import run_classifier\n",
    "# from bert import optimization\n",
    "from bert import tokenization\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from tensorflow import keras\n",
    "#### if use tensorflow=2.0.0, then import tensorflow.keras.model_selection \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-wrist",
   "metadata": {},
   "source": [
    "## Install BERT and BERT Tokenizer from the HuggingFace Transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-playlist",
   "metadata": {},
   "source": [
    "We can easily switch between variants of BERT by changing out which model we import from HuggingFace; the rest of the code just flows unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fifteen-brisbane",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# For DistilBERT:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.TFDistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.TFBertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "## For BERT Large, use this:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.AutoModelWithLMHead, ppb.AutoTokenizer, 'bert-large-uncased')\n",
    "# from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# For ROBERTa base model, use this:\n",
    "# model_class, tokenizer_class, pretrained_weights = (ppb.TFRobertaModel, ppb.RobertaTokenizer, 'roberta-base')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-universal",
   "metadata": {},
   "source": [
    "## Let's do an initial import on the sample dataset Padma created for HS4 codes 8712 and 8714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ambient-morris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2019\n",
      " HS\n",
      " enigma\n",
      " import_data_2017_enhanced.parq\n",
      " import_data_2018_enhanced.parq\n",
      " import_data_2019_enhanced.parq\n",
      " import_data_2020_enhanced.parq\n",
      " linux-bsd.gif\n",
      "'manifestDB - CMDT Codes directory.xlsx'\n",
      "'manifestDB - Data Fields Description.xlsx'\n",
      "'manifestDB - Main Coverage Ports Directory.xlsx'\n",
      "'manifestDB - Packing Unit Description.xlsx'\n",
      " raw_import_20152016.parq\n",
      " raw_import_2017.parq\n",
      " raw_import_2018.parq\n",
      " raw_import_2019.parq\n",
      " raw_import_2020.parq\n",
      " us_customs_aug2019.parq\n",
      " w210_raw_data.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls /data/common/trade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bound-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_df = dd.read_parquet('/data/common/trade_data/2019/data_samples/sample_87128714.parq', engine='fastparquet', chunksize=\"100MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "promotional-depression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['System Identity Id', 'Estimate Arrival Date', 'Actual Arrival Date',\n",
       "       'Bill of Lading', 'Master Bill of Lading', 'Bill Type Code',\n",
       "       'Carrier SASC Code', 'Vessel Country Code', 'Vessel Code',\n",
       "       'Vessel Name', 'Voyage', 'Inbond Type', 'Manifest No',\n",
       "       'Mode of Transportation', 'Loading Port', 'Last Vist Foreign Port',\n",
       "       'US Clearing District', 'Unloading Port', 'Place of Receipt', 'Country',\n",
       "       'Country Sure Level', 'Weight in KG', 'Weight', 'Weight Unit', 'TEU',\n",
       "       'Quantity', 'Quantity Unit', 'Measure in CM', 'Measure', 'Measure Unit',\n",
       "       'Container Id', 'Container Size', 'Container Type',\n",
       "       'Container Desc Code', 'Container Load Status',\n",
       "       'Container Type of Service', 'Shipper Name', 'Shipper Address ',\n",
       "       'Raw Shipper Name', 'Raw Shipper Addr1', 'Raw Shipper Addr2',\n",
       "       'Raw Shipper Addr3', 'Raw Shipper Addr4', 'Raw Shipper Addr Others',\n",
       "       'Consignee Name', 'Consignee Address ', 'Raw Consignee Name',\n",
       "       'Raw Consignee Addr1', 'Raw Consignee Addr2', 'Raw Consignee Addr3',\n",
       "       'Raw Consignee Addr4', 'Raw Consignee Addr Others', 'Notify Party Name',\n",
       "       'Notify Party Address ', 'Raw Notify Party Name',\n",
       "       'Raw Notify Party Addr1', 'Raw Notify Party Addr2',\n",
       "       'Raw Notify Party Addr3', 'Raw Notify Party Addr4',\n",
       "       'Raw Notify Party Addr Others', 'Product Desc', 'Marks & Numbers',\n",
       "       'HS Code Sure Level', 'CIF', 'Indicator of true supplier',\n",
       "       'Indicator of true buyer', 'END', 'HS Code', 'HS_Code',\n",
       "       'Merged_Description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "transsexual-aside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System Identity Id</th>\n",
       "      <th>Estimate Arrival Date</th>\n",
       "      <th>Actual Arrival Date</th>\n",
       "      <th>Bill of Lading</th>\n",
       "      <th>Master Bill of Lading</th>\n",
       "      <th>Bill Type Code</th>\n",
       "      <th>Carrier SASC Code</th>\n",
       "      <th>Vessel Country Code</th>\n",
       "      <th>Vessel Code</th>\n",
       "      <th>Vessel Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Product Desc</th>\n",
       "      <th>Marks &amp; Numbers</th>\n",
       "      <th>HS Code Sure Level</th>\n",
       "      <th>CIF</th>\n",
       "      <th>Indicator of true supplier</th>\n",
       "      <th>Indicator of true buyer</th>\n",
       "      <th>END</th>\n",
       "      <th>HS Code</th>\n",
       "      <th>HS_Code</th>\n",
       "      <th>Merged_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6003201907090000254809</td>\n",
       "      <td>20190705</td>\n",
       "      <td>20190705</td>\n",
       "      <td>CHSLTPE19060165</td>\n",
       "      <td>EGLV003901609611</td>\n",
       "      <td>H</td>\n",
       "      <td>CHSL, CHISLEY MOTOR COACHES</td>\n",
       "      <td>PA</td>\n",
       "      <td>9306990</td>\n",
       "      <td>OOCL VANCOUVER</td>\n",
       "      <td>...</td>\n",
       "      <td>ELLIPTICAL TRAINER,TREADMILL, ESCALATE&lt;br/&gt;</td>\n",
       "      <td>NO MARKS&lt;br/&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>END</td>\n",
       "      <td>871200</td>\n",
       "      <td>871200</td>\n",
       "      <td>Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6003201901040000381477</td>\n",
       "      <td>20181230</td>\n",
       "      <td>20190103</td>\n",
       "      <td>AMAWSHNGBA807064</td>\n",
       "      <td>APLUNPFB000624</td>\n",
       "      <td>H</td>\n",
       "      <td>AMAW</td>\n",
       "      <td>CN</td>\n",
       "      <td>APL BARCELONA</td>\n",
       "      <td>APL BARCELONA</td>\n",
       "      <td>...</td>\n",
       "      <td>SCH 830 TREADMILL&lt;br/&gt;</td>\n",
       "      <td>AS ADDRESSED&lt;br/&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>END</td>\n",
       "      <td>871200</td>\n",
       "      <td>871200</td>\n",
       "      <td>Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       System Identity Id  Estimate Arrival Date  Actual Arrival Date  \\\n",
       "0  6003201907090000254809  20190705               20190705              \n",
       "1  6003201901040000381477  20181230               20190103              \n",
       "\n",
       "     Bill of Lading Master Bill of Lading Bill Type Code  \\\n",
       "0  CHSLTPE19060165   EGLV003901609611      H               \n",
       "1  AMAWSHNGBA807064  APLUNPFB000624        H               \n",
       "\n",
       "             Carrier SASC Code Vessel Country Code    Vessel Code  \\\n",
       "0  CHSL, CHISLEY MOTOR COACHES  PA                  9306990         \n",
       "1  AMAW                         CN                  APL BARCELONA   \n",
       "\n",
       "      Vessel Name  ...                                 Product Desc  \\\n",
       "0  OOCL VANCOUVER  ...  ELLIPTICAL TRAINER,TREADMILL, ESCALATE<br/>   \n",
       "1  APL BARCELONA   ...  SCH 830 TREADMILL<br/>                        \n",
       "\n",
       "     Marks & Numbers  HS Code Sure Level  CIF Indicator of true supplier  \\\n",
       "0  NO MARKS<br/>      5                   0.0  Y                           \n",
       "1  AS ADDRESSED<br/>  5                   0.0  Y                           \n",
       "\n",
       "  Indicator of true buyer  END HS Code HS_Code  \\\n",
       "0  Y                       END  871200  871200   \n",
       "1  Y                       END  871200  871200   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Merged_Description  \n",
       "0  Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles  \n",
       "1  Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles  \n",
       "\n",
       "[2 rows x 70 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't truncate text fields in the display\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "\n",
    "import_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "seeing-image",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "[########################################] | 100% Completed |  1.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7269"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(import_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-administrator",
   "metadata": {},
   "source": [
    "Let's tokenize the description fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "mysterious-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Tokenize text\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return list(\n",
    "        filter(lambda word: word.isalnum(), tokens)\n",
    "    )\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# ## Add some common words from text\n",
    "# stop_words.extend([\"from\",\"subject\",\"summary\",\"keywords\",\"article\"])\n",
    "def remove_stopwords(words):\n",
    "    '''\n",
    "    Remove stop words from the list of words\n",
    "    '''\n",
    "    \n",
    "    filtered = filter(lambda word: word not in stop_words, words)\n",
    "    \n",
    "    return list(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-mechanics",
   "metadata": {},
   "source": [
    "### Create train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "valued-legislature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "Size of train set is 4350\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "Size of dev set is 1477\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "Size of test set is 1442\n"
     ]
    }
   ],
   "source": [
    "# 20% to test, 20% to dev and 60% to train\n",
    "\n",
    "# Set aside a test set\n",
    "train_df, test_df = train_test_split(import_df, test_size=0.2)\n",
    "\n",
    "# Split into train and dev sets\n",
    "train_df, dev_df = train_test_split(train_df, test_size=0.25)\n",
    "\n",
    "print('Size of train set is', len(train_df.index))\n",
    "print('Size of dev set is', len(dev_df.index))\n",
    "print('Size of test set is', len(test_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "external-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "[########################################] | 100% Completed |  0.1s\n",
      "[########################################] | 100% Completed |  0.2s\n",
      "[########################################] | 100% Completed |  0.3s\n",
      "[########################################] | 100% Completed |  0.4s\n",
      "[########################################] | 100% Completed |  0.5s\n",
      "[########################################] | 100% Completed |  0.6s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.7s\n",
      "[########################################] | 100% Completed |  0.8s\n",
      "[########################################] | 100% Completed |  0.9s\n",
      "[########################################] | 100% Completed |  1.0s\n"
     ]
    }
   ],
   "source": [
    "# convert the dataframes back to pandas\n",
    "# I couldn't get the BERT model and code I had to run when it was a dask dataframe\n",
    "train_df_pd = train_df.compute()\n",
    "dev_df_pd = dev_df.compute()\n",
    "test_df_pd = test_df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-dietary",
   "metadata": {},
   "source": [
    "## BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "successful-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data, column_name, max_length):\n",
    "    \"\"\"\n",
    "    Function takes inputs:\n",
    "    - data in the form of a pandas dataframe\n",
    "    - column_name containing the text to be embedded\n",
    "    - max length\n",
    "    and produces as output the input data BERT requires as an array consisting of:\n",
    "    - Sentence IDs padded to the max length\n",
    "    - BERT Masks that tell BERT which of the Sentence IDs are 0 and should be ignored\n",
    "    - SequenceIDs which are all 0 for our classification task\n",
    "    \"\"\"\n",
    "    # Tokenize each tweet and add the special beginning/end tokens\n",
    "    tokenized = data[column_name].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True)))\n",
    "      \n",
    "    # Create the padding based on the max length so all are same shape\n",
    "    bertSentenceIDs = np.array([i + [0]*(max_length-len(i)) for i in tokenized.values])\n",
    "    \n",
    "    # Create the attention mask so BERT knows which contain values and which are 0s that should be ignored\n",
    "    bertMasks = np.where(bertSentenceIDs != 0, 1, 0)\n",
    "\n",
    "    # Create the BERT sequence IDs. In this case they are all 0 since it's the same sentence input.\n",
    "    bertSequenceIDs = np.array([np.zeros(max_length) for i in tokenized.values], dtype=int)\n",
    "    \n",
    "    # Create and return the data array containing both the padded and the attention mask\n",
    "    X_data = np.array([bertSentenceIDs, bertMasks, bertSequenceIDs])\n",
    "\n",
    "    # Also look at the vocabulary size in the tokenizer\n",
    "\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-nursery",
   "metadata": {},
   "source": [
    "In the past we have run into memory issues depending on the length of the input, so we set up a variable to truncate the tokens being input for each record.\n",
    "\n",
    "First, let's check the max length of the different tokenized columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "therapeutic-brunswick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of column Product Desc 37\n",
      "Max length of column Merged_Description 184\n"
     ]
    }
   ],
   "source": [
    "length_lst = ['Product Desc', 'Merged_Description']\n",
    "\n",
    "for l in length_lst:\n",
    "    tokenized = []\n",
    "    tokenized = train_df_pd[l].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "    # Find the max length for the tokenized examples\n",
    "    max_length = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_length:\n",
    "            max_length = len(i)\n",
    "\n",
    "    print('Max length of column', l, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bearing-height",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System Identity Id</th>\n",
       "      <th>Estimate Arrival Date</th>\n",
       "      <th>Actual Arrival Date</th>\n",
       "      <th>Bill of Lading</th>\n",
       "      <th>Master Bill of Lading</th>\n",
       "      <th>Bill Type Code</th>\n",
       "      <th>Carrier SASC Code</th>\n",
       "      <th>Vessel Country Code</th>\n",
       "      <th>Vessel Code</th>\n",
       "      <th>Vessel Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Product Desc</th>\n",
       "      <th>Marks &amp; Numbers</th>\n",
       "      <th>HS Code Sure Level</th>\n",
       "      <th>CIF</th>\n",
       "      <th>Indicator of true supplier</th>\n",
       "      <th>Indicator of true buyer</th>\n",
       "      <th>END</th>\n",
       "      <th>HS Code</th>\n",
       "      <th>HS_Code</th>\n",
       "      <th>Merged_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6003201901040000381477</td>\n",
       "      <td>20181230</td>\n",
       "      <td>20190103</td>\n",
       "      <td>AMAWSHNGBA807064</td>\n",
       "      <td>APLUNPFB000624</td>\n",
       "      <td>H</td>\n",
       "      <td>AMAW</td>\n",
       "      <td>CN</td>\n",
       "      <td>APL BARCELONA</td>\n",
       "      <td>APL BARCELONA</td>\n",
       "      <td>...</td>\n",
       "      <td>SCH 830 TREADMILL&lt;br/&gt;</td>\n",
       "      <td>AS ADDRESSED&lt;br/&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>END</td>\n",
       "      <td>871200</td>\n",
       "      <td>871200</td>\n",
       "      <td>Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6003201901070000524799</td>\n",
       "      <td>20181231</td>\n",
       "      <td>20190105</td>\n",
       "      <td>ONEYSH8KM4604300</td>\n",
       "      <td>None</td>\n",
       "      <td>R</td>\n",
       "      <td>ONEY</td>\n",
       "      <td>PA</td>\n",
       "      <td>9395159</td>\n",
       "      <td>HAMBURG BRIDGE</td>\n",
       "      <td>...</td>\n",
       "      <td>LOUISIANA GRILLS LG900 W FRONT SHELF&lt;br/&gt;</td>\n",
       "      <td>NO MARKS&lt;br/&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>360510.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>END</td>\n",
       "      <td>871200</td>\n",
       "      <td>871200</td>\n",
       "      <td>Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6003201908030000292253</td>\n",
       "      <td>20190802</td>\n",
       "      <td>20190802</td>\n",
       "      <td>EGLV142951865627</td>\n",
       "      <td>None</td>\n",
       "      <td>R</td>\n",
       "      <td>EGLV, EAGLE VAN LINES INC</td>\n",
       "      <td>MT</td>\n",
       "      <td>9727613</td>\n",
       "      <td>CAPE KORTIA</td>\n",
       "      <td>...</td>\n",
       "      <td>BICYCLE BICYCLE HS#871200 PO#7311123256 IF APPLICA</td>\n",
       "      <td>THE SAME THE SAME THE SAME THE SAME&lt;br/&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>END</td>\n",
       "      <td>871200</td>\n",
       "      <td>871200</td>\n",
       "      <td>Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6003201905210000407464</td>\n",
       "      <td>20190519</td>\n",
       "      <td>20190519</td>\n",
       "      <td>ACZDNA3H190089</td>\n",
       "      <td>COSU6212247850</td>\n",
       "      <td>H</td>\n",
       "      <td>ACZD</td>\n",
       "      <td>PA</td>\n",
       "      <td>COSCO ASIA</td>\n",
       "      <td>COSCO ASIA</td>\n",
       "      <td>...</td>\n",
       "      <td>26INCH BIKE&lt;br/&gt;</td>\n",
       "      <td>PROGRAM&lt;br/&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>END</td>\n",
       "      <td>871200</td>\n",
       "      <td>871200</td>\n",
       "      <td>Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6003201903080000032852</td>\n",
       "      <td>20190224</td>\n",
       "      <td>20190306</td>\n",
       "      <td>YMLUE234014732</td>\n",
       "      <td>None</td>\n",
       "      <td>R</td>\n",
       "      <td>YMLU, YANG MING MARINE TRANSPORT CORP</td>\n",
       "      <td>MH</td>\n",
       "      <td>9424912</td>\n",
       "      <td>MOL MAJESTY</td>\n",
       "      <td>...</td>\n",
       "      <td>BICYCLES OTH CYCLES (INC DEL TRICYCLE) NO M 4002 K</td>\n",
       "      <td>// // //&lt;br/&gt;// // //&lt;br/&gt;// // //&lt;br/&gt;// // //&lt;br</td>\n",
       "      <td>5</td>\n",
       "      <td>12656520.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>END</td>\n",
       "      <td>871200</td>\n",
       "      <td>871200</td>\n",
       "      <td>Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       System Identity Id  Estimate Arrival Date  Actual Arrival Date  \\\n",
       "1  6003201901040000381477  20181230               20190103              \n",
       "4  6003201901070000524799  20181231               20190105              \n",
       "6  6003201908030000292253  20190802               20190802              \n",
       "7  6003201905210000407464  20190519               20190519              \n",
       "8  6003201903080000032852  20190224               20190306              \n",
       "\n",
       "     Bill of Lading Master Bill of Lading Bill Type Code  \\\n",
       "1  AMAWSHNGBA807064  APLUNPFB000624        H               \n",
       "4  ONEYSH8KM4604300  None                  R               \n",
       "6  EGLV142951865627  None                  R               \n",
       "7  ACZDNA3H190089    COSU6212247850        H               \n",
       "8  YMLUE234014732    None                  R               \n",
       "\n",
       "                       Carrier SASC Code Vessel Country Code    Vessel Code  \\\n",
       "1  AMAW                                   CN                  APL BARCELONA   \n",
       "4  ONEY                                   PA                  9395159         \n",
       "6  EGLV, EAGLE VAN LINES INC              MT                  9727613         \n",
       "7  ACZD                                   PA                  COSCO ASIA      \n",
       "8  YMLU, YANG MING MARINE TRANSPORT CORP  MH                  9424912         \n",
       "\n",
       "      Vessel Name  ...                                        Product Desc  \\\n",
       "1  APL BARCELONA   ...  SCH 830 TREADMILL<br/>                               \n",
       "4  HAMBURG BRIDGE  ...  LOUISIANA GRILLS LG900 W FRONT SHELF<br/>            \n",
       "6  CAPE KORTIA     ...  BICYCLE BICYCLE HS#871200 PO#7311123256 IF APPLICA   \n",
       "7  COSCO ASIA      ...  26INCH BIKE<br/>                                     \n",
       "8  MOL MAJESTY     ...  BICYCLES OTH CYCLES (INC DEL TRICYCLE) NO M 4002 K   \n",
       "\n",
       "                                      Marks & Numbers  HS Code Sure Level  \\\n",
       "1  AS ADDRESSED<br/>                                   5                    \n",
       "4  NO MARKS<br/>                                       5                    \n",
       "6  THE SAME THE SAME THE SAME THE SAME<br/>            8                    \n",
       "7  PROGRAM<br/>                                        5                    \n",
       "8  // // //<br/>// // //<br/>// // //<br/>// // //<br  5                    \n",
       "\n",
       "          CIF Indicator of true supplier Indicator of true buyer  END HS Code  \\\n",
       "1  0.0         Y                          Y                       END  871200   \n",
       "4  360510.0    N                          N                       END  871200   \n",
       "6  0.0         Y                          Y                       END  871200   \n",
       "7  0.0         N                          N                       END  871200   \n",
       "8  12656520.0  Y                          Y                       END  871200   \n",
       "\n",
       "  HS_Code  \\\n",
       "1  871200   \n",
       "4  871200   \n",
       "6  871200   \n",
       "7  871200   \n",
       "8  871200   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Merged_Description  \n",
       "1  Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles  \n",
       "4  Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles  \n",
       "6  Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles  \n",
       "7  Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles  \n",
       "8  Bicycles and other cycles (including delivery tricycles), not motorized ;Bicycles having both wheels not exceeding 63.5 cm in diameter;Having both wheels not exceeding 50 cm in diameter;Having both wheels exceeding 50 cm but not exceeding 55 cm in diameter;Having both wheels exceeding 55 cm but not exceeding 63.5 cm in diameter;If weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm;Bicycles having both wheels exceeding 63.5 cm in diameter ;Other;Bicycles having a front wheel exceeding 55 cm but not exceeding 63.5 cm in diameter and a rear wheel exceeding 63.5 cm in diameter, weighing less than 16.3 kg complete without accessories and not designed for use with tires having a cross-sectional diameter exceeding 4.13 cm, valued $200 or more each;Other bicycles;Other cycles  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-drain",
   "metadata": {},
   "source": [
    "This could be a problem if we have very, very short entries in the description fields.\n",
    "\n",
    "Some possible solutions:\n",
    "1. Augment each entry with the dictionary definition.\n",
    "2. Add extra training records of just the dictionary definition.\n",
    "3. Train on only the dictionary definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "relevant-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 40\n",
    "\n",
    "X_train = pre_process(train_df_pd, 'Product Desc', max_length)\n",
    "X_dev = pre_process(dev_df_pd, 'Product Desc', max_length)\n",
    "\n",
    "# Create the labels using the 'HS_Code' column and as cast as int because that's what the keras loss function expects\n",
    "# https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class\n",
    "train_labels = np.array(train_df_pd['HS_Code']).astype(str)\n",
    "dev_labels = np.array(dev_df_pd['HS_Code']).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-outline",
   "metadata": {},
   "source": [
    "Encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "disabled-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(Y):\n",
    "    \"\"\"\n",
    "    Function that takes in a np.array of string categorical labels and returns a one-hot encoded transformation.\n",
    "    \"\"\"\n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    encoded_Y = encoder.transform(Y)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    return np_utils.to_categorical(encoded_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "prompt-python",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_train_labels = encode_labels(train_labels)\n",
    "dummy_dev_labels = encode_labels(dev_labels)\n",
    "dummy_train_labels[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "complimentary-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3, 4350, 40)\n",
      "X_train labels shape: (4350,)\n",
      "\n",
      "X_dev shape: (3, 1477, 40)\n",
      "X_dev labels shape: (1477,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure we end up with the shapes we expect.\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_train labels shape:', train_labels.shape)\n",
    "print()\n",
    "\n",
    "print('X_dev shape:', X_dev.shape)\n",
    "print('X_dev labels shape:', dev_labels.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "architectural-passing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101, 13012, 23490,  1010,  8040, 17206,  2121, 13012, 23490,\n",
       "        8040, 17206,  2121, 29449, 19912,  1012,  1030,  1030,  1030,\n",
       "        1041,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "outer-access",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871200"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-experiment",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "roman-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_model(max_input_length, train_layers, optimizer):\n",
    "    \"\"\"\n",
    "    UPDATE doc string\n",
    "    Implementation of multi classification model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_input_length,), dtype='int32', name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_input_length,), dtype='int32', name=\"attention_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_input_length,), dtype='int32', name=\"segment_ids\")\n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    \n",
    "    # Note: Bert layer from Hugging Face returns two values: sequence ouput, and pooled output. Here, we only want\n",
    "    # the former. (See https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) \n",
    "    \n",
    "    bert_layer = model_class.from_pretrained(pretrained_weights)\n",
    "    \n",
    "    \n",
    "    # Freeze layers, i.e. only train number of layers specified, starting from the top\n",
    "    \n",
    "    retrain_layers = []\n",
    "    \n",
    "    for retrain_layer_number in range(train_layers):\n",
    "        \n",
    "        layer_code = '_' + str(11 - retrain_layer_number)\n",
    "        retrain_layers.append(layer_code)\n",
    "    \n",
    "    for w in bert_layer.weights:\n",
    "        if not any([x in w.name for x in retrain_layers]):\n",
    "            w._trainable = False\n",
    "            \n",
    "    # End of freezing section\n",
    "    \n",
    "    # bert_layer(bert_inputs) generates two outputs. \n",
    "    # Select this to use the average of the non-CLS tokens to make the prediction\n",
    "    # bert_sequence = tf.math.reduce_mean(bert_layer(bert_inputs)[0], axis=1) # take the average of all the non-CLS tokens\n",
    "    \n",
    "    # Select this to use the CLS token to make the prediction\n",
    "    bert_sequence = bert_layer(bert_inputs)[1]\n",
    "    \n",
    "    print('Let us check the shape of the BERT layer output:', bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dropped = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(1, activation='softmax', name='target')(dropped) \n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "   \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy'])  \n",
    "\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "necessary-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us check the shape of the BERT layer output: KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name='tf_bert_model_10/bert/pooler/dense/Tanh:0', description=\"created by layer 'tf_bert_model_10'\")\n",
      "pred:  KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='target/Softmax:0', description=\"created by layer 'target'\")\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_masks (InputLayer)    [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model_10 (TFBertModel)  TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_masks[0][0]            \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      tf_bert_model_10[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_415 (Dropout)           (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "target (Dense)                  (None, 1)            257         dropout_415[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 109,679,361\n",
      "Trainable params: 109,679,361\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:755 train_step\n        loss = self.compiled_loss(\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/losses.py:1537 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/backend.py:4833 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 10) and (None, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-53a0476f8253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                                         verbose=0, mode='auto')\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Capture the history of the model fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     35\u001b[0m     x = {\"input_ids\": training_data[0],\n\u001b[1;32m     36\u001b[0m          \u001b[0;34m\"attention_masks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:755 train_step\n        loss = self.compiled_loss(\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/losses.py:1537 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/backend.py:4833 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_shape.py:1134 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 10) and (None, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# Set hyperparameters\n",
    "adam_customized = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "train_layers = 2\n",
    "\n",
    "# ###Convert the labels into 1 and 0 for binary_crossentropy\n",
    "\n",
    "# new_train_labels = np.where(train_labels_b==\"TIN\", 1, 0)\n",
    "\n",
    "# new_dev_labels = np.where(dev_labels_b==\"TIN\", 1, 0)\n",
    "\n",
    "\n",
    "training_labels = dummy_train_labels\n",
    "dev_labels = dummy_dev_labels\n",
    "\n",
    "\n",
    "training_data = X_train\n",
    "\n",
    "dev_data = X_dev\n",
    "\n",
    "\n",
    "model = sent_model(max_length, train_layers=train_layers, optimizer = adam_customized)\n",
    "\n",
    "\n",
    "\n",
    "# Set parameters for early stopping\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                        min_delta=0.001,\n",
    "                                                        patience=5,\n",
    "                                                        verbose=0, mode='auto')\n",
    "# Capture the history of the model fitting\n",
    "history = model.fit(\n",
    "    x = {\"input_ids\": training_data[0],\n",
    "         \"attention_masks\": training_data[1],\n",
    "         \"segment_ids\": training_data[2]}, y = {\"target\": training_labels},\n",
    "    validation_data=({\"input_ids\": dev_data[0],\n",
    "         \"attention_masks\": dev_data[1],\n",
    "         \"segment_ids\": dev_data[2]}, {\"target\": dev_labels}),\n",
    "    verbose=2,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-mileage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
